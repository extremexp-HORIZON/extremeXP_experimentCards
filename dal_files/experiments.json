{
    "experiments": [
        {
            "8szqOpcB8Xy5J8OG5bYK": {
                "id": "8szqOpcB8Xy5J8OG5bYK",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "NMzqOpcB8Xy5J8OG57cz"
                ],
                "start": "2025-06-04T15:29:22Z",
                "end": "2025-06-04T15:29:35Z"
            }
        },
        {
            "uMzoOpcB8Xy5J8OGzHiy": {
                "id": "uMzoOpcB8Xy5J8OGzHiy",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "NszoOpcB8Xy5J8OG0Hlr"
                ],
                "start": "2025-06-04T15:27:04Z",
                "end": "2025-06-04T15:27:21Z"
            }
        },
        {
            "WgYPNZcBbsUoTyvNi_DF": {
                "id": "WgYPNZcBbsUoTyvNi_DF",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    // define output data xtrain;\n    // define output data xtest;\n    // define output data ytrain;\n    // define output data ytest;\n    // define output data ypred;\n    // define output data rocdata;\n    // define output data model;\n\n    // configure data xtrain {\n    //     zenoh_name \"X_train.csv\";\n    // }\n    // configure data xtest {\n    //     zenoh_name \"X_test.csv\";\n    // }\n    // configure data ytrain {\n    //     zenoh_name \"Y_train.csv\";\n    // }\n    // configure data ytest {\n    //     zenoh_name \"Y_test.csv\";\n    // }\n    // configure data ypred {\n    //     zenoh_name \"Y_pred.csv\";\n    // }\n    // configure data model {\n    //     zenoh_name \"model.pkl\";\n    // }\n    // configure data rocdata {\n    //     zenoh_name \"roc_data.json\";\n    // }\n\n    define output data mlAnalysis;\n    configure data mlAnalysis {\n        zenoh_project \"ml-analysis\";\n    }\n    \n    Explainability.mlAnalysis --> mlAnalysis;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.trained_model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fAYPNZcBbsUoTyvNjfCn"
                ],
                "start": "2025-06-03T09:11:41Z",
                "end": "2025-06-03T09:15:15Z"
            }
        },
        {
            "pRl0P5YBMEQjgoDtvK_D": {
                "id": "pRl0P5YBMEQjgoDtvK_D",
                "name": "Panos",
                "intent": "create a good experiment",
                "metadata": {
                    "reason": "test",
                    "location": "tampere"
                },
                "status": "completed",
                "comment": "Trying to understand why i can set it completed",
                "model": "package example; workflow A { }",
                "workflow_ids": [
                    "phkPQ5YBMEQjgoDtoq_M",
                    "pxkfQ5YBMEQjgoDtha9X",
                    "qBkhQ5YBMEQjgoDtsq-t",
                    "qRkuQ5YBMEQjgoDtJq-w",
                    "qhk8Q5YBMEQjgoDtH6_U"
                ],
                "metric_ids": [
                    "sBlLQ5YBMEQjgoDtm699"
                ]
            }
        },
        {
            "2RmlRJYBMEQjgoDto69S": {
                "id": "2RmlRJYBMEQjgoDto69S",
                "name": "Panos5.0",
                "intent": "create a good experiment",
                "metadata": {
                    "reason": "test",
                    "location": "tampere"
                },
                "status": "completed",
                "comment": "Trying to understand why i can set it completed",
                "model": "package example; workflow A { }",
                "start": "2025-04-17T00:00:00Z",
                "workflow_ids": [
                    "2hmrRJYBMEQjgoDtSq9s",
                    "3RmtRJYBMEQjgoDtGq8X",
                    "4BnORJYBMEQjgoDtVq92"
                ]
            }
        },
        {
            "vRlnQ5YBMEQjgoDtUK9K": {
                "id": "vRlnQ5YBMEQjgoDtUK9K",
                "name": "Panos4.0",
                "intent": "create a good experiment",
                "metadata": {
                    "reason": "test",
                    "location": "tampere"
                },
                "status": "new",
                "comment": "Trying to understand why i can set it completed",
                "model": "package example; workflow A { }",
                "workflow_ids": [
                    "vhlnQ5YBMEQjgoDtfq-_",
                    "wRloQ5YBMEQjgoDtH68P",
                    "yRmOQ5YBMEQjgoDtga9B",
                    "zBmPQ5YBMEQjgoDtoa_-"
                ]
            }
        },
        {
            "1xmjRJYBMEQjgoDtSq8m": {
                "id": "1xmjRJYBMEQjgoDtSq8m",
                "name": "Panos5.0",
                "intent": "create a good experiment",
                "metadata": {
                    "reason": "test",
                    "location": "tampere"
                },
                "status": "completed",
                "comment": "Trying to understand why i can set it completed",
                "model": "package example; workflow A { }",
                "workflow_ids": []
            }
        },
        {
            "thljQ5YBMEQjgoDtb6_N": {
                "id": "thljQ5YBMEQjgoDtb6_N",
                "name": "Panos3.0",
                "intent": "create a good experiment",
                "metadata": {
                    "reason": "test",
                    "location": "tampere"
                },
                "status": "new",
                "comment": "Trying to understand why i can set it completed",
                "model": "package example; workflow A { }",
                "workflow_ids": []
            }
        },
        {
            "ohlgP5YBMEQjgoDtk6-K": {
                "id": "ohlgP5YBMEQjgoDtk6-K",
                "name": "provide a name",
                "intent": "provide a name",
                "metadata": {
                    "reason": "test",
                    "location": "Prague"
                },
                "status": "running",
                "comment": "some comment",
                "model": "package example; workflow A { }",
                "workflow_ids": []
            }
        },
        {
            "oRlfP5YBMEQjgoDtja-w": {
                "id": "oRlfP5YBMEQjgoDtja-w",
                "name": "provide a name",
                "intent": "provide a name",
                "metadata": {
                    "reason": "test",
                    "location": "prague"
                },
                "status": "new",
                "comment": "some comment",
                "model": "package example; workflow A { }",
                "workflow_ids": []
            }
        },
        {
            "5xlNXJYBMEQjgoDtI6_7": {
                "id": "5xlNXJYBMEQjgoDtI6_7",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6BlNXJYBMEQjgoDtJq_N",
                    "7BlPXJYBMEQjgoDtQK-d"
                ],
                "start": "2025-04-22T09:01:25Z"
            }
        },
        {
            "sRlRQ5YBMEQjgoDteK9P": {
                "id": "sRlRQ5YBMEQjgoDteK9P",
                "name": "Panos2.0",
                "intent": "create a good experiment",
                "metadata": {
                    "reason": "test",
                    "location": "tampere"
                },
                "status": "new",
                "comment": "Trying to understand why i can set it completed",
                "model": "package example; workflow A { }",
                "workflow_ids": [
                    "shlcQ5YBMEQjgoDt2a_R",
                    "tBlgQ5YBMEQjgoDtE69J",
                    "txlkQ5YBMEQjgoDtGK9w",
                    "uhlkQ5YBMEQjgoDtsq8Z"
                ]
            }
        },
        {
            "WxmeGZYBMEQjgoDtDK-X": {
                "id": "WxmeGZYBMEQjgoDtDK-X",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XBmeGZYBMEQjgoDtD6-o",
                    "XRmeGZYBMEQjgoDtEK9k",
                    "XhmeGZYBMEQjgoDtEK-Z",
                    "XxmeGZYBMEQjgoDtF6_Q",
                    "YBmeGZYBMEQjgoDtF6_8",
                    "YRmeGZYBMEQjgoDtGK8Y"
                ],
                "start": "2025-04-09T08:15:10Z",
                "end": "2025-04-09T08:15:19Z"
            }
        },
        {
            "aRm-GZYBMEQjgoDtpK_d": {
                "id": "aRm-GZYBMEQjgoDtpK_d",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ahm-GZYBMEQjgoDtpq9w",
                    "axm-GZYBMEQjgoDtpq-Q",
                    "bBm-GZYBMEQjgoDtpq-r",
                    "bRm-GZYBMEQjgoDtq6_u",
                    "bhm-GZYBMEQjgoDtrK8Q",
                    "bxm-GZYBMEQjgoDtrK8s"
                ],
                "start": "2025-04-09T08:50:46Z",
                "end": "2025-04-09T08:53:03Z"
            }
        },
        {
            "cBm_GZYBMEQjgoDtAa_j": {
                "id": "cBm_GZYBMEQjgoDtAa_j",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cRm_GZYBMEQjgoDtA68y",
                    "chm_GZYBMEQjgoDtA69U",
                    "cxm_GZYBMEQjgoDtA69v",
                    "dBm_GZYBMEQjgoDtCK-l",
                    "dRm_GZYBMEQjgoDtCK-_",
                    "dhm_GZYBMEQjgoDtCK_c"
                ],
                "start": "2025-04-09T08:51:10Z",
                "end": "2025-04-09T08:53:28Z"
            }
        },
        {
            "dxnCGZYBMEQjgoDtDa-Q": {
                "id": "dxnCGZYBMEQjgoDtDa-Q",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "eBnCGZYBMEQjgoDtDq_e",
                    "eRnCGZYBMEQjgoDtDq__",
                    "ehnCGZYBMEQjgoDtD68p",
                    "exnCGZYBMEQjgoDtFK9t",
                    "fBnCGZYBMEQjgoDtFK-D",
                    "fRnCGZYBMEQjgoDtFK-Y"
                ],
                "start": "2025-04-09T08:54:30Z"
            }
        },
        {
            "fhnJGZYBMEQjgoDtU68Q": {
                "id": "fhnJGZYBMEQjgoDtU68Q",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fxnJGZYBMEQjgoDtVK-U",
                    "gBnJGZYBMEQjgoDtVK-0",
                    "gRnJGZYBMEQjgoDtVK_S",
                    "ghnJGZYBMEQjgoDtWq85",
                    "gxnJGZYBMEQjgoDtWq9U",
                    "hBnJGZYBMEQjgoDtWq9v"
                ],
                "start": "2025-04-09T09:02:26Z",
                "end": "2025-04-09T09:02:45Z"
            }
        },
        {
            "hRmGGpYBMEQjgoDtlK9l": {
                "id": "hRmGGpYBMEQjgoDtlK9l",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "hhmGGpYBMEQjgoDtla_Y",
                    "hxmGGpYBMEQjgoDtla_4",
                    "iBmGGpYBMEQjgoDtlq8O",
                    "iRmGGpYBMEQjgoDtm69v",
                    "ihmGGpYBMEQjgoDtm6-T",
                    "ixmGGpYBMEQjgoDtm6-q"
                ],
                "start": "2025-04-09T12:29:09Z",
                "end": "2025-04-09T12:29:27Z"
            }
        },
        {
            "jBmKGpYBMEQjgoDtmK-X": {
                "id": "jBmKGpYBMEQjgoDtmK-X",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jRmKGpYBMEQjgoDtma_l",
                    "jhmKGpYBMEQjgoDtmq8F",
                    "jxmKGpYBMEQjgoDtmq8d",
                    "kBmKGpYBMEQjgoDtn694",
                    "kRmKGpYBMEQjgoDtn6-Q",
                    "khmKGpYBMEQjgoDtn6-j"
                ],
                "start": "2025-04-09T12:33:33Z",
                "end": "2025-04-09T12:33:49Z"
            }
        },
        {
            "kxk5JJYBMEQjgoDtBK83": {
                "id": "kxk5JJYBMEQjgoDtBK83",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "lBk5JJYBMEQjgoDtBa-o",
                    "lRk5JJYBMEQjgoDtBa_L",
                    "lhk5JJYBMEQjgoDtBa_g",
                    "lxk5JJYBMEQjgoDtC6-y",
                    "mBk5JJYBMEQjgoDtC6_N",
                    "mRk5JJYBMEQjgoDtC6_q"
                ],
                "start": "2025-04-11T09:40:38Z",
                "end": "2025-04-11T09:40:56Z"
            }
        },
        {
            "mhlRJJYBMEQjgoDtxq8T": {
                "id": "mhlRJJYBMEQjgoDtxq8T",
                "name": "parallel_nodes",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n          START -> (S1 || S2);\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy gridsearch;\n        param param1_vp = enum(2, 5, 1);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 6);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "mxlRJJYBMEQjgoDtx69r",
                    "nBlRJJYBMEQjgoDtx6-L",
                    "nRlRJJYBMEQjgoDtx6-j",
                    "nhlRJJYBMEQjgoDtza8M",
                    "nxlRJJYBMEQjgoDtza8n",
                    "oBlRJJYBMEQjgoDtza87"
                ],
                "start": "2025-04-11T10:07:41Z",
                "end": "2025-04-11T10:07:57Z"
            }
        },
        {
            "8xlUXJYBMEQjgoDt2q9z": {
                "id": "8xlUXJYBMEQjgoDt2q9z",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9BlUXJYBMEQjgoDt3a9T"
                ],
                "start": "2025-04-22T09:09:51Z"
            }
        },
        {
            "-BlVXJYBMEQjgoDtRK-t": {
                "id": "-BlVXJYBMEQjgoDtRK-t",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-RlVXJYBMEQjgoDtR6-Q",
                    "_RlVXJYBMEQjgoDtV692"
                ],
                "start": "2025-04-22T09:10:18Z"
            }
        },
        {
            "_hlVXJYBMEQjgoDtu6-u": {
                "id": "_hlVXJYBMEQjgoDtu6-u",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_xlVXJYBMEQjgoDtvq-E",
                    "AxlXXJYBMEQjgoDt2LCl"
                ],
                "start": "2025-04-22T09:10:48Z"
            }
        },
        {
            "7RlQXJYBMEQjgoDtRa8K": {
                "id": "7RlQXJYBMEQjgoDtRa8K",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "7hlQXJYBMEQjgoDtR6_j",
                    "8hlSXJYBMEQjgoDtYK-a"
                ],
                "start": "2025-04-22T09:04:50Z"
            }
        },
        {
            "CRljXJYBMEQjgoDtjrC-": {
                "id": "CRljXJYBMEQjgoDtjrC-",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ChljXJYBMEQjgoDtkbDA"
                ],
                "start": "2025-04-22T09:25:55Z"
            }
        },
        {
            "DhljXJYBMEQjgoDt0bAT": {
                "id": "DhljXJYBMEQjgoDt0bAT",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DxljXJYBMEQjgoDt17D7"
                ],
                "start": "2025-04-22T09:26:11Z"
            }
        },
        {
            "ExlkXJYBMEQjgoDtFLA5": {
                "id": "ExlkXJYBMEQjgoDtFLA5",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "FBlkXJYBMEQjgoDtF7Ae",
                    "GBlkXJYBMEQjgoDtJrBu"
                ],
                "start": "2025-04-22T09:26:29Z"
            }
        },
        {
            "BBliXJYBMEQjgoDtU7DB": {
                "id": "BBliXJYBMEQjgoDtU7DB",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "BRliXJYBMEQjgoDtVrCu"
                ],
                "start": "2025-04-22T09:24:34Z"
            }
        },
        {
            "IxlmXJYBMEQjgoDt1rAL": {
                "id": "IxlmXJYBMEQjgoDt1rAL",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "JBlmXJYBMEQjgoDt2LDl",
                    "KBlmXJYBMEQjgoDt6rBm"
                ],
                "start": "2025-04-22T09:29:29Z"
            }
        },
        {
            "KRlnXJYBMEQjgoDtUbCv": {
                "id": "KRlnXJYBMEQjgoDtUbCv",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "KhlnXJYBMEQjgoDtVLCP",
                    "LhlnXJYBMEQjgoDtabCN"
                ],
                "start": "2025-04-22T09:30:01Z"
            }
        },
        {
            "GRlkXJYBMEQjgoDtyLBV": {
                "id": "GRlkXJYBMEQjgoDtyLBV",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GhlkXJYBMEQjgoDty7Al"
                ],
                "start": "2025-04-22T09:27:15Z"
            }
        },
        {
            "HhllXJYBMEQjgoDtbLAD": {
                "id": "HhllXJYBMEQjgoDtbLAD",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "HxllXJYBMEQjgoDtbrDp"
                ],
                "start": "2025-04-22T09:27:57Z"
            }
        },
        {
            "LxlnXJYBMEQjgoDtz7Dm": {
                "id": "LxlnXJYBMEQjgoDtz7Dm",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MBlnXJYBMEQjgoDt0rCj",
                    "NBlpXJYBMEQjgoDt5LCo"
                ],
                "start": "2025-04-22T09:30:33Z"
            }
        },
        {
            "OhlqXJYBMEQjgoDt-bAl": {
                "id": "OhlqXJYBMEQjgoDt-bAl",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "OxlqXJYBMEQjgoDt-7Dn",
                    "PxltXJYBMEQjgoDtGLCM"
                ],
                "start": "2025-04-22T09:34:00Z"
            }
        },
        {
            "NRlqXJYBMEQjgoDtJrBt": {
                "id": "NRlqXJYBMEQjgoDtJrBt",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "NhlqXJYBMEQjgoDtKbA-"
                ],
                "start": "2025-04-22T09:33:06Z"
            }
        },
        {
            "TBl-XJYBMEQjgoDt9rB-": {
                "id": "TBl-XJYBMEQjgoDt9rB-",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n  workflow ValidationTask from validation_task {\n    task ValidateModel {\n      implementation \"T2_PassMachineFilesThroughTheBestModel\";\n    }\n  }\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "TRl_XJYBMEQjgoDtgLBG": {
                "id": "TRl_XJYBMEQjgoDtgLBG",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow ValidationTask from validation_task {\n  task ValidateModel {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ThmDXJYBMEQjgoDtcLBH": {
                "id": "ThmDXJYBMEQjgoDtcLBH",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow ValidationTask from validation_task {\n  task ValidateModel {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space Val of ValidationTask {\n    strategy gridsearch;\n    param random = enum(1);\n    task ValidateModel {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Shl9XJYBMEQjgoDtq7Az": {
                "id": "Shl9XJYBMEQjgoDtq7Az",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n  space ValidationTask of validation_task {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    task ValidateModel {\n      implementation \"T2_PassMachineFilesThroughTheBestModel\";\n    }\n  }\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Sxl-XJYBMEQjgoDt77Ba": {
                "id": "Sxl-XJYBMEQjgoDt77Ba",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n  workflow ValidationTask from validation_task {\n    task ValidateModel {\n      implementation \"T2_PassMachineFilesThroughTheBestModel\";\n    }\n  }\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SBl8XJYBMEQjgoDt_bAq": {
                "id": "SBl8XJYBMEQjgoDt_bAq",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n  space ValidationTask of validation_task {\n    param epochs_vp = enum(2);\n\n    task ValidateModel {\n      implementation \"T2_PassMachineFilesThroughTheBestModel\";\n    }\n  }\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SRl9XJYBMEQjgoDtTbAe": {
                "id": "SRl9XJYBMEQjgoDtTbAe",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n  space ValidationTask of validation_task {\n    strategy gridsearch;\n    task ValidateModel {\n      implementation \"T2_PassMachineFilesThroughTheBestModel\";\n    }\n  }\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Rxl8XJYBMEQjgoDtRrB6": {
                "id": "Rxl8XJYBMEQjgoDtRrB6",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n  space ValidationTask of validation_task {\n    task ValidateModel {\n      implementation \"T2_PassMachineFilesThroughTheBestModel\";\n    }\n  }\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Rhl6XJYBMEQjgoDthrBN": {
                "id": "Rhl6XJYBMEQjgoDthrBN",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow ValidationTask {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> ValidationTask { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> ValidationTask;\n    S1B -> ValidationTask;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //ValidationTask -> I1 -> END;\n    ValidationTask -> END;\n\n    // T1 es best_model_evaluator\n    // ValidationTask es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "TxmDXJYBMEQjgoDtu7BW": {
                "id": "TxmDXJYBMEQjgoDtu7BW",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow ValidationTask from validation_task {\n  task ValidateModel {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> Val { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> Val;\n    S1B -> Val;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //Val -> I1 -> END;\n    Val -> END;\n\n    // T1 es best_model_evaluator\n    // Val es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space Val of ValidationTask {\n    strategy gridsearch;\n    param random = enum(1);\n    task ValidateModel {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n\n\n\n\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "UBmIXJYBMEQjgoDtIrDm": {
                "id": "UBmIXJYBMEQjgoDtIrDm",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow validation_task {\n\n  // Task CONNECTIONS\n  START -> ValidateModel -> END;\n\n  task ValidateModel;\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  ValidationDataFile --> ValidateModel.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow ValidationTask from validation_task {\n  task ValidateModel {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> I1 -> END;\n    V1 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space V1 of ValidationTask {\n    strategy gridsearch;\n    param random = enum(1);\n    task ValidateModel {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n\n\n\n\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "URmIXJYBMEQjgoDtJbDa",
                    "VRmKXJYBMEQjgoDtPLCQ"
                ],
                "start": "2025-04-22T10:05:52Z"
            }
        },
        {
            "WBk2fJYBMEQjgoDtEbAq": {
                "id": "WBk2fJYBMEQjgoDtEbAq",
                "name": "I2CAT_workflow1a",
                "model": "workflow uc2_workflow1 {\n\n    START -> ReadData -> Partitioning -> ModelTrain -> ModelPredict -> END;\n\n    task ReadData {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task Partitioning {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task ModelTrain;\n\n    task ModelPredict {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> ReadData.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task ModelTrain {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth_vp = range(3, 7);\n        param n_estimators_vp = range(5, 10);\n\n        task ModelTrain {\n          param max_depth = max_depth_vp;\n          param n_estimators = n_estimators_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "QBltXJYBMEQjgoDtlrDC": {
                "id": "QBltXJYBMEQjgoDtlrDC",
                "name": "mega_flow",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFileBinary;\n\n  configure data ValidationDataFileBinary {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> T2 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> T2;\n    S1B -> T2;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //T2 -> I1 -> END;\n    T2 -> END;\n\n    // T1 es best_model_evaluator\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es useriteration\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  task T1 {\n    implementation \"T1_best_model_evaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_PassMachineFilesThroughTheBestModel\";\n\n//    define output data ValidationDataFile; // NOTE No funciona\n\n//    configure data ValidationDataFile {\n//      path \"validation/**\";\n//    }\n\n  }\n\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "QRltXJYBMEQjgoDtmbCi",
                    "RRlvXJYBMEQjgoDtrLCr"
                ],
                "start": "2025-04-22T09:36:52Z"
            }
        },
        {
            "WxmHfJYBMEQjgoDtx7Cw": {
                "id": "WxmHfJYBMEQjgoDtx7Cw",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XBmHfJYBMEQjgoDtyrDu"
                ],
                "start": "2025-04-28T15:13:20Z",
                "end": "2025-04-28T15:14:08Z"
            }
        },
        {
            "YBmKfJYBMEQjgoDt4bDN": {
                "id": "YBmKfJYBMEQjgoDt4bDN",
                "name": "IDEKO_main_oneRun",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n  define task ReadData;\n  define task TrainModel;\n  define task EvaluateModel;\n  define task PrepareData;\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  configure task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  configure task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  configure task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  configure task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  configure task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    configure task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Zhl9gJYBMEQjgoDtNrDM": {
                "id": "Zhl9gJYBMEQjgoDtNrDM",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"UserInteraction.Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Zxl_gJYBMEQjgoDtMbBo": {
                "id": "Zxl_gJYBMEQjgoDtMbBo",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"UserInteraction.Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "YRmdfJYBMEQjgoDtULBn": {
                "id": "YRmdfJYBMEQjgoDtULBn",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YhmdfJYBMEQjgoDtUrCq"
                ],
                "start": "2025-04-28T15:36:51Z",
                "end": "2025-04-28T15:37:33Z"
            }
        },
        {
            "aBl_gJYBMEQjgoDtX7Dw": {
                "id": "aBl_gJYBMEQjgoDtX7Dw",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WRk2fJYBMEQjgoDtOLDA": {
                "id": "WRk2fJYBMEQjgoDtOLDA",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow1a {\n\n    START -> dataset_generation -> END;\n\n    task dataset_generation {\n        implementation \"I2CAT.dataset_generation\";\n    }\n\n    define input data ExternalDataFile;\n    \n    ExternalDataFile --> dataset_generation.ExternalDataFile;\n    \n    configure data ExternalDataFile {\n        path \"uc2-config/**\";\n    }\n}\n\nworkflow Assembled_uc2_workflow1a from uc2_workflow1a {\n}\n\nexperiment uc2_workflow1a_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow1a {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Whk2fJYBMEQjgoDtOrCN"
                ],
                "start": "2025-04-28T13:44:15Z",
                "end": "2025-04-28T13:44:29Z"
            }
        },
        {
            "axmBgJYBMEQjgoDtarDe": {
                "id": "axmBgJYBMEQjgoDtarDe",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bBmBgJYBMEQjgoDtbLCc"
                ],
                "start": "2025-04-29T09:44:51Z",
                "end": "2025-04-29T09:45:03Z"
            }
        },
        {
            "aRmAgJYBMEQjgoDtJrDQ": {
                "id": "aRmAgJYBMEQjgoDtJrDQ",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ahmAgJYBMEQjgoDtKLCS"
                ],
                "start": "2025-04-29T09:43:28Z",
                "end": "2025-04-29T09:43:45Z"
            }
        },
        {
            "dxmUgJYBMEQjgoDtErBs": {
                "id": "dxmUgJYBMEQjgoDtErBs",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "eBmUgJYBMEQjgoDtFLAp"
                ],
                "start": "2025-04-29T10:05:14Z",
                "end": "2025-04-29T10:05:26Z"
            }
        },
        {
            "cRmNgJYBMEQjgoDtJ7B3": {
                "id": "cRmNgJYBMEQjgoDtJ7B3",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "chmNgJYBMEQjgoDtKbAk"
                ],
                "start": "2025-04-29T09:57:41Z",
                "end": "2025-04-29T09:57:52Z"
            }
        },
        {
            "bRmJgJYBMEQjgoDtMbDS": {
                "id": "bRmJgJYBMEQjgoDtMbDS",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bhmJgJYBMEQjgoDtM7CP"
                ],
                "start": "2025-04-29T09:53:21Z",
                "end": "2025-04-29T09:53:33Z"
            }
        },
        {
            "bxmLgJYBMEQjgoDtK7A0": {
                "id": "bxmLgJYBMEQjgoDtK7A0",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cBmLgJYBMEQjgoDtLLDz"
                ],
                "start": "2025-04-29T09:55:31Z",
                "end": "2025-04-29T09:55:42Z"
            }
        },
        {
            "dRmSgJYBMEQjgoDt3bDG": {
                "id": "dRmSgJYBMEQjgoDt3bDG",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "dhmSgJYBMEQjgoDt37CY"
                ],
                "start": "2025-04-29T10:03:55Z",
                "end": "2025-04-29T10:04:08Z"
            }
        },
        {
            "cxmSgJYBMEQjgoDtJLDz": {
                "id": "cxmSgJYBMEQjgoDtJLDz",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "dBmSgJYBMEQjgoDtJrCt"
                ],
                "start": "2025-04-29T10:03:08Z",
                "end": "2025-04-29T10:03:19Z"
            }
        },
        {
            "fRmcgJYBMEQjgoDtM7DW": {
                "id": "fRmcgJYBMEQjgoDtM7DW",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fhmcgJYBMEQjgoDtNbCT"
                ],
                "start": "2025-04-29T10:14:07Z",
                "end": "2025-04-29T10:14:18Z"
            }
        },
        {
            "fxm6gJYBMEQjgoDtJLAa": {
                "id": "fxm6gJYBMEQjgoDtJLAa",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "gBm6gJYBMEQjgoDtJbDh"
                ],
                "start": "2025-04-29T10:46:49Z",
                "end": "2025-04-29T10:47:02Z"
            }
        },
        {
            "gxnEgJYBMEQjgoDteLCx": {
                "id": "gxnEgJYBMEQjgoDteLCx",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "hBnEgJYBMEQjgoDterBt"
                ],
                "start": "2025-04-29T10:58:06Z",
                "end": "2025-04-29T10:58:18Z"
            }
        },
        {
            "hRnFgJYBMEQjgoDtTLBm": {
                "id": "hRnFgJYBMEQjgoDtTLBm",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "hhnFgJYBMEQjgoDtTrAe"
                ],
                "start": "2025-04-29T10:59:00Z",
                "end": "2025-04-29T10:59:12Z"
            }
        },
        {
            "eRmVgJYBMEQjgoDts7B0": {
                "id": "eRmVgJYBMEQjgoDts7B0",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ehmVgJYBMEQjgoDttbBD"
                ],
                "start": "2025-04-29T10:07:01Z",
                "end": "2025-04-29T10:07:13Z"
            }
        },
        {
            "exmZgJYBMEQjgoDto7B1": {
                "id": "exmZgJYBMEQjgoDto7B1",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fBmZgJYBMEQjgoDtpbAh"
                ],
                "start": "2025-04-29T10:11:19Z",
                "end": "2025-04-29T10:11:31Z"
            }
        },
        {
            "gRnAgJYBMEQjgoDt5rAE": {
                "id": "gRnAgJYBMEQjgoDt5rAE",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ghnAgJYBMEQjgoDt57DK"
                ],
                "start": "2025-04-29T10:54:12Z",
                "end": "2025-04-29T10:54:24Z"
            }
        },
        {
            "ixnSgJYBMEQjgoDtgLBZ": {
                "id": "ixnSgJYBMEQjgoDtgLBZ",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\ndefine output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "khnjgJYBMEQjgoDtfrDC": {
                "id": "khnjgJYBMEQjgoDtfrDC",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "kxnjgJYBMEQjgoDtgLCL"
                ],
                "start": "2025-04-29T11:31:59Z",
                "end": "2025-04-29T11:32:12Z"
            }
        },
        {
            "lBnjgJYBMEQjgoDt5bBH": {
                "id": "lBnjgJYBMEQjgoDt5bBH",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "lRnjgJYBMEQjgoDt57AG"
                ],
                "start": "2025-04-29T11:32:25Z",
                "end": "2025-04-29T11:32:37Z"
            }
        },
        {
            "kBnYgJYBMEQjgoDt_bDs": {
                "id": "kBnYgJYBMEQjgoDt_bDs",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "kRnYgJYBMEQjgoDt_7Ch"
                ],
                "start": "2025-04-29T11:20:31Z",
                "end": "2025-04-29T11:20:43Z"
            }
        },
        {
            "iRnGgJYBMEQjgoDtb7AU": {
                "id": "iRnGgJYBMEQjgoDtb7AU",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ihnGgJYBMEQjgoDtcLC5"
                ],
                "start": "2025-04-29T11:00:15Z",
                "end": "2025-04-29T11:00:27Z"
            }
        },
        {
            "jBnVgJYBMEQjgoDtOLBF": {
                "id": "jBnVgJYBMEQjgoDtOLBF",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jRnVgJYBMEQjgoDtOrAD"
                ],
                "start": "2025-04-29T11:16:24Z",
                "end": "2025-04-29T11:16:35Z"
            }
        },
        {
            "jhnXgJYBMEQjgoDtZrAJ": {
                "id": "jhnXgJYBMEQjgoDtZrAJ",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jxnXgJYBMEQjgoDtZ7DH"
                ],
                "start": "2025-04-29T11:18:46Z",
                "end": "2025-04-29T11:18:58Z"
            }
        },
        {
            "hxnFgJYBMEQjgoDtybDw": {
                "id": "hxnFgJYBMEQjgoDtybDw",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "iBnFgJYBMEQjgoDty7C0"
                ],
                "start": "2025-04-29T10:59:32Z",
                "end": "2025-04-29T10:59:44Z"
            }
        },
        {
            "tBnygJYBMEQjgoDtNrDp": {
                "id": "tBnygJYBMEQjgoDtNrDp",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "tRnygJYBMEQjgoDtOLCr"
                ],
                "start": "2025-04-29T11:48:04Z",
                "end": "2025-04-29T11:48:15Z"
            }
        },
        {
            "thnygJYBMEQjgoDt2rDM": {
                "id": "thnygJYBMEQjgoDt2rDM",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-29T12:48:46Z"
            }
        },
        {
            "txnygJYBMEQjgoDt27DK": {
                "id": "txnygJYBMEQjgoDt27DK",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-29T12:48:46Z"
            }
        },
        {
            "whn2gJYBMEQjgoDtJbBe": {
                "id": "whn2gJYBMEQjgoDtJbBe",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "wxn2gJYBMEQjgoDtJrAO"
                ],
                "start": "2025-04-29T12:52:21Z"
            }
        },
        {
            "wBn1gJYBMEQjgoDt-rDs": {
                "id": "wBn1gJYBMEQjgoDt-rDs",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "wRn1gJYBMEQjgoDt_LCj"
                ],
                "start": "2025-04-29T11:52:11Z",
                "end": "2025-04-29T11:52:25Z"
            }
        },
        {
            "qhnsgJYBMEQjgoDtsbAv": {
                "id": "qhnsgJYBMEQjgoDtsbAv",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "qxnsgJYBMEQjgoDtsbDX"
                ],
                "start": "2025-04-29T12:42:02Z"
            }
        },
        {
            "shntgJYBMEQjgoDtZrAs": {
                "id": "shntgJYBMEQjgoDtZrAs",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "sxntgJYBMEQjgoDtZ7Da"
                ],
                "start": "2025-04-29T11:42:48Z",
                "end": "2025-04-29T11:43:01Z"
            }
        },
        {
            "nhnmgJYBMEQjgoDt_bDq": {
                "id": "nhnmgJYBMEQjgoDt_bDq",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "nxnmgJYBMEQjgoDt_rCj"
                ],
                "start": "2025-04-29T12:35:48Z"
            }
        },
        {
            "uBnzgJYBMEQjgoDtnLC5": {
                "id": "uBnzgJYBMEQjgoDtnLC5",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "uRnzgJYBMEQjgoDtnbBd"
                ],
                "start": "2025-04-29T12:49:35Z"
            }
        },
        {
            "lhnlgJYBMEQjgoDtn7Ce": {
                "id": "lhnlgJYBMEQjgoDtn7Ce",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "lxnlgJYBMEQjgoDtoLBJ"
                ],
                "start": "2025-04-29T12:34:19Z"
            }
        },
        {
            "phnqgJYBMEQjgoDtn7BP": {
                "id": "phnqgJYBMEQjgoDtn7BP",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "pxnqgJYBMEQjgoDtobAH"
                ],
                "start": "2025-04-29T11:39:46Z"
            }
        },
        {
            "qBnqgJYBMEQjgoDt47DV": {
                "id": "qBnqgJYBMEQjgoDt47DV",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "qRnqgJYBMEQjgoDt5bCN"
                ],
                "start": "2025-04-29T11:40:04Z",
                "end": "2025-04-29T11:40:16Z"
            }
        },
        {
            "1Bn4gJYBMEQjgoDtnrCp": {
                "id": "1Bn4gJYBMEQjgoDtnrCp",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "1Rn4gJYBMEQjgoDtn7Bf"
                ],
                "start": "2025-04-29T12:55:04Z",
                "end": "2025-04-29T13:03:01Z"
            }
        },
        {
            "4RkBgZYBMEQjgoDtObB8": {
                "id": "4RkBgZYBMEQjgoDtObB8",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "4hkBgZYBMEQjgoDt4bAV": {
                "id": "4hkBgZYBMEQjgoDt4bAV",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "4xkCgZYBMEQjgoDtIbA_": {
                "id": "4xkCgZYBMEQjgoDtIbA_",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "5BkCgZYBMEQjgoDtIbDp"
                ],
                "start": "2025-04-29T13:05:27Z",
                "end": "2025-04-29T13:06:04Z"
            }
        },
        {
            "3hn-gJYBMEQjgoDtQbAG": {
                "id": "3hn-gJYBMEQjgoDtQbAG",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "3xn-gJYBMEQjgoDtQrDS"
                ],
                "start": "2025-04-29T12:01:13Z",
                "end": "2025-04-29T12:01:27Z"
            }
        },
        {
            "4Bn_gJYBMEQjgoDtarAm": {
                "id": "4Bn_gJYBMEQjgoDtarAm",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "yhn3gJYBMEQjgoDtSrCu": {
                "id": "yhn3gJYBMEQjgoDtSrCu",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "yxn3gJYBMEQjgoDtS7BY"
                ],
                "start": "2025-04-29T12:53:37Z"
            }
        },
        {
            "0hn3gJYBMEQjgoDtprDj": {
                "id": "0hn3gJYBMEQjgoDtprDj",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "0xn3gJYBMEQjgoDtqLCW"
                ],
                "start": "2025-04-29T11:54:00Z",
                "end": "2025-04-29T11:54:13Z"
            }
        },
        {
            "_hkGgZYBMEQjgoDt97AU": {
                "id": "_hkGgZYBMEQjgoDt97AU",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow1a {\n\n    START -> dataset_generation -> END;\n\n    task dataset_generation {\n        implementation \"I2CAT.dataset_generation\";\n    }\n\n    define input data ExternalDataFile;\n    \n    ExternalDataFile --> dataset_generation.ExternalDataFile;\n    \n    configure data ExternalDataFile {\n        path \"uc2-config/**\";\n    }\n}\n\nworkflow Assembled_uc2_workflow1a from uc2_workflow1a {\n}\n\nexperiment uc2_workflow1a_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow1a {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "completed",
                "workflow_ids": [
                    "_xkGgZYBMEQjgoDt97Cq"
                ],
                "start": "2025-04-29T13:10:44Z",
                "end": "2025-04-29T13:10:52Z"
            }
        },
        {
            "ABkHgZYBMEQjgoDtfrFp": {
                "id": "ABkHgZYBMEQjgoDtfrFp",
                "name": "I2CAT_workflow1",
                "model": "workflow uc2_workflow1 {\n\n    START -> ReadData -> Partitioning -> ModelTrain -> ModelPredict -> END;\n\n    task ReadData {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task Partitioning {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task ModelTrain;\n\n    task ModelPredict {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> ReadData.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n}\n\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task ModelTrain {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    //Automated\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth_vp = range(3, 6,3);\n        param n_estimators_vp = range(5, 11,5);\n\n        task ModelTrain {\n          param max_depth = max_depth_vp;\n          param n_estimators = n_estimators_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "ARkHgZYBMEQjgoDtf7ED",
                    "BRkHgZYBMEQjgoDtf7G_"
                ],
                "start": "2025-04-29T13:11:18Z",
                "end": "2025-04-29T13:12:05Z"
            }
        },
        {
            "6xkDgZYBMEQjgoDtT7DI": {
                "id": "6xkDgZYBMEQjgoDtT7DI",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "7BkDgZYBMEQjgoDtULBk"
                ],
                "start": "2025-04-29T13:06:44Z",
                "end": "2025-04-29T13:07:20Z"
            }
        },
        {
            "8xkEgZYBMEQjgoDtIrCK": {
                "id": "8xkEgZYBMEQjgoDtIrCK",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> AddPadding -> SplitData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n  task AddPadding {\n    implementation \"IDEKO.AddPadding\";\n  }\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task SplitData {\n    implementation \"IDEKO.SplitData\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> AddPadding.X41;\n    ReadData.Y --> AddPadding.Y;\n    ReadData.IndicatorList --> AddPadding.IndicatorList;\n    AddPadding.XPad --> SplitData.XPad;\n    AddPadding.YPad --> SplitData.YPad;\n\n    SplitData.Timestamps41 --> TrainModel.Timestamps;\n    SplitData.Features --> TrainModel.Features;\n    SplitData.XTrain --> TrainModel.XTrain;\n    SplitData.XTest --> TrainModel.XTest;\n    SplitData.YTrain --> TrainModel.YTrain;\n    SplitData.YTest --> TrainModel.YTest;\n    SplitData.XPad --> TrainModel.XPad;\n    SplitData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(5);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "9BkEgZYBMEQjgoDtI7Ad",
                    "-RkEgZYBMEQjgoDttrB3"
                ],
                "start": "2025-04-29T13:07:38Z",
                "end": "2025-04-29T13:08:53Z"
            }
        },
        {
            "3Bn8gJYBMEQjgoDtLrC7": {
                "id": "3Bn8gJYBMEQjgoDtLrC7",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "3Rn8gJYBMEQjgoDtMLCA"
                ],
                "start": "2025-04-29T11:58:57Z",
                "end": "2025-04-29T11:59:12Z"
            }
        },
        {
            "HRkwgZYBMEQjgoDtprHw": {
                "id": "HRkwgZYBMEQjgoDtprHw",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "HhkwgZYBMEQjgoDtqLGi"
                ],
                "start": "2025-04-29T12:56:16Z",
                "end": "2025-04-29T12:56:28Z"
            }
        },
        {
            "FxkpgZYBMEQjgoDt6rEP": {
                "id": "FxkpgZYBMEQjgoDt6rEP",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"/opt/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GBkpgZYBMEQjgoDt67HC"
                ],
                "start": "2025-04-29T12:48:54Z"
            }
        },
        {
            "GRkqgZYBMEQjgoDtu7Fo": {
                "id": "GRkqgZYBMEQjgoDtu7Fo",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"/opt/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GhkqgZYBMEQjgoDtvbE7"
                ],
                "start": "2025-04-29T12:49:48Z",
                "end": "2025-04-29T12:50:02Z"
            }
        },
        {
            "ExkegZYBMEQjgoDtr7Go": {
                "id": "ExkegZYBMEQjgoDtr7Go",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "FBkegZYBMEQjgoDtsbFe"
                ],
                "start": "2025-04-29T12:36:38Z",
                "end": "2025-04-29T12:36:50Z"
            }
        },
        {
            "CRkUgZYBMEQjgoDtjbEc": {
                "id": "CRkUgZYBMEQjgoDtjbEc",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ChkUgZYBMEQjgoDtjrHi"
                ],
                "start": "2025-04-29T12:25:34Z",
                "end": "2025-04-29T12:25:47Z"
            }
        },
        {
            "CxkVgZYBMEQjgoDtWLHH": {
                "id": "CxkVgZYBMEQjgoDtWLHH",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DBkVgZYBMEQjgoDtWrFx"
                ],
                "start": "2025-04-29T12:26:26Z",
                "end": "2025-04-29T12:26:38Z"
            }
        },
        {
            "DxkXgZYBMEQjgoDtebHe": {
                "id": "DxkXgZYBMEQjgoDtebHe",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "EBkXgZYBMEQjgoDte7GT"
                ],
                "start": "2025-04-29T12:28:46Z",
                "end": "2025-04-29T12:28:58Z"
            }
        },
        {
            "ERkbgZYBMEQjgoDtKbG4": {
                "id": "ERkbgZYBMEQjgoDtKbG4",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "EhkbgZYBMEQjgoDtK7F1"
                ],
                "start": "2025-04-29T12:32:48Z",
                "end": "2025-04-29T12:32:59Z"
            }
        },
        {
            "FRkngZYBMEQjgoDtTrGS": {
                "id": "FRkngZYBMEQjgoDtTrGS",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"C:/opt/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "FhkngZYBMEQjgoDtULFX"
                ],
                "start": "2025-04-29T12:46:03Z",
                "end": "2025-04-29T12:46:15Z"
            }
        },
        {
            "GxktgZYBMEQjgoDtfLGK": {
                "id": "GxktgZYBMEQjgoDtfLGK",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"/opt/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "HBktgZYBMEQjgoDtfrE6"
                ],
                "start": "2025-04-29T12:52:48Z",
                "end": "2025-04-29T12:53:00Z"
            }
        },
        {
            "DRkWgZYBMEQjgoDtOLH7": {
                "id": "DRkWgZYBMEQjgoDtOLH7",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DhkWgZYBMEQjgoDtOrGw"
                ],
                "start": "2025-04-29T12:27:24Z",
                "end": "2025-04-29T12:27:36Z"
            }
        },
        {
            "NBlNgZYBMEQjgoDtJ7E4": {
                "id": "NBlNgZYBMEQjgoDtJ7E4",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> AddPadding -> SplitData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n  task AddPadding {\n    implementation \"IDEKO.AddPadding\";\n  }\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task SplitData {\n    implementation \"IDEKO.SplitData\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> AddPadding.X41;\n    ReadData.Y --> AddPadding.Y;\n    ReadData.IndicatorList --> AddPadding.IndicatorList;\n    AddPadding.XPad --> SplitData.XPad;\n    AddPadding.YPad --> SplitData.YPad;\n\n    SplitData.Timestamps41 --> TrainModel.Timestamps;\n    SplitData.Features --> TrainModel.Features;\n    SplitData.XTrain --> TrainModel.XTrain;\n    SplitData.XTest --> TrainModel.XTest;\n    SplitData.YTrain --> TrainModel.YTrain;\n    SplitData.YTest --> TrainModel.YTest;\n    SplitData.XPad --> TrainModel.XPad;\n    SplitData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(5);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "NRlNgZYBMEQjgoDtJ7HV",
                    "OhlNgZYBMEQjgoDtuLHi"
                ],
                "start": "2025-04-29T14:27:24Z",
                "end": "2025-04-29T14:28:38Z"
            }
        },
        {
            "PxlOgZYBMEQjgoDtSLGE": {
                "id": "PxlOgZYBMEQjgoDtSLGE",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "QBlOgZYBMEQjgoDtSrHX"
                ],
                "start": "2025-04-29T13:28:38Z",
                "end": "2025-04-29T13:29:23Z"
            }
        },
        {
            "IxkzgZYBMEQjgoDty7HX": {
                "id": "IxkzgZYBMEQjgoDty7HX",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "JBkzgZYBMEQjgoDtzbGd"
                ],
                "start": "2025-04-29T12:59:42Z",
                "end": "2025-04-29T12:59:54Z"
            }
        },
        {
            "LRlDgZYBMEQjgoDtYrFS": {
                "id": "LRlDgZYBMEQjgoDtYrFS",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "LhlDgZYBMEQjgoDtZLED"
                ],
                "start": "2025-04-29T13:16:44Z",
                "end": "2025-04-29T13:16:55Z"
            }
        },
        {
            "HxkygZYBMEQjgoDtdLGC": {
                "id": "HxkygZYBMEQjgoDtdLGC",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "IBkygZYBMEQjgoDtdrFJ"
                ],
                "start": "2025-04-29T12:58:14Z",
                "end": "2025-04-29T12:58:26Z"
            }
        },
        {
            "IRkzgZYBMEQjgoDtHLGu": {
                "id": "IRkzgZYBMEQjgoDtHLGu",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "IhkzgZYBMEQjgoDtHrFn"
                ],
                "start": "2025-04-29T12:58:57Z",
                "end": "2025-04-29T12:59:09Z"
            }
        },
        {
            "JRk6gZYBMEQjgoDtA7Fv": {
                "id": "JRk6gZYBMEQjgoDtA7Fv",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/auto.csv\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Jhk6gZYBMEQjgoDtBbEx"
                ],
                "start": "2025-04-29T13:06:29Z",
                "end": "2025-04-29T13:06:41Z"
            }
        },
        {
            "Jxk9gZYBMEQjgoDthbFt": {
                "id": "Jxk9gZYBMEQjgoDthbFt",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "KBk9gZYBMEQjgoDth7Ei"
                ],
                "start": "2025-04-29T13:10:19Z",
                "end": "2025-04-29T13:10:31Z"
            }
        },
        {
            "KxlAgZYBMEQjgoDteLFB": {
                "id": "KxlAgZYBMEQjgoDteLFB",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "LBlAgZYBMEQjgoDtebHx"
                ],
                "start": "2025-04-29T13:13:33Z",
                "end": "2025-04-29T13:13:44Z"
            }
        },
        {
            "KRk-gZYBMEQjgoDtwLG0": {
                "id": "KRk-gZYBMEQjgoDtwLG0",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Khk-gZYBMEQjgoDtwrF3"
                ],
                "start": "2025-04-29T13:11:40Z",
                "end": "2025-04-29T13:11:52Z"
            }
        },
        {
            "LxlMgZYBMEQjgoDtRLGr": {
                "id": "LxlMgZYBMEQjgoDtRLGr",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MBlMgZYBMEQjgoDtRrH0"
                ],
                "start": "2025-04-29T13:26:26Z",
                "end": "2025-04-29T13:27:13Z"
            }
        },
        {
            "VBlRgZYBMEQjgoDtxrG-": {
                "id": "VBlRgZYBMEQjgoDtxrG-",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VRnhgZYBMEQjgoDtULGt": {
                "id": "VRnhgZYBMEQjgoDtULGt",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "VhnhgZYBMEQjgoDtU7EH"
                ],
                "start": "2025-04-29T16:09:13Z",
                "end": "2025-04-29T16:09:58Z"
            }
        },
        {
            "WhnigZYBMEQjgoDtP7H5": {
                "id": "WhnigZYBMEQjgoDtP7H5",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"UserInteraction.Task1\";\n    }\n\n    task Task2 {\n        implementation \"UserInteraction.Task2\";\n    }\n\n    task Task3 {\n        implementation \"UserInteraction.Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "RBlPgZYBMEQjgoDtuLFu": {
                "id": "RBlPgZYBMEQjgoDtuLFu",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> AddPadding -> SplitData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n  task AddPadding {\n    implementation \"IDEKO.AddPadding\";\n  }\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task SplitData {\n    implementation \"IDEKO.SplitData\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> AddPadding.X41;\n    ReadData.Y --> AddPadding.Y;\n    ReadData.IndicatorList --> AddPadding.IndicatorList;\n    AddPadding.XPad --> SplitData.XPad;\n    AddPadding.YPad --> SplitData.YPad;\n\n    SplitData.Timestamps41 --> TrainModel.Timestamps;\n    SplitData.Features --> TrainModel.Features;\n    SplitData.XTrain --> TrainModel.XTrain;\n    SplitData.XTest --> TrainModel.XTest;\n    SplitData.YTrain --> TrainModel.YTrain;\n    SplitData.YTest --> TrainModel.YTest;\n    SplitData.XPad --> TrainModel.XPad;\n    SplitData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(5);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "RRlPgZYBMEQjgoDtuLH-",
                    "ShlQgZYBMEQjgoDtRLGA"
                ],
                "start": "2025-04-29T14:30:12Z",
                "end": "2025-04-29T14:31:25Z"
            }
        },
        {
            "TxlQgZYBMEQjgoDtsrF3": {
                "id": "TxlQgZYBMEQjgoDtsrF3",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "UBlQgZYBMEQjgoDttLHc"
                ],
                "start": "2025-04-29T13:31:16Z",
                "end": "2025-04-29T13:32:00Z"
            }
        },
        {
            "WxnigZYBMEQjgoDtrLEy": {
                "id": "WxnigZYBMEQjgoDtrLEy",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"UserInteraction.Task1\";\n    }\n\n    task Task2 {\n        implementation \"UserInteraction.Task2\";\n    }\n\n    task Task3 {\n        implementation \"UserInteraction.Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "XBnjgZYBMEQjgoDtGLEc": {
                "id": "XBnjgZYBMEQjgoDtGLEc",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"UserInteraction.Task1\";\n    }\n\n    task Task2 {\n        implementation \"UserInteraction.Task2\";\n    }\n\n    task Task3 {\n        implementation \"UserInteraction.Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ZhmbhZYBMEQjgoDtdbHJ": {
                "id": "ZhmbhZYBMEQjgoDtdbHJ",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ZxmbhZYBMEQjgoDtd7GH"
                ],
                "start": "2025-04-30T09:31:24Z",
                "end": "2025-04-30T09:31:36Z"
            }
        },
        {
            "YhmNhZYBMEQjgoDtjrFi": {
                "id": "YhmNhZYBMEQjgoDtjrFi",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YxmNhZYBMEQjgoDtkLEu"
                ],
                "start": "2025-04-30T09:16:13Z",
                "end": "2025-04-30T09:16:25Z"
            }
        },
        {
            "XRnmgZYBMEQjgoDtVLEu": {
                "id": "XRnmgZYBMEQjgoDtVLEu",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XhnmgZYBMEQjgoDtVrEK"
                ],
                "start": "2025-04-29T16:14:42Z",
                "end": "2025-04-29T16:15:20Z"
            }
        },
        {
            "YBmMhZYBMEQjgoDtHLFz": {
                "id": "YBmMhZYBMEQjgoDtHLFz",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YRmMhZYBMEQjgoDtHrE8"
                ],
                "start": "2025-04-30T09:14:38Z",
                "end": "2025-04-30T09:14:54Z"
            }
        },
        {
            "ZBmahZYBMEQjgoDtXLG6": {
                "id": "ZBmahZYBMEQjgoDtXLG6",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ZRmahZYBMEQjgoDtXrF8"
                ],
                "start": "2025-04-30T09:30:12Z",
                "end": "2025-04-30T09:30:24Z"
            }
        },
        {
            "dhmzhZYBMEQjgoDtMbEc": {
                "id": "dhmzhZYBMEQjgoDtMbEc",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> AddPadding -> SplitData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n  task AddPadding {\n    implementation \"IDEKO.AddPadding\";\n  }\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task SplitData {\n    implementation \"IDEKO.SplitData\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> AddPadding.X41;\n    ReadData.Y --> AddPadding.Y;\n    ReadData.IndicatorList --> AddPadding.IndicatorList;\n    AddPadding.XPad --> SplitData.XPad;\n    AddPadding.YPad --> SplitData.YPad;\n\n    SplitData.Timestamps41 --> TrainModel.Timestamps;\n    SplitData.Features --> TrainModel.Features;\n    SplitData.XTrain --> TrainModel.XTrain;\n    SplitData.XTest --> TrainModel.XTest;\n    SplitData.YTrain --> TrainModel.YTrain;\n    SplitData.YTest --> TrainModel.YTest;\n    SplitData.XPad --> TrainModel.XPad;\n    SplitData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(5);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "dxmzhZYBMEQjgoDtM7Gy",
                    "fBmzhZYBMEQjgoDtvrGz"
                ],
                "start": "2025-04-30T10:57:20Z",
                "end": "2025-04-30T10:58:33Z"
            }
        },
        {
            "dBmthZYBMEQjgoDtN7HU": {
                "id": "dBmthZYBMEQjgoDtN7HU",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-30T09:50:48Z",
                "end": "2025-04-30T09:50:49Z"
            }
        },
        {
            "dRmthZYBMEQjgoDtrbHU": {
                "id": "dRmthZYBMEQjgoDtrbHU",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-30T09:51:19Z",
                "end": "2025-04-30T09:51:20Z"
            }
        },
        {
            "hBkohpYBMEQjgoDt7LHU": {
                "id": "hBkohpYBMEQjgoDt7LHU",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "hRkohpYBMEQjgoDt7bFu"
                ],
                "start": "2025-04-30T13:05:55Z",
                "end": "2025-04-30T13:06:41Z"
            }
        },
        {
            "gRn8hZYBMEQjgoDtM7GZ": {
                "id": "gRn8hZYBMEQjgoDtM7GZ",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ghn8hZYBMEQjgoDtNbF_"
                ],
                "start": "2025-04-30T11:17:04Z",
                "end": "2025-04-30T11:17:46Z"
            }
        },
        {
            "aBmehZYBMEQjgoDtoLEj": {
                "id": "aBmehZYBMEQjgoDtoLEj",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "aRmehZYBMEQjgoDtobHm"
                ],
                "start": "2025-04-30T09:34:52Z",
                "end": "2025-04-30T09:35:03Z"
            }
        },
        {
            "ahmfhZYBMEQjgoDtFrG7": {
                "id": "ahmfhZYBMEQjgoDtFrG7",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "axmfhZYBMEQjgoDtGLFz"
                ],
                "start": "2025-04-30T09:35:22Z",
                "end": "2025-04-30T09:35:34Z"
            }
        },
        {
            "bhmlhZYBMEQjgoDtXLFN": {
                "id": "bhmlhZYBMEQjgoDtXLFN",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bxmlhZYBMEQjgoDtXrED"
                ],
                "start": "2025-04-30T09:42:13Z",
                "end": "2025-04-30T09:42:24Z"
            }
        },
        {
            "cBmlhZYBMEQjgoDtt7FP": {
                "id": "cBmlhZYBMEQjgoDtt7FP",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cRmlhZYBMEQjgoDtubEI"
                ],
                "start": "2025-04-30T09:42:37Z",
                "end": "2025-04-30T09:42:48Z"
            }
        },
        {
            "bBmhhZYBMEQjgoDtXrF3": {
                "id": "bBmhhZYBMEQjgoDtXrF3",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bRmhhZYBMEQjgoDtYLFG"
                ],
                "start": "2025-04-30T09:37:51Z",
                "end": "2025-04-30T09:38:04Z"
            }
        },
        {
            "chmphZYBMEQjgoDtmLFd": {
                "id": "chmphZYBMEQjgoDtmLFd",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cxmphZYBMEQjgoDtmrEZ"
                ],
                "start": "2025-04-30T09:46:50Z",
                "end": "2025-04-30T09:47:03Z"
            }
        },
        {
            "1BlGhpYBMEQjgoDtebES": {
                "id": "1BlGhpYBMEQjgoDtebES",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "1RlGhpYBMEQjgoDtebGs"
                ],
                "start": "2025-04-30T13:38:12Z",
                "end": "2025-04-30T13:39:06Z"
            }
        },
        {
            "jBkyhpYBMEQjgoDtEbHF": {
                "id": "jBkyhpYBMEQjgoDtEbHF",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "jRkyhpYBMEQjgoDtErFs"
                ],
                "start": "2025-04-30T13:15:55Z",
                "end": "2025-04-30T13:16:42Z"
            }
        },
        {
            "lBkyhpYBMEQjgoDt2bE5": {
                "id": "lBkyhpYBMEQjgoDt2bE5",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "lRkyhpYBMEQjgoDt2bHc"
                ],
                "start": "2025-04-30T13:16:46Z",
                "end": "2025-04-30T13:17:34Z"
            }
        },
        {
            "pBk0hpYBMEQjgoDttLGb": {
                "id": "pBk0hpYBMEQjgoDttLGb",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "pRk0hpYBMEQjgoDttbE4"
                ],
                "start": "2025-04-30T13:18:48Z",
                "end": "2025-04-30T13:19:37Z"
            }
        },
        {
            "rBk2hpYBMEQjgoDtabH7": {
                "id": "rBk2hpYBMEQjgoDtabH7",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "rRk2hpYBMEQjgoDtarGf"
                ],
                "start": "2025-04-30T13:20:40Z",
                "end": "2025-04-30T13:21:28Z"
            }
        },
        {
            "vBk_hpYBMEQjgoDtSrEn": {
                "id": "vBk_hpYBMEQjgoDtSrEn",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "vRk_hpYBMEQjgoDtSrHI"
                ],
                "start": "2025-04-30T13:30:21Z",
                "end": "2025-04-30T13:31:13Z"
            }
        },
        {
            "xBlAhpYBMEQjgoDtdrG4": {
                "id": "xBlAhpYBMEQjgoDtdrG4",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "xRlAhpYBMEQjgoDtd7FZ"
                ],
                "start": "2025-04-30T13:31:38Z",
                "end": "2025-04-30T13:32:27Z"
            }
        },
        {
            "zBlEhpYBMEQjgoDtYbGW": {
                "id": "zBlEhpYBMEQjgoDtYbGW",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "zRlEhpYBMEQjgoDtYrEp"
                ],
                "start": "2025-04-30T13:35:55Z",
                "end": "2025-04-30T13:36:46Z"
            }
        },
        {
            "tBk7hpYBMEQjgoDtWrF-": {
                "id": "tBk7hpYBMEQjgoDtWrF-",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "tRk7hpYBMEQjgoDtW7Ef"
                ],
                "start": "2025-04-30T13:26:03Z",
                "end": "2025-04-30T13:26:58Z"
            }
        },
        {
            "nBkzhpYBMEQjgoDtorFE": {
                "id": "nBkzhpYBMEQjgoDtorFE",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "nRkzhpYBMEQjgoDtorHr"
                ],
                "start": "2025-04-30T13:17:37Z",
                "end": "2025-04-30T13:18:27Z"
            }
        },
        {
            "5BlRhpYBMEQjgoDtrrGD": {
                "id": "5BlRhpYBMEQjgoDtrrGD",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "5RlRhpYBMEQjgoDtr7Ed"
                ],
                "start": "2025-04-30T13:50:27Z",
                "end": "2025-04-30T13:51:42Z"
            }
        },
        {
            "3BlJhpYBMEQjgoDtK7GF": {
                "id": "3BlJhpYBMEQjgoDtK7GF",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "3RlJhpYBMEQjgoDtLLEl"
                ],
                "start": "2025-04-30T13:41:09Z",
                "end": "2025-04-30T13:42:00Z"
            }
        },
        {
            "FBlchpYBMEQjgoDtALIu": {
                "id": "FBlchpYBMEQjgoDtALIu",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "FRlchpYBMEQjgoDtALLT",
                    "HBlchpYBMEQjgoDtAbLW"
                ],
                "start": "2025-04-30T14:01:43Z",
                "end": "2025-04-30T14:03:35Z"
            }
        },
        {
            "7BlThpYBMEQjgoDtdrER": {
                "id": "7BlThpYBMEQjgoDtdrER",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "7RlThpYBMEQjgoDtdrG0"
                ],
                "start": "2025-04-30T13:52:23Z",
                "end": "2025-04-30T13:53:20Z"
            }
        },
        {
            "9BlUhpYBMEQjgoDtfLFo": {
                "id": "9BlUhpYBMEQjgoDtfLFo",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "9RlUhpYBMEQjgoDtfbEC"
                ],
                "start": "2025-04-30T13:53:30Z",
                "end": "2025-04-30T13:54:22Z"
            }
        },
        {
            "DBlahpYBMEQjgoDtqbIq": {
                "id": "DBlahpYBMEQjgoDtqbIq",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "DRlahpYBMEQjgoDtqbLP"
                ],
                "start": "2025-04-30T14:00:15Z",
                "end": "2025-04-30T14:01:11Z"
            }
        },
        {
            "_BlWhpYBMEQjgoDtZrEY": {
                "id": "_BlWhpYBMEQjgoDtZrEY",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "_RlWhpYBMEQjgoDtZrG1"
                ],
                "start": "2025-04-30T13:55:36Z",
                "end": "2025-04-30T13:56:32Z"
            }
        },
        {
            "BBlZhpYBMEQjgoDtErIR": {
                "id": "BBlZhpYBMEQjgoDtErIR",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "BRlZhpYBMEQjgoDtErLa"
                ],
                "start": "2025-04-30T13:58:31Z",
                "end": "2025-04-30T13:59:29Z"
            }
        },
        {
            "PWydhpYBSvJv8IAYCihu": {
                "id": "PWydhpYBSvJv8IAYCihu",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n    define output data DatasetsFolder;\n\n    split_dataset.OutputFolder --> DatasetsFolder\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    configure data DatasetsFolder {\n    path \"output/datasets/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "RhlkhpYBMEQjgoDttbJ_": {
                "id": "RhlkhpYBMEQjgoDttbJ_",
                "name": "test-workflow-query",
                "status": "new",
                "workflow_ids": [
                    "RxllhpYBMEQjgoDtBrLF"
                ]
            }
        },
        {
            "NhljhpYBMEQjgoDtfrIS": {
                "id": "NhljhpYBMEQjgoDtfrIS",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "NxljhpYBMEQjgoDtfrKv",
                    "PhljhpYBMEQjgoDtf7Ku"
                ],
                "start": "2025-04-30T14:09:54Z",
                "end": "2025-04-30T14:11:37Z"
            }
        },
        {
            "PGychpYBSvJv8IAYhSiH": {
                "id": "PGychpYBSvJv8IAYhSiH",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n    define output data DatasetsFolder;\n\n    split_dataset.OutputFolder --> DatasetsFolder\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    configure data DatasetsFolder {\n    path \"output/datasets/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "MhlfhpYBMEQjgoDtsbL2": {
                "id": "MhlfhpYBMEQjgoDtsbL2",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-30T13:05:45Z",
                "end": "2025-04-30T13:05:46Z"
            }
        },
        {
            "IxlehpYBMEQjgoDtaLK6": {
                "id": "IxlehpYBMEQjgoDtaLK6",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "JBlehpYBMEQjgoDtabJU",
                    "KxlehpYBMEQjgoDtarJU"
                ],
                "start": "2025-04-30T14:04:21Z",
                "end": "2025-04-30T14:06:03Z"
            }
        },
        {
            "MxlfhpYBMEQjgoDt-LJp": {
                "id": "MxlfhpYBMEQjgoDt-LJp",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-30T13:06:03Z",
                "end": "2025-04-30T13:06:04Z"
            }
        },
        {
            "NBlhhpYBMEQjgoDtqLL1": {
                "id": "NBlhhpYBMEQjgoDtqLL1",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-30T13:07:54Z",
                "end": "2025-04-30T13:07:55Z"
            }
        },
        {
            "NRlihpYBMEQjgoDtZLIg": {
                "id": "NRlihpYBMEQjgoDtZLIg",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-30T13:08:42Z",
                "end": "2025-04-30T13:08:43Z"
            }
        },
        {
            "SBllhpYBMEQjgoDt6LKR": {
                "id": "SBllhpYBMEQjgoDt6LKR",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "SRllhpYBMEQjgoDt6rJN"
                ],
                "start": "2025-04-30T13:12:32Z",
                "end": "2025-04-30T13:12:45Z"
            }
        },
        {
            "ShlmhpYBMEQjgoDtObJF": {
                "id": "ShlmhpYBMEQjgoDtObJF",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "SxlmhpYBMEQjgoDtOrL1"
                ],
                "start": "2025-04-30T13:12:53Z",
                "end": "2025-04-30T13:13:05Z"
            }
        },
        {
            "WxlrhpYBMEQjgoDtorJB": {
                "id": "WxlrhpYBMEQjgoDtorJB",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "XBlrhpYBMEQjgoDtorLa",
                    "YxlrhpYBMEQjgoDto7Ls"
                ],
                "start": "2025-04-30T14:18:47Z",
                "end": "2025-04-30T14:20:37Z"
            }
        },
        {
            "TBlqhpYBMEQjgoDtdLIz": {
                "id": "TBlqhpYBMEQjgoDtdLIz",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "TRlqhpYBMEQjgoDtdLLV",
                    "VBlqhpYBMEQjgoDtdbLn"
                ],
                "start": "2025-04-30T14:17:30Z"
            }
        },
        {
            "RRlkhpYBMEQjgoDtBbKZ": {
                "id": "RRlkhpYBMEQjgoDtBbKZ",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-04-30T13:10:28Z",
                "end": "2025-04-30T13:10:30Z"
            }
        },
        {
            "pzWhpZYBc0POvO1cnr2z": {
                "id": "pzWhpZYBc0POvO1cnr2z",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "qDWhpZYBc0POvO1cob34",
                    "rDW1pZYBc0POvO1cNb2E",
                    "sDW9pZYBc0POvO1ck72D"
                ],
                "start": "2025-05-06T14:45:55Z"
            }
        },
        {
            "nDWdpZYBc0POvO1cmL15": {
                "id": "nDWdpZYBc0POvO1cmL15",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    //START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    //S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> T2 -> I1 -> END;\n    //V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "nTWdpZYBc0POvO1cnL0S"
                ],
                "start": "2025-05-06T14:41:31Z"
            }
        },
        {
            "oTWfpZYBc0POvO1cpb2-": {
                "id": "oTWfpZYBc0POvO1cpb2-",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    //START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    //S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> T2 -> I1 -> END;\n    //V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ojWfpZYBc0POvO1cqb1p",
                    "pjWgpZYBc0POvO1cab1M"
                ],
                "start": "2025-05-06T14:43:45Z"
            }
        },
        {
            "PmydhpYBSvJv8IAYbyj3": {
                "id": "PmydhpYBSvJv8IAYbyj3",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data TrainedModelFolder;\n    define output data DatasetsFolder;\n\n    split_dataset.OutputFolder --> DatasetsFolder;\n    train_model.OutputFolder --> TrainedModelFolder;\n    Explainability.OutputFolder --> TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    }\n\n    configure data DatasetsFolder {\n    path \"output/datasets/**\";\n    }\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.train_data;\n    split_dataset.test_data --> Explainability.test_data;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "P2ydhpYBSvJv8IAYcCjw",
                    "RmydhpYBSvJv8IAYcij0"
                ],
                "start": "2025-04-30T15:13:11Z",
                "end": "2025-04-30T15:15:11Z"
            }
        },
        {
            "tDW-pZYBc0POvO1cLL0_": {
                "id": "tDW-pZYBc0POvO1cLL0_",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "tTW-pZYBc0POvO1cMr1s"
                ],
                "start": "2025-05-06T16:17:07Z",
                "end": "2025-05-06T16:17:13Z"
            }
        },
        {
            "UWzhkJYBSvJv8IAYmyiW": {
                "id": "UWzhkJYBSvJv8IAYmyiW",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "UmzhkJYBSvJv8IAYoihZ"
                ],
                "start": "2025-05-02T14:03:51Z",
                "end": "2025-05-02T14:04:07Z"
            }
        },
        {
            "TWzakJYBSvJv8IAY5ijd": {
                "id": "TWzakJYBSvJv8IAY5ijd",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "TmzakJYBSvJv8IAY8Shu"
                ],
                "start": "2025-05-02T13:56:32Z",
                "end": "2025-05-02T13:56:48Z"
            }
        },
        {
            "T2zckJYBSvJv8IAY9Cgd": {
                "id": "T2zckJYBSvJv8IAY9Cgd",
                "name": "new_exp",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    define output data TrainedModelFolder;\n\n  Task1.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "UGzckJYBSvJv8IAY-Cj0"
                ],
                "start": "2025-05-02T13:58:46Z",
                "end": "2025-05-02T13:59:26Z"
            }
        },
        {
            "zDXPpZYBc0POvO1c8r2o": {
                "id": "zDXPpZYBc0POvO1c8r2o",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "zTXPpZYBc0POvO1c9b3H",
                    "0TXQpZYBc0POvO1cor0b",
                    "1TXRpZYBc0POvO1cPL2N",
                    "2TXRpZYBc0POvO1c9b1m"
                ],
                "start": "2025-05-06T15:36:31Z"
            }
        },
        {
            "uTXDpZYBc0POvO1cv70I": {
                "id": "uTXDpZYBc0POvO1cv70I",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ujXDpZYBc0POvO1cwr0f",
                    "vjXEpZYBc0POvO1car3x",
                    "wjXFpZYBc0POvO1cHr1x",
                    "xjXFpZYBc0POvO1cyL2f"
                ],
                "start": "2025-05-06T15:23:11Z"
            }
        },
        {
            "xzXOpZYBc0POvO1cqr0Y": {
                "id": "xzXOpZYBc0POvO1cqr0Y",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "yDXOpZYBc0POvO1cq70e"
                ],
                "start": "2025-05-06T16:35:07Z",
                "end": "2025-05-06T16:35:10Z"
            }
        },
        {
            "2jXipZYBc0POvO1c9r13": {
                "id": "2jXipZYBc0POvO1c9r13",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "2zXipZYBc0POvO1c971_"
                ],
                "start": "2025-05-06T16:57:17Z",
                "end": "2025-05-06T16:57:20Z"
            }
        },
        {
            "3zXrpZYBc0POvO1c0b3I": {
                "id": "3zXrpZYBc0POvO1c0b3I",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "4DXrpZYBc0POvO1c0r3t"
                ],
                "start": "2025-05-06T17:06:57Z",
                "end": "2025-05-06T17:07:02Z"
            }
        },
        {
            "6TUDppYBc0POvO1cWb0Y": {
                "id": "6TUDppYBc0POvO1cWb0Y",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6jUDppYBc0POvO1cWr3g"
                ],
                "start": "2025-05-06T17:32:40Z",
                "end": "2025-05-06T17:32:43Z"
            }
        },
        {
            "8zUKppYBc0POvO1cm73e": {
                "id": "8zUKppYBc0POvO1cm73e",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9DUKppYBc0POvO1cnr0U"
                ],
                "start": "2025-05-06T17:40:36Z",
                "end": "2025-05-06T17:40:38Z"
            }
        },
        {
            "7jUJppYBc0POvO1cH70s": {
                "id": "7jUJppYBc0POvO1cH70s",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "7zUJppYBc0POvO1cIL3w"
                ],
                "start": "2025-05-06T17:38:58Z",
                "end": "2025-05-06T17:39:02Z"
            }
        },
        {
            "5DUAppYBc0POvO1cqr0A": {
                "id": "5DUAppYBc0POvO1cqr0A",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5TUAppYBc0POvO1crL0o"
                ],
                "start": "2025-05-06T17:29:44Z",
                "end": "2025-05-06T17:29:48Z"
            }
        },
        {
            "_TV-qZYBc0POvO1cVr12": {
                "id": "_TV-qZYBc0POvO1cVr12",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    //START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    //S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> T2 -> I1 -> END;\n    //V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_jV-qZYBc0POvO1cWr1n"
                ],
                "start": "2025-05-07T08:45:53Z"
            }
        },
        {
            "DDWSqZYBc0POvO1cub4Q": {
                "id": "DDWSqZYBc0POvO1cub4Q",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.X_train;\n    split_dataset.test_data --> Explainability.X_test;\n    split_dataset.train_labels --> Explainability.Y_train;\n    split_dataset.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "GzWVqZYBc0POvO1cOb5J": {
                "id": "GzWVqZYBc0POvO1cOb5J",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    train_model.model --> Explainability.model;\n    split_dataset.train_data --> Explainability.X_train;\n    split_dataset.test_data --> Explainability.X_test;\n    split_dataset.train_labels --> Explainability.Y_train;\n    split_dataset.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "BzWJqZYBc0POvO1c475B": {
                "id": "BzWJqZYBc0POvO1c475B",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CDWJqZYBc0POvO1c5L5D"
                ],
                "start": "2025-05-07T09:58:28Z",
                "end": "2025-05-07T09:58:30Z"
            }
        },
        {
            "HDWXqZYBc0POvO1cS76O": {
                "id": "HDWXqZYBc0POvO1cS76O",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "HTWXqZYBc0POvO1cTL6c"
                ],
                "start": "2025-05-07T10:13:07Z",
                "end": "2025-05-07T10:13:43Z"
            }
        },
        {
            "-DV0qZYBc0POvO1cMr1U": {
                "id": "-DV0qZYBc0POvO1cMr1U",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-TV0qZYBc0POvO1cM71l"
                ],
                "start": "2025-05-07T09:34:46Z",
                "end": "2025-05-07T09:34:49Z"
            }
        },
        {
            "DTWSqZYBc0POvO1cz76n": {
                "id": "DTWSqZYBc0POvO1cz76n",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DjWSqZYBc0POvO1c0b4J"
                ],
                "start": "2025-05-07T10:08:13Z",
                "end": "2025-05-07T10:08:16Z"
            }
        },
        {
            "EjWSqZYBc0POvO1c6b6p": {
                "id": "EjWSqZYBc0POvO1c6b6p",
                "name": "I2CAT_workflow1",
                "model": "workflow uc2_workflow1 {\n\n    START -> ReadData -> Partitioning -> ModelTrain -> ModelPredict -> END;\n\n    task ReadData {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task Partitioning {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task ModelTrain;\n\n    task ModelPredict {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> ReadData.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n}\n\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task ModelTrain {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    //Automated\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth_vp = range(3, 6,3);\n        param n_estimators_vp = range(5, 11,5);\n\n        task ModelTrain {\n          param max_depth = max_depth_vp;\n          param n_estimators = n_estimators_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "EzWSqZYBc0POvO1c6r7Q",
                    "FzWSqZYBc0POvO1c7L6Q"
                ],
                "start": "2025-05-07T10:08:19Z",
                "end": "2025-05-07T10:09:46Z"
            }
        },
        {
            "AjWHqZYBc0POvO1cG74l": {
                "id": "AjWHqZYBc0POvO1cG74l",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "AzWHqZYBc0POvO1cHL5F"
                ],
                "start": "2025-05-07T09:55:26Z",
                "end": "2025-05-07T09:55:28Z"
            }
        },
        {
            "hzW0qZYBc0POvO1c075k": {
                "id": "hzW0qZYBc0POvO1c075k",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    train_model.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "iDW0qZYBc0POvO1c1b66",
                    "jzW0qZYBc0POvO1c2L7V"
                ],
                "start": "2025-05-07T10:45:22Z",
                "end": "2025-05-07T10:52:01Z"
            }
        },
        {
            "VDWtqZYBc0POvO1cTL6Q": {
                "id": "VDWtqZYBc0POvO1cTL6Q",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "VTWtqZYBc0POvO1cTr6d",
                    "aDWtqZYBc0POvO1clb7y"
                ],
                "start": "2025-05-07T10:37:09Z",
                "end": "2025-05-07T10:37:33Z"
            }
        },
        {
            "WTWtqZYBc0POvO1caL6x": {
                "id": "WTWtqZYBc0POvO1caL6x",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"output/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "WjWtqZYBc0POvO1cab7U",
                    "YTWtqZYBc0POvO1cbL5U"
                ],
                "start": "2025-05-07T10:37:15Z",
                "end": "2025-05-07T10:39:27Z"
            }
        },
        {
            "eDWyqZYBc0POvO1co767": {
                "id": "eDWyqZYBc0POvO1co767",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    train_model.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "eTWyqZYBc0POvO1cpL7O",
                    "gDWyqZYBc0POvO1cp76D"
                ],
                "start": "2025-05-07T10:42:58Z",
                "end": "2025-05-07T10:45:08Z"
            }
        },
        {
            "ljW3qZYBc0POvO1cIL5K": {
                "id": "ljW3qZYBc0POvO1cIL5K",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "MDWlqZYBc0POvO1cUb7W": {
                "id": "MDWlqZYBc0POvO1cUb7W",
                "name": "main_binary",
                "model": "package binary;\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> ValidateModel -> UserIteration -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task ValidateModel {\n    implementation \"binary.ValidateModel\";\n  }\n\n  task UserIteration {\n    implementation \"binary.UserIteration\";\n  }\n\n  task PrepareData {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  // DATA VALIDATION\n  define input data ValidationDataFile;\n\n  // DATA CONNECTIONS\n  ValidationDataFile --> ValidateModel.FileToValidate;\n  ValidationDataFile --> UserIteration.FileToValidate;\n\n  configure data ValidationDataFile {\n    path \"validation/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.model_path --> EvaluateModel.model_path;\n\n    EvaluateModel.OutputFolder --> ValidateModel.OutputFolder;\n}\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModel {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModel {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModel {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    // S1 -> S2 -> S3;\n    //START -> S1 -> S2 -> S3 -> END;\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2 of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3 of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MTWlqZYBc0POvO1cU74T"
                ],
                "start": "2025-05-07T10:28:26Z",
                "end": "2025-05-07T10:44:49Z"
            }
        },
        {
            "NjWmqZYBc0POvO1cjr4_": {
                "id": "NjWmqZYBc0POvO1cjr4_",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "NzWmqZYBc0POvO1cj75T",
                    "PjWmqZYBc0POvO1ckr48"
                ],
                "start": "2025-05-07T10:29:46Z",
                "end": "2025-05-07T10:31:55Z"
            }
        },
        {
            "ITWiqZYBc0POvO1ci74g": {
                "id": "ITWiqZYBc0POvO1ci74g",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "IjWiqZYBc0POvO1cjL5A",
                    "KTWiqZYBc0POvO1cjr7u"
                ],
                "start": "2025-05-07T10:25:23Z",
                "end": "2025-05-07T10:27:51Z"
            }
        },
        {
            "RTWrqZYBc0POvO1cS74W": {
                "id": "RTWrqZYBc0POvO1cS74W",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "RjWrqZYBc0POvO1cTL41",
                    "TTWrqZYBc0POvO1cTr7s"
                ],
                "start": "2025-05-07T10:34:57Z",
                "end": "2025-05-07T10:37:02Z"
            }
        },
        {
            "aTWwqZYBc0POvO1cbL74": {
                "id": "aTWwqZYBc0POvO1cbL74",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    train_model.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "ajWwqZYBc0POvO1cbr7q",
                    "cTWwqZYBc0POvO1ccb6k"
                ],
                "start": "2025-05-07T10:40:33Z",
                "end": "2025-05-07T10:42:40Z"
            }
        },
        {
            "nTW8qZYBc0POvO1czL53": {
                "id": "nTW8qZYBc0POvO1czL53",
                "name": "IDEKO_main_oneRun",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  configure task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  configure task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    configure task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "lzW8qZYBc0POvO1cpL4Y": {
                "id": "lzW8qZYBc0POvO1cpL4Y",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "mDW8qZYBc0POvO1cpr49",
                    "nDW8qZYBc0POvO1cxL55"
                ],
                "start": "2025-05-07T10:53:54Z",
                "end": "2025-05-07T10:54:08Z"
            }
        },
        {
            "njXBqZYBc0POvO1cML6W": {
                "id": "njXBqZYBc0POvO1cML6W",
                "name": "IDEKO_main_oneRun",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  configure task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  configure task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    configure task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "nzXBqZYBc0POvO1ciL62": {
                "id": "nzXBqZYBc0POvO1ciL62",
                "name": "IDEKO_main_oneRun",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    configure task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "oDXBqZYBc0POvO1ctb7b": {
                "id": "oDXBqZYBc0POvO1ctb7b",
                "name": "IDEKO_main_oneRun",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "oTXCqZYBc0POvO1cDL4d": {
                "id": "oTXCqZYBc0POvO1cDL4d",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "vTXFqZYBc0POvO1c9L6L": {
                "id": "vTXFqZYBc0POvO1c9L6L",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "vjXFqZYBc0POvO1c9r6K",
                    "wjXFqZYBc0POvO1c-b4X",
                    "xjXHqZYBc0POvO1cbr6D"
                ],
                "start": "2025-05-07T10:04:05Z",
                "end": "2025-05-07T10:06:28Z"
            }
        },
        {
            "tDXEqZYBc0POvO1c5r4g": {
                "id": "tDXEqZYBc0POvO1c5r4g",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "tTXEqZYBc0POvO1c6L4Y",
                    "uTXEqZYBc0POvO1c7r7E"
                ],
                "start": "2025-05-07T10:02:55Z"
            }
        },
        {
            "ojXDqZYBc0POvO1cD766": {
                "id": "ojXDqZYBc0POvO1cD766",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "ozXDqZYBc0POvO1cEb7N",
                    "pzXDqZYBc0POvO1cFL5Z"
                ],
                "start": "2025-05-07T10:00:55Z"
            }
        },
        {
            "qzXDqZYBc0POvO1c9L7i": {
                "id": "qzXDqZYBc0POvO1c9L7i",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "rDXDqZYBc0POvO1c9r7o",
                    "sDXDqZYBc0POvO1c_b6F"
                ],
                "start": "2025-05-07T10:01:54Z"
            }
        },
        {
            "yjUVqpYBc0POvO1c2L6T": {
                "id": "yjUVqpYBc0POvO1c2L6T",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "yzUVqpYBc0POvO1c2r6u",
                    "zzUVqpYBc0POvO1c3r7M",
                    "0zUXqpYBc0POvO1cjr5n"
                ],
                "start": "2025-05-07T11:31:21Z"
            }
        },
        {
            "4DUgqpYBc0POvO1cHL41": {
                "id": "4DUgqpYBc0POvO1cHL41",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "4TUgqpYBc0POvO1cHr5U",
                    "5TUgqpYBc0POvO1cIb4D",
                    "6TUhqpYBc0POvO1ct77L"
                ],
                "start": "2025-05-07T11:42:33Z"
            }
        },
        {
            "1zUdqpYBc0POvO1cWb4D": {
                "id": "1zUdqpYBc0POvO1cWb4D",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"IDEKO_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "2DUdqpYBc0POvO1cW74F",
                    "3DUdqpYBc0POvO1cXb6i"
                ],
                "start": "2025-05-07T11:39:32Z"
            }
        },
        {
            "8zUmqpYBc0POvO1c-r4u": {
                "id": "8zUmqpYBc0POvO1c-r4u",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9DUmqpYBc0POvO1c_L75",
                    "-DUnqpYBc0POvO1cUr5Y"
                ],
                "start": "2025-05-07T12:50:03Z",
                "end": "2025-05-07T12:50:32Z"
            }
        },
        {
            "-TUnqpYBc0POvO1csr5c": {
                "id": "-TUnqpYBc0POvO1csr5c",
                "name": "user_interaction_example",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "-jUoqpYBc0POvO1ckr5Q": {
                "id": "-jUoqpYBc0POvO1ckr5Q",
                "name": "user_interaction_example",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"UserInteraction.Task1\";\n    }\n\n    task Task2 {\n        implementation \"UserInteraction.Task2\";\n    }\n\n    task Task3 {\n        implementation \"UserInteraction.Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "FjVkqpYBc0POvO1cx780": {
                "id": "FjVkqpYBc0POvO1cx780",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "FzVkqpYBc0POvO1cyr9Q",
                    "GzVlqpYBc0POvO1cZr93"
                ],
                "start": "2025-05-07T13:57:34Z"
            }
        },
        {
            "EDVGqpYBc0POvO1c4L-0": {
                "id": "EDVGqpYBc0POvO1c4L-0",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ETVGqpYBc0POvO1c47-q",
                    "FTVHqpYBc0POvO1ccr9v"
                ],
                "start": "2025-05-07T13:24:54Z"
            }
        },
        {
            "BDU7qpYBc0POvO1cM78g": {
                "id": "BDU7qpYBc0POvO1cM78g",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "BTU7qpYBc0POvO1cNr8x",
                    "CTU7qpYBc0POvO1ckb-g"
                ],
                "start": "2025-05-07T13:12:09Z"
            }
        },
        {
            "CjVFqpYBc0POvO1crb_S": {
                "id": "CjVFqpYBc0POvO1crb_S",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CzVFqpYBc0POvO1csL-n",
                    "DzVGqpYBc0POvO1cE78j"
                ],
                "start": "2025-05-07T13:23:35Z"
            }
        },
        {
            "7TUkqpYBc0POvO1cuL6O": {
                "id": "7TUkqpYBc0POvO1cuL6O",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "7jUkqpYBc0POvO1cu77H",
                    "8jUlqpYBc0POvO1cEr7S"
                ],
                "start": "2025-05-07T12:47:35Z",
                "end": "2025-05-07T12:48:05Z"
            }
        },
        {
            "_jU4qpYBc0POvO1cIL70": {
                "id": "_jU4qpYBc0POvO1cIL70",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_zU4qpYBc0POvO1cI773",
                    "AzU4qpYBc0POvO1chL-t"
                ],
                "start": "2025-05-07T13:08:47Z",
                "end": "2025-05-07T13:09:18Z"
            }
        },
        {
            "-zUoqpYBc0POvO1c-761": {
                "id": "-zUoqpYBc0POvO1c-761",
                "name": "user_interaction_example",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "_DUoqpYBc0POvO1c_b5v"
                ],
                "start": "2025-05-07T11:52:14Z",
                "end": "2025-05-07T11:53:30Z"
            }
        },
        {
            "HDVyqpYBc0POvO1c-L-i": {
                "id": "HDVyqpYBc0POvO1c-L-i",
                "name": "IDEKO_main",
                "model": "package IDEKO;\n\nworkflow IDEKO_multi_main {\n\n  // Task CONNECTIONS\n  START -> ReadData -> AddPadding -> SplitData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"IDEKO.ReadData\";\n  }\n  task AddPadding {\n    implementation \"IDEKO.AddPadding\";\n  }\n  task EvaluateModel {\n    implementation \"IDEKO.EvaluateModel\";\n  }\n\n  task SplitData {\n    implementation \"IDEKO.SplitData\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"v1/ideko-subset/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n  }\n\n    ReadData.X41 --> AddPadding.X41;\n    ReadData.Y --> AddPadding.Y;\n    ReadData.IndicatorList --> AddPadding.IndicatorList;\n    AddPadding.XPad --> SplitData.XPad;\n    AddPadding.YPad --> SplitData.YPad;\n\n    SplitData.Timestamps41 --> TrainModel.Timestamps;\n    SplitData.Features --> TrainModel.Features;\n    SplitData.XTrain --> TrainModel.XTrain;\n    SplitData.XTest --> TrainModel.XTest;\n    SplitData.YTrain --> TrainModel.YTrain;\n    SplitData.YTest --> TrainModel.YTest;\n    SplitData.XPad --> TrainModel.XPad;\n    SplitData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n\n}\n\nworkflow TrainModelNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN from IDEKO_multi_main {\n  task TrainModel {\n    implementation \"IDEKO.TrainModelRNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    S1 -> S2;\n  }\n\n  space S1 of TrainModelNN {\n    strategy gridsearch;\n    param epochs_vp = enum(5);\n    // param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n\n  space S2 of TrainModelRNN {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n    }\n  }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "HTVyqpYBc0POvO1c-b-5"
                ],
                "start": "2025-05-07T14:13:03Z"
            }
        },
        {
            "IjVzqpYBc0POvO1cB7_a": {
                "id": "IjVzqpYBc0POvO1cB7_a",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    train_model.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "IzVzqpYBc0POvO1cCL_c",
                    "KjVzqpYBc0POvO1cC7-D"
                ],
                "start": "2025-05-07T14:13:07Z"
            }
        },
        {
            "XjWDqpYBc0POvO1cr7_O": {
                "id": "XjWDqpYBc0POvO1cr7_O",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "XzWDqpYBc0POvO1csL_M",
                    "ZjWDqpYBc0POvO1cs79R"
                ],
                "start": "2025-05-07T14:31:18Z"
            }
        },
        {
            "QDV2qpYBc0POvO1c2b88": {
                "id": "QDV2qpYBc0POvO1c2b88",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    train_model.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "QTV2qpYBc0POvO1c2r84",
                    "SDV2qpYBc0POvO1c3b9_"
                ],
                "start": "2025-05-07T14:17:17Z"
            }
        },
        {
            "TzV_qpYBc0POvO1cRb-0": {
                "id": "TzV_qpYBc0POvO1cRb-0",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "UDV_qpYBc0POvO1cRr_B",
                    "VzV_qpYBc0POvO1cSr8_"
                ],
                "start": "2025-05-07T14:26:29Z"
            }
        },
        {
            "MTV0qpYBc0POvO1ciL-q": {
                "id": "MTV0qpYBc0POvO1ciL-q",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    train_model.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.X_train;\n    benchmark_model.test_data --> Explainability.X_test;\n    benchmark_model.train_labels --> Explainability.Y_train;\n    benchmark_model.test_labels --> Explainability.Y_test;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "MjV0qpYBc0POvO1cib_A",
                    "OTV0qpYBc0POvO1cjL9o"
                ],
                "start": "2025-05-07T14:14:45Z"
            }
        },
        {
            "bTWIqpYBc0POvO1cv79f": {
                "id": "bTWIqpYBc0POvO1cv79f",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "bjWIqpYBc0POvO1cwL9l",
                    "dTWIqpYBc0POvO1cwr_g"
                ],
                "start": "2025-05-07T14:36:50Z",
                "end": "2025-05-07T14:38:40Z"
            }
        },
        {
            "fDWKqpYBc0POvO1c2r_R": {
                "id": "fDWKqpYBc0POvO1c2r_R",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "fTWKqpYBc0POvO1c27-9",
                    "hDWKqpYBc0POvO1c3r_U"
                ],
                "start": "2025-05-07T14:39:08Z",
                "end": "2025-05-07T14:40:34Z"
            }
        },
        {
            "qTWUqpYBc0POvO1cMb_C": {
                "id": "qTWUqpYBc0POvO1cMb_C",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "qjWUqpYBc0POvO1cMr_i",
                    "sTWUqpYBc0POvO1cNb-9"
                ],
                "start": "2025-05-07T14:49:20Z",
                "end": "2025-05-07T14:51:24Z"
            }
        },
        {
            "uDWUqpYBc0POvO1c-r8H": {
                "id": "uDWUqpYBc0POvO1c-r8H",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "uTWUqpYBc0POvO1c_L-H",
                    "vTWVqpYBc0POvO1cnb8r"
                ],
                "start": "2025-05-07T14:50:12Z"
            }
        },
        {
            "-jWoqpYBc0POvO1cbL99": {
                "id": "-jWoqpYBc0POvO1cbL99",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "-zWoqpYBc0POvO1cbb-E",
                    "AjWoqpYBc0POvO1cccAi"
                ],
                "start": "2025-05-07T15:11:26Z",
                "end": "2025-05-07T15:13:28Z"
            }
        },
        {
            "mjWPqpYBc0POvO1c_r-0": {
                "id": "mjWPqpYBc0POvO1c_r-0",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "mzWPqpYBc0POvO1c_7_u",
                    "ojWQqpYBc0POvO1cA7-6"
                ],
                "start": "2025-05-07T14:44:45Z",
                "end": "2025-05-07T14:46:36Z"
            }
        },
        {
            "vjWXqpYBc0POvO1cR790": {
                "id": "vjWXqpYBc0POvO1cR790",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "vzWXqpYBc0POvO1cSr_I",
                    "xjWXqpYBc0POvO1cTb9w"
                ],
                "start": "2025-05-07T14:52:42Z",
                "end": "2025-05-07T14:54:33Z"
            }
        },
        {
            "zTWaqpYBc0POvO1cy789": {
                "id": "zTWaqpYBc0POvO1cy789",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "zjWaqpYBc0POvO1czL90",
                    "1TWaqpYBc0POvO1cz78X"
                ],
                "start": "2025-05-07T14:56:33Z",
                "end": "2025-05-07T14:58:19Z"
            }
        },
        {
            "3DWfqpYBc0POvO1c-L9s": {
                "id": "3DWfqpYBc0POvO1c-L9s",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "3TWfqpYBc0POvO1c-b-C",
                    "5DWfqpYBc0POvO1c-7_t"
                ],
                "start": "2025-05-07T15:02:12Z",
                "end": "2025-05-07T15:03:55Z"
            }
        },
        {
            "6zWkqpYBc0POvO1ceL-f": {
                "id": "6zWkqpYBc0POvO1ceL-f",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "7DWkqpYBc0POvO1ceb_C",
                    "8zWkqpYBc0POvO1cfL94"
                ],
                "start": "2025-05-07T15:07:07Z",
                "end": "2025-05-07T15:08:56Z"
            }
        },
        {
            "izWMqpYBc0POvO1c1L_A": {
                "id": "izWMqpYBc0POvO1c1L_A",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "jDWMqpYBc0POvO1c1b_O",
                    "kzWMqpYBc0POvO1c2L-P"
                ],
                "start": "2025-05-07T14:41:18Z",
                "end": "2025-05-07T14:43:10Z"
            }
        },
        {
            "dDXWqpYBc0POvO1cBsDa": {
                "id": "dDXWqpYBc0POvO1cBsDa",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "czXVqpYBc0POvO1c_8C0": {
                "id": "czXVqpYBc0POvO1c_8C0",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "VDXFqpYBc0POvO1cAsCl": {
                "id": "VDXFqpYBc0POvO1cAsCl",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "VTXFqpYBc0POvO1cesDR": {
                "id": "VTXFqpYBc0POvO1cesDR",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "VjXFqpYBc0POvO1ce8DR",
                    "XTXFqpYBc0POvO1cfsBv"
                ],
                "start": "2025-05-07T15:43:10Z"
            }
        },
        {
            "JzW5qpYBc0POvO1cl8BN": {
                "id": "JzW5qpYBc0POvO1cl8BN",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "KDW5qpYBc0POvO1cmMBT",
                    "LzW5qpYBc0POvO1cm8AK"
                ],
                "start": "2025-05-07T15:30:11Z",
                "end": "2025-05-07T15:32:18Z"
            }
        },
        {
            "ZDXGqpYBc0POvO1c1cAc": {
                "id": "ZDXGqpYBc0POvO1c1cAc",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "ZTXGqpYBc0POvO1c1sAk",
                    "bDXGqpYBc0POvO1c2MD3"
                ],
                "start": "2025-05-07T15:44:39Z",
                "end": "2025-05-07T15:46:44Z"
            }
        },
        {
            "CTWsqpYBc0POvO1c9MA9": {
                "id": "CTWsqpYBc0POvO1c9MA9",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "CjWsqpYBc0POvO1c9cA_",
                    "ETWsqpYBc0POvO1c98CR"
                ],
                "start": "2025-05-07T15:16:23Z",
                "end": "2025-05-07T15:18:04Z"
            }
        },
        {
            "NjW7qpYBc0POvO1cocD9": {
                "id": "NjW7qpYBc0POvO1cocD9",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "NzW7qpYBc0POvO1cosD1",
                    "PjW7qpYBc0POvO1cpcA1"
                ],
                "start": "2025-05-07T15:32:25Z",
                "end": "2025-05-07T15:34:31Z"
            }
        },
        {
            "RTW-qpYBc0POvO1cScBh": {
                "id": "RTW-qpYBc0POvO1cScBh",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "RjW-qpYBc0POvO1cSsBa",
                    "TTW-qpYBc0POvO1cTMDp"
                ],
                "start": "2025-05-07T15:35:19Z",
                "end": "2025-05-07T15:37:21Z"
            }
        },
        {
            "GDWuqpYBc0POvO1cuMAP": {
                "id": "GDWuqpYBc0POvO1cuMAP",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    split_dataset.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n    benchmark_model.train_labels --> Explainability.train_labels;\n    benchmark_model.test_labels --> Explainability.test_labels;\n\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "GTWuqpYBc0POvO1cucAn",
                    "IDWuqpYBc0POvO1cu8DD"
                ],
                "start": "2025-05-07T15:18:18Z",
                "end": "2025-05-07T15:20:06Z"
            }
        },
        {
            "hDXYqpYBc0POvO1cGsCA": {
                "id": "hDXYqpYBc0POvO1cGsCA",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "hTXYqpYBc0POvO1cG8Cc",
                    "jDXYqpYBc0POvO1cHsAU"
                ],
                "start": "2025-05-07T16:03:31Z",
                "end": "2025-05-07T16:05:27Z"
            }
        },
        {
            "7zW9rpYBc0POvO1cxsBp": {
                "id": "7zW9rpYBc0POvO1cxsBp",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "ojXtqpYBc0POvO1cwMCo": {
                "id": "ojXtqpYBc0POvO1cwMCo",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "ozXtqpYBc0POvO1cwcDj",
                    "qjXtqpYBc0POvO1cxMBU"
                ],
                "start": "2025-05-07T16:27:09Z"
            }
        },
        {
            "3jUHq5YBc0POvO1c3cDt": {
                "id": "3jUHq5YBc0POvO1c3cDt",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "3zUHq5YBc0POvO1c38AF",
                    "5jUHq5YBc0POvO1c4cC1"
                ],
                "start": "2025-05-07T16:55:41Z"
            }
        },
        {
            "sTX4qpYBc0POvO1cfsDl": {
                "id": "sTX4qpYBc0POvO1cfsDl",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "sjX4qpYBc0POvO1cf8Dr",
                    "uTX4qpYBc0POvO1cgsCA"
                ],
                "start": "2025-05-07T16:38:53Z",
                "end": "2025-05-07T16:41:16Z"
            }
        },
        {
            "dTXWqpYBc0POvO1cYsC4": {
                "id": "dTXWqpYBc0POvO1cYsC4",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "djXWqpYBc0POvO1cZMAA",
                    "fTXWqpYBc0POvO1cZsCJ"
                ],
                "start": "2025-05-07T16:01:38Z",
                "end": "2025-05-07T16:03:21Z"
            }
        },
        {
            "kzXfqpYBc0POvO1cq8Bj": {
                "id": "kzXfqpYBc0POvO1cq8Bj",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "lDXfqpYBc0POvO1crMDK",
                    "mzXfqpYBc0POvO1cr8BU"
                ],
                "start": "2025-05-07T16:11:46Z",
                "end": "2025-05-07T16:13:41Z"
            }
        },
        {
            "zzUCq5YBc0POvO1c6MA4": {
                "id": "zzUCq5YBc0POvO1c6MA4",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "0DUCq5YBc0POvO1c6cBJ",
                    "1zUCq5YBc0POvO1c68Dz"
                ],
                "start": "2025-05-07T16:50:16Z",
                "end": "2025-05-07T16:52:59Z"
            }
        },
        {
            "wDX_qpYBc0POvO1cg8AG": {
                "id": "wDX_qpYBc0POvO1cg8AG",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    benchmark_model.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "wTX_qpYBc0POvO1chMAa",
                    "yDX_qpYBc0POvO1ch8Do"
                ],
                "start": "2025-05-07T16:46:33Z",
                "end": "2025-05-07T16:48:59Z"
            }
        },
        {
            "MzXOrpYBc0POvO1cvMEF": {
                "id": "MzXOrpYBc0POvO1cvMEF",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,10,2);\n        param gamma = range(1,5,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "NDXOrpYBc0POvO1cvcEi",
                    "PjXOrpYBc0POvO1cwMHu",
                    "SDXOrpYBc0POvO1cxMGs",
                    "UjXOrpYBc0POvO1cyMF_",
                    "XDXOrpYBc0POvO1czMFj",
                    "ZjXOrpYBc0POvO1c0MEz",
                    "cDXOrpYBc0POvO1c08HN",
                    "ejXOrpYBc0POvO1c18Fw",
                    "hDXOrpYBc0POvO1c2sHx",
                    "jjXOrpYBc0POvO1c3sGO",
                    "mDXOrpYBc0POvO1c4sE8",
                    "ojXOrpYBc0POvO1c5cHJ",
                    "rDXOrpYBc0POvO1c6cFp",
                    "tjXOrpYBc0POvO1c7MHz",
                    "wDXOrpYBc0POvO1c8ME6",
                    "yjXOrpYBc0POvO1c88GD",
                    "1DXOrpYBc0POvO1c9sHf",
                    "3jXOrpYBc0POvO1c-sG8",
                    "6DXOrpYBc0POvO1c_sFo",
                    "8jXPrpYBc0POvO1cAsFW"
                ],
                "start": "2025-05-08T10:31:45Z",
                "end": "2025-05-08T10:52:00Z"
            }
        },
        {
            "zkwpxJYBpHPS2GeIUx5a": {
                "id": "zkwpxJYBpHPS2GeIUx5a",
                "name": "uc3_fire_detection",
                "model": "workflow FireDetectionWorkflow {\n\n    task DetectAlert {\n        implementation \"DetectAlert\";\n    }\n\n    task SelectUsers {\n        implementation \"SelectUsers\";\n    }\n\n    task UserResponse {\n        implementation \"UserResponse\";\n    }\n\n    START -> DetectAlert -> SelectUsers -> UserResponse -> END;\n\n\n    define input data ModelFile;\n\n    configure data ModelFile {\n        path \"DetectAlert/**\";\n    }\n\n    ModelFile --> DetectAlert.ModelFile;\n}\n\nworkflow FireDetectionWorkflow1 from FireDetectionWorkflow {\n\n}\n\nexperiment Exp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of FireDetectionWorkflow1 {\n        strategy gridsearch;\n        //strategy randomsearch;\n        runs = 1;\n\n        // DetectAlert\n        param nb_recheck_values = range(1,20);\n        //param detection_confidence_threshold_values = range(0,100);\n        //param frame_sampling_interval_values = range(1,60);\n        //param process_images_values = enum(\"true\",\"false\");\n        //param process_videos_values = enum(\"true\",\"false\");\n        //param inference_device_values = enum(\"cpu\",\"cuda\");\n        //param max_files_to_process_values = range(0,100);\n\n        // SelectUsers\n        param num_users_selection_values = range(1,5);\n        param selection_diameter_km_values = range(1,20);\n        param user_profile_selection_values = enum(\"driving\",\"walking\",\"cycling\");\n        param filter_only_available_values = enum(\"true\",\"false\");\n        param sort_by_values = enum(\"distance\",\"travel_time\");\n        param osrm_timeout_s_values = range(1,10);\n        param euclidian_filter_km_values = range(1,50);\n\n        // UserResponse\n        param accept_string_values       = enum(\"accept\",\"yes\",\"ok\");\n        param reject_string_values       = enum(\"reject\",\"no\",\"cancel\");\n        param case_sensitive_values      = enum(\"true\",\"false\");\n        param default_response_values    = enum(\"accept\",\"reject\",\"none\");\n\n        task DetectAlert {\n            param nb_recheck                    = nb_recheck_values;\n            //param detection_confidence_threshold = detection_confidence_threshold_values;\n            //param frame_sampling_interval        = frame_sampling_interval_values;\n            //param process_images                 = process_images_values;\n            //param process_videos                 = process_videos_values;\n            //param inference_device               = inference_device_values;\n            //param max_files_to_process           = max_files_to_process_values;\n        }\n        \n        task SelectUsers {\n            param num_users_selection      = num_users_selection_values;\n            param selection_diameter_km    = selection_diameter_km_values;\n            param user_profile_selection             = user_profile_selection_values;\n            param filter_only_available    = filter_only_available_values;\n            param sort_by                  = sort_by_values;\n            param osrm_timeout_s           = osrm_timeout_s_values;\n            param euclidian_filter_km      = euclidian_filter_km_values;\n        }\n\n        task UserResponse {\n            param accept_string    = accept_string_values;\n            param reject_string    = reject_string_values;\n            param case_sensitive   = case_sensitive_values;\n            param default_response = default_response_values;\n        }\n\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "8jW_rpYBc0POvO1cGcDq": {
                "id": "8jW_rpYBc0POvO1cGcDq",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "8zXArpYBc0POvO1cO8C8": {
                "id": "8zXArpYBc0POvO1cO8C8",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "8DW-rpYBc0POvO1cYcCi": {
                "id": "8DW-rpYBc0POvO1cYcCi",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "8TW-rpYBc0POvO1cxcC7": {
                "id": "8TW-rpYBc0POvO1cxcC7",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "9DXArpYBc0POvO1c2sDp": {
                "id": "9DXArpYBc0POvO1c2sDp",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "9TXArpYBc0POvO1c3MAt",
                    "_zXArpYBc0POvO1c38DQ"
                ],
                "start": "2025-05-08T10:16:36Z",
                "end": "2025-05-08T10:19:55Z"
            }
        },
        {
            "CTXIrpYBc0POvO1czMFN": {
                "id": "CTXIrpYBc0POvO1czMFN",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    define output data TrainedModel;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n    train_model.TrainedModel --> TrainedModel;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n    configure data TrainedModel {\n    path \"TrainedModel/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "CjXIrpYBc0POvO1czcF4",
                    "FDXIrpYBc0POvO1c0sEC"
                ],
                "start": "2025-05-08T10:25:16Z",
                "end": "2025-05-08T10:27:41Z"
            }
        },
        {
            "HjXLrpYBc0POvO1cpMFW": {
                "id": "HjXLrpYBc0POvO1cpMFW",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,2,10);\n        param gamma = range(1,2,5);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "HzXLrpYBc0POvO1cpcFq",
                    "KTXLrpYBc0POvO1cqcFc"
                ],
                "start": "2025-05-08T10:28:23Z",
                "end": "2025-05-08T10:30:25Z"
            }
        },
        {
            "_DXnrpYBc0POvO1cmcGO": {
                "id": "_DXnrpYBc0POvO1cmcGO",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 10, 5);\n        param min_child_weight = range(1,10,2);\n        param gamma = range(1,5,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "_TXnrpYBc0POvO1cmsGS",
                    "BzXnrpYBc0POvO1cnsI8",
                    "ETXnrpYBc0POvO1cocLI",
                    "GzXnrpYBc0POvO1cpcJU",
                    "JTXnrpYBc0POvO1cqcJa",
                    "LzXnrpYBc0POvO1crMLj",
                    "OTXnrpYBc0POvO1csMJz",
                    "QzXnrpYBc0POvO1cs8Lu",
                    "TTXnrpYBc0POvO1ct8J_",
                    "VzXnrpYBc0POvO1cusLo",
                    "YTXnrpYBc0POvO1cvsKZ",
                    "azXnrpYBc0POvO1cwsJJ",
                    "dTXnrpYBc0POvO1cxcKz",
                    "fzXnrpYBc0POvO1cycKQ",
                    "iTXnrpYBc0POvO1czcIt",
                    "kzXnrpYBc0POvO1c0MKU",
                    "nTXnrpYBc0POvO1c1MIv",
                    "pzXnrpYBc0POvO1c18LC",
                    "sTXnrpYBc0POvO1c28Iy",
                    "uzXnrpYBc0POvO1c38I4"
                ],
                "start": "2025-05-08T10:58:55Z"
            }
        },
        {
            "80wIxZYBpHPS2GeIjx7w": {
                "id": "80wIxZYBpHPS2GeIjx7w",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9EwIxZYBpHPS2GeIkh7e"
                ],
                "start": "2025-05-12T17:06:34Z"
            }
        },
        {
            "-EwMxZYBpHPS2GeINB4U": {
                "id": "-EwMxZYBpHPS2GeINB4U",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,7,2);\n        param gamma = range(1,4,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "-UwMxZYBpHPS2GeINB7Q",
                    "A0wMxZYBpHPS2GeINh-g",
                    "DUwMxZYBpHPS2GeIOB9v",
                    "F0wMxZYBpHPS2GeIOh88",
                    "IUwMxZYBpHPS2GeIOx_4",
                    "K0wMxZYBpHPS2GeIPR-x",
                    "NUwMxZYBpHPS2GeIPx90",
                    "P0wMxZYBpHPS2GeIQR82",
                    "SUwMxZYBpHPS2GeIQh_1",
                    "U0wMxZYBpHPS2GeIRB-b",
                    "XUwMxZYBpHPS2GeIRh9M",
                    "Z0wMxZYBpHPS2GeISB8J",
                    "cUwMxZYBpHPS2GeISR-8",
                    "e0wMxZYBpHPS2GeISx92",
                    "hUwMxZYBpHPS2GeITR88",
                    "j0wMxZYBpHPS2GeITh_g",
                    "mUwMxZYBpHPS2GeIUB-N",
                    "o0wMxZYBpHPS2GeIUh9R",
                    "rUwMxZYBpHPS2GeIVB8E",
                    "t0wMxZYBpHPS2GeIVR-8",
                    "wUwMxZYBpHPS2GeIVx94",
                    "y0wMxZYBpHPS2GeIWR8p",
                    "1UwMxZYBpHPS2GeIWh_Y",
                    "30wMxZYBpHPS2GeIXB-L",
                    "6UwMxZYBpHPS2GeIXh8v",
                    "80wMxZYBpHPS2GeIXx_L",
                    "_UwMxZYBpHPS2GeIYR96",
                    "B0wMxZYBpHPS2GeIYyAu",
                    "EUwMxZYBpHPS2GeIZCDc",
                    "G0wMxZYBpHPS2GeIZiCE",
                    "JUwMxZYBpHPS2GeIaCBh",
                    "L0wMxZYBpHPS2GeIaiAI",
                    "OUwMxZYBpHPS2GeIayC8",
                    "Q0wMxZYBpHPS2GeIbSDc",
                    "TUwMxZYBpHPS2GeIbyCP",
                    "V0wMxZYBpHPS2GeIcSAs"
                ],
                "start": "2025-05-12T18:10:33Z"
            }
        },
        {
            "YUwOxZYBpHPS2GeINiAT": {
                "id": "YUwOxZYBpHPS2GeINiAT",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,7,2);\n        param gamma = range(1,4,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "YkwOxZYBpHPS2GeINiDT",
                    "bEwOxZYBpHPS2GeIOCCB",
                    "dkwOxZYBpHPS2GeIOiAo",
                    "gEwOxZYBpHPS2GeIOyDp",
                    "ikwOxZYBpHPS2GeIPSCl",
                    "lEwOxZYBpHPS2GeIPyBs",
                    "nkwOxZYBpHPS2GeIQSA0",
                    "qEwOxZYBpHPS2GeIQiDj",
                    "skwOxZYBpHPS2GeIRCCM",
                    "vEwOxZYBpHPS2GeIRiAz",
                    "xkwOxZYBpHPS2GeIRyDg",
                    "0EwOxZYBpHPS2GeISSDd",
                    "2kwOxZYBpHPS2GeISyCC",
                    "5EwOxZYBpHPS2GeITSBD",
                    "7kwOxZYBpHPS2GeITyAB",
                    "-EwOxZYBpHPS2GeIUCC0",
                    "AkwOxZYBpHPS2GeIUiFl",
                    "DEwOxZYBpHPS2GeIVCEq",
                    "FkwOxZYBpHPS2GeIVSHi",
                    "IEwOxZYBpHPS2GeIVyGT",
                    "KkwOxZYBpHPS2GeIWSF_",
                    "NEwOxZYBpHPS2GeIWyEr",
                    "PkwOxZYBpHPS2GeIXCHi",
                    "SEwOxZYBpHPS2GeIXiGK",
                    "UkwOxZYBpHPS2GeIYCE7",
                    "XEwOxZYBpHPS2GeIYSHc",
                    "ZkwOxZYBpHPS2GeIYyF8",
                    "cEwOxZYBpHPS2GeIZSEg",
                    "ekwOxZYBpHPS2GeIZiHU",
                    "hEwOxZYBpHPS2GeIaCF9",
                    "jkwOxZYBpHPS2GeIaiEU",
                    "mEwOxZYBpHPS2GeIayGq",
                    "okwOxZYBpHPS2GeIbSFO",
                    "rEwOxZYBpHPS2GeIbyEJ",
                    "tkwOxZYBpHPS2GeIcCHB",
                    "wEwOxZYBpHPS2GeIciFs"
                ],
                "start": "2025-05-12T18:12:44Z",
                "end": "2025-05-12T18:44:36Z"
            }
        },
        {
            "7kwsxZYBpHPS2GeICiGn": {
                "id": "7kwsxZYBpHPS2GeICiGn",
                "name": "UC1_surrogate_model",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> TrainModelAPI -> ReadMetrics -> END;\n\n    task TrainModelAPI;\n\n    task ReadMetrics {\n        implementation \"UC1.surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"UC1/nimes_2005_STAC.json\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModelAPI.FileToRead;\n    TrainModelAPI.jobID --> ReadMetrics.jobID;\n    TrainModelAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"UC1.surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "70wsxZYBpHPS2GeIDCFs": {
                "id": "70wsxZYBpHPS2GeIDCFs",
                "name": "UC1_surrogate_model",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> TrainModelAPI -> ReadMetrics -> END;\n\n    task TrainModelAPI;\n\n    task ReadMetrics {\n        implementation \"UC1.surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"UC1/nimes_2005_STAC.json\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModelAPI.FileToRead;\n    TrainModelAPI.jobID --> ReadMetrics.jobID;\n    TrainModelAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"UC1.surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "-0xlxpYBpHPS2GeI_yHg": {
                "id": "-0xlxpYBpHPS2GeI_yHg",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "_ExmxpYBpHPS2GeINyGg": {
                "id": "_ExmxpYBpHPS2GeINyGg",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define input data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //     path \"demo_datasets/demo_data.txt\";\n    // }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "_UxmxpYBpHPS2GeIviGO": {
                "id": "_UxmxpYBpHPS2GeIviGO",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define input data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //     path \"demo_datasets/demo_data.txt\";\n    // }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "-ExixpYBpHPS2GeIbSFb": {
                "id": "-ExixpYBpHPS2GeIbSFb",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define output data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //      path \"output/demo_oslo_output/**\";\n    // }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n        path \"output/demo_oslo_output/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-UxixpYBpHPS2GeIbiFr"
                ],
                "start": "2025-05-12T21:24:21Z",
                "end": "2025-05-12T21:24:29Z"
            }
        },
        {
            "-kxlxpYBpHPS2GeIpSFa": {
                "id": "-kxlxpYBpHPS2GeIpSFa",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "9kxcxpYBpHPS2GeIPyEo": {
                "id": "9kxcxpYBpHPS2GeIPyEo",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define output data OutputFile;\n\n    // Task1.OutputFile --> OutputFile;\n\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "90xcxpYBpHPS2GeIQCFd"
                ],
                "start": "2025-05-12T21:17:36Z",
                "end": "2025-05-12T21:17:45Z"
            }
        },
        {
            "8kxOxpYBpHPS2GeIEyEZ": {
                "id": "8kxOxpYBpHPS2GeIEyEZ",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define output data OutputFile;\n\n    Task1.OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/demo_oslo_output/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "80xOxpYBpHPS2GeIFCEn"
                ],
                "start": "2025-05-12T21:02:07Z",
                "end": "2025-05-12T21:02:16Z"
            }
        },
        {
            "9ExbxpYBpHPS2GeIQCH4": {
                "id": "9ExbxpYBpHPS2GeIQCH4",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define output data OutputFile;\n\n    // Task1.OutputFile --> OutputFile;\n\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9UxbxpYBpHPS2GeIQiEN"
                ],
                "start": "2025-05-12T21:16:31Z",
                "end": "2025-05-12T21:16:39Z"
            }
        },
        {
            "8ExMxpYBpHPS2GeIVCH2": {
                "id": "8ExMxpYBpHPS2GeIVCH2",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define output data OutputFile;\n\n    Task1.OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "8UxMxpYBpHPS2GeIViEs"
                ],
                "start": "2025-05-12T21:00:13Z",
                "end": "2025-05-12T21:00:22Z"
            }
        },
        {
            "_kxrxpYBpHPS2GeI7SGA": {
                "id": "_kxrxpYBpHPS2GeI7SGA",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_0xrxpYBpHPS2GeI7iGZ"
                ],
                "start": "2025-05-12T21:34:44Z",
                "end": "2025-05-12T21:34:52Z"
            }
        },
        {
            "Bkx5xpYBpHPS2GeIJCLx": {
                "id": "Bkx5xpYBpHPS2GeIJCLx",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "B0x5xpYBpHPS2GeIJiIE"
                ],
                "start": "2025-05-12T21:49:10Z",
                "end": "2025-05-12T21:49:18Z"
            }
        },
        {
            "DkyGxpYBpHPS2GeIaSI6": {
                "id": "DkyGxpYBpHPS2GeIaSI6",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \".\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "D0yGxpYBpHPS2GeIaiJN"
                ],
                "start": "2025-05-12T22:03:39Z",
                "end": "2025-05-12T22:03:48Z"
            }
        },
        {
            "EEyGxpYBpHPS2GeI6SIx": {
                "id": "EEyGxpYBpHPS2GeI6SIx",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "EUyGxpYBpHPS2GeI6iJA"
                ],
                "start": "2025-05-12T22:04:12Z",
                "end": "2025-05-12T22:04:20Z"
            }
        },
        {
            "Akx1xpYBpHPS2GeIuSIS": {
                "id": "Akx1xpYBpHPS2GeIuSIS",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define input data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //     path \"demo_datasets/demo_data.txt\";\n    // }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "A0x1xpYBpHPS2GeIuiIz"
                ],
                "start": "2025-05-12T21:45:26Z",
                "end": "2025-05-12T21:45:34Z"
            }
        },
        {
            "BEx2xpYBpHPS2GeIISLf": {
                "id": "BEx2xpYBpHPS2GeIISLf",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "BUx2xpYBpHPS2GeIIiLs"
                ],
                "start": "2025-05-12T21:45:53Z",
                "end": "2025-05-12T21:46:01Z"
            }
        },
        {
            "CEyBxpYBpHPS2GeIWyLi": {
                "id": "CEyBxpYBpHPS2GeIWyLi",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CUyBxpYBpHPS2GeIXCL5"
                ],
                "start": "2025-05-12T21:58:08Z",
                "end": "2025-05-12T21:58:17Z"
            }
        },
        {
            "CkyCxpYBpHPS2GeIySLS": {
                "id": "CkyCxpYBpHPS2GeIySLS",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "C0yCxpYBpHPS2GeIyyIP"
                ],
                "start": "2025-05-12T21:59:42Z",
                "end": "2025-05-12T21:59:50Z"
            }
        },
        {
            "DEyExpYBpHPS2GeI_SJ8": {
                "id": "DEyExpYBpHPS2GeI_SJ8",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DUyExpYBpHPS2GeI_iKT"
                ],
                "start": "2025-05-12T22:02:06Z",
                "end": "2025-05-12T22:02:15Z"
            }
        },
        {
            "AExwxpYBpHPS2GeI_iIZ": {
                "id": "AExwxpYBpHPS2GeI_iIZ",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "AUxwxpYBpHPS2GeI_yJP"
                ],
                "start": "2025-05-12T21:40:16Z",
                "end": "2025-05-12T21:40:24Z"
            }
        },
        {
            "HkyfxpYBpHPS2GeI1CKw": {
                "id": "HkyfxpYBpHPS2GeI1CKw",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "H0yfxpYBpHPS2GeI1SLa"
                ],
                "start": "2025-05-12T22:31:25Z",
                "end": "2025-05-12T22:31:34Z"
            }
        },
        {
            "IEyhxpYBpHPS2GeIeyKu": {
                "id": "IEyhxpYBpHPS2GeIeyKu",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define input data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //     path \"\";\n    // }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "HEyYxpYBpHPS2GeI4CJr": {
                "id": "HEyYxpYBpHPS2GeI4CJr",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "HUyYxpYBpHPS2GeI4SKo"
                ],
                "start": "2025-05-12T22:23:50Z",
                "end": "2025-05-12T22:23:58Z"
            }
        },
        {
            "JUynxpYBpHPS2GeIqCIG": {
                "id": "JUynxpYBpHPS2GeIqCIG",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define input data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //     path \"\";\n    // }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "JkynxpYBpHPS2GeIqSIT"
                ],
                "start": "2025-05-12T22:39:58Z",
                "end": "2025-05-12T22:40:06Z"
            }
        },
        {
            "GEyRxpYBpHPS2GeIySLQ": {
                "id": "GEyRxpYBpHPS2GeIySLQ",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GUyRxpYBpHPS2GeIyiLl"
                ],
                "start": "2025-05-12T22:16:05Z",
                "end": "2025-05-12T22:16:13Z"
            }
        },
        {
            "GkySxpYBpHPS2GeIRiLg": {
                "id": "GkySxpYBpHPS2GeIRiLg",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "G0ySxpYBpHPS2GeIRyLr"
                ],
                "start": "2025-05-12T22:16:37Z",
                "end": "2025-05-12T22:16:45Z"
            }
        },
        {
            "FEyPxpYBpHPS2GeI8CJh": {
                "id": "FEyPxpYBpHPS2GeI8CJh",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "FUyPxpYBpHPS2GeI8SKK"
                ],
                "start": "2025-05-12T22:14:04Z",
                "end": "2025-05-12T22:14:12Z"
            }
        },
        {
            "FkyQxpYBpHPS2GeInyL2": {
                "id": "FkyQxpYBpHPS2GeInyL2",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "F0yQxpYBpHPS2GeIoSIC"
                ],
                "start": "2025-05-12T22:14:49Z",
                "end": "2025-05-12T22:14:57Z"
            }
        },
        {
            "EkyNxpYBpHPS2GeINCLV": {
                "id": "EkyNxpYBpHPS2GeINCLV",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"\";\n    }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "E0yNxpYBpHPS2GeINiJr"
                ],
                "start": "2025-05-12T22:11:05Z",
                "end": "2025-05-12T22:11:13Z"
            }
        },
        {
            "IUyixpYBpHPS2GeIkSKK": {
                "id": "IUyixpYBpHPS2GeIkSKK",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define input data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //     path \"\";\n    // }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "IkyixpYBpHPS2GeIkiKX"
                ],
                "start": "2025-05-12T22:34:25Z",
                "end": "2025-05-12T22:34:33Z"
            }
        },
        {
            "I0ykxpYBpHPS2GeIMSJl": {
                "id": "I0ykxpYBpHPS2GeIMSJl",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    // define input data InputFile;\n    // InputFile --> Task1.InputFile;\n    // configure data InputFile {\n    //     path \"\";\n    // }\n\n    // define output data OutputFile;\n    // Task1.OutputFile --> OutputFile;\n    // configure data OutputFile {\n    //     path \"output/demo_oslo_output/**\";\n    // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "JEykxpYBpHPS2GeIMiJ4"
                ],
                "start": "2025-05-12T22:36:11Z",
                "end": "2025-05-12T22:36:19Z"
            }
        },
        {
            "UEzQyJYBpHPS2GeIESLa": {
                "id": "UEzQyJYBpHPS2GeIESLa",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "UUzQyJYBpHPS2GeIEiLo",
                    "U0zQyJYBpHPS2GeIEyIO"
                ],
                "start": "2025-05-13T08:43:21Z",
                "end": "2025-05-13T08:43:31Z"
            }
        },
        {
            "VUzQyJYBpHPS2GeIqSI8": {
                "id": "VUzQyJYBpHPS2GeIqSI8",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detectio /ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VkzSyJYBpHPS2GeIESK2": {
                "id": "VkzSyJYBpHPS2GeIESK2",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "V0zSyJYBpHPS2GeIEiK4",
                    "WUzSyJYBpHPS2GeIEiLa"
                ],
                "start": "2025-05-13T08:45:32Z",
                "end": "2025-05-13T08:45:42Z"
            }
        },
        {
            "W0zSyJYBpHPS2GeIHSJ5": {
                "id": "W0zSyJYBpHPS2GeIHSJ5",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XEzSyJYBpHPS2GeIHiKJ",
                    "XkzSyJYBpHPS2GeIHiKk"
                ],
                "start": "2025-05-13T08:45:35Z",
                "end": "2025-05-13T08:45:45Z"
            }
        },
        {
            "YEzSyJYBpHPS2GeImyJa": {
                "id": "YEzSyJYBpHPS2GeImyJa",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "LUy2xpYBpHPS2GeIqCLk": {
                "id": "LUy2xpYBpHPS2GeIqCLk",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        // param param1_vp = enum(2, 5, 7);\n        // task Task1 {\n        //     param param1 = param1_vp;\n        // }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Lky2xpYBpHPS2GeIqSLz"
                ],
                "start": "2025-05-12T22:56:21Z",
                "end": "2025-05-12T22:56:30Z"
            }
        },
        {
            "MUycyJYBpHPS2GeIDiKI": {
                "id": "MUycyJYBpHPS2GeIDiKI",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        // param param1_vp = enum(2, 5, 7);\n        // task Task1 {\n        //     param param1 = param1_vp;\n        // }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MkycyJYBpHPS2GeIDyKV"
                ],
                "start": "2025-05-13T07:46:32Z",
                "end": "2025-05-13T07:46:42Z"
            }
        },
        {
            "NEycyJYBpHPS2GeIkiLC": {
                "id": "NEycyJYBpHPS2GeIkiLC",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        // param param1_vp = enum(2, 5, 7);\n        // task Task1 {\n        //     param param1 = param1_vp;\n        // }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "NUycyJYBpHPS2GeIkyLN"
                ],
                "start": "2025-05-13T07:47:06Z",
                "end": "2025-05-13T07:47:15Z"
            }
        },
        {
            "J0y0xpYBpHPS2GeIwCJY": {
                "id": "J0y0xpYBpHPS2GeIwCJY",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "KEy0xpYBpHPS2GeIwSKU"
                ],
                "start": "2025-05-12T22:54:16Z",
                "end": "2025-05-12T22:54:25Z"
            }
        },
        {
            "Kky1xpYBpHPS2GeIlSLn": {
                "id": "Kky1xpYBpHPS2GeIlSLn",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "K0y1xpYBpHPS2GeIliLw"
                ],
                "start": "2025-05-12T22:55:11Z",
                "end": "2025-05-12T22:55:19Z"
            }
        },
        {
            "OEzEyJYBpHPS2GeIoyKD": {
                "id": "OEzEyJYBpHPS2GeIoyKD",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "OUzEyJYBpHPS2GeIpCKM"
                ],
                "start": "2025-05-13T08:30:52Z",
                "end": "2025-05-13T08:31:00Z"
            }
        },
        {
            "O0zFyJYBpHPS2GeIdiI0": {
                "id": "O0zFyJYBpHPS2GeIdiI0",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "PEzFyJYBpHPS2GeIdyI-"
                ],
                "start": "2025-05-13T08:31:46Z",
                "end": "2025-05-13T08:31:54Z"
            }
        },
        {
            "PkzHyJYBpHPS2GeIpyIm": {
                "id": "PkzHyJYBpHPS2GeIpyIm",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "P0zHyJYBpHPS2GeIqCIw",
                    "QUzHyJYBpHPS2GeIqCJU"
                ],
                "start": "2025-05-13T08:34:10Z",
                "end": "2025-05-13T08:34:19Z"
            }
        },
        {
            "SUzNyJYBpHPS2GeI-SKR": {
                "id": "SUzNyJYBpHPS2GeI-SKR",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "SkzNyJYBpHPS2GeI-iLJ",
                    "TEzNyJYBpHPS2GeI-iLx"
                ],
                "start": "2025-05-13T08:41:04Z",
                "end": "2025-05-13T08:41:14Z"
            }
        },
        {
            "Q0zMyJYBpHPS2GeIsyLX": {
                "id": "Q0zMyJYBpHPS2GeIsyLX",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "REzMyJYBpHPS2GeItCLd",
                    "RkzMyJYBpHPS2GeItSIF"
                ],
                "start": "2025-05-13T08:39:40Z",
                "end": "2025-05-13T08:39:50Z"
            }
        },
        {
            "gUzkyJYBpHPS2GeIhyKK": {
                "id": "gUzkyJYBpHPS2GeIhyKK",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detectio /ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "gkzlyJYBpHPS2GeIdyJY": {
                "id": "gkzlyJYBpHPS2GeIdyJY",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detectio /ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "jkzvyJYBpHPS2GeItiI1": {
                "id": "jkzvyJYBpHPS2GeItiI1",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "j0zvyJYBpHPS2GeIuiJl",
                    "k0zwyJYBpHPS2GeIECIe",
                    "l0zwyJYBpHPS2GeIUiIb",
                    "m0zwyJYBpHPS2GeIlSIJ"
                ],
                "start": "2025-05-13T11:17:52Z"
            }
        },
        {
            "nEzzyJYBpHPS2GeIFyLG": {
                "id": "nEzzyJYBpHPS2GeIFyLG",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "dUziyJYBpHPS2GeIHCJe": {
                "id": "dUziyJYBpHPS2GeIHCJe",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "dkziyJYBpHPS2GeIHiK5"
                ],
                "start": "2025-05-13T11:03:03Z",
                "end": "2025-05-13T11:03:10Z"
            }
        },
        {
            "gEzkyJYBpHPS2GeIYiLF": {
                "id": "gEzkyJYBpHPS2GeIYiLF",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detectio /ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "hUztyJYBpHPS2GeI7SL1": {
                "id": "hUztyJYBpHPS2GeI7SL1",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "hkztyJYBpHPS2GeI8iJu",
                    "ikzuyJYBpHPS2GeIuiJD"
                ],
                "start": "2025-05-13T11:15:56Z"
            }
        },
        {
            "YUzVyJYBpHPS2GeImSKD": {
                "id": "YUzVyJYBpHPS2GeImSKD",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YkzVyJYBpHPS2GeImiK2",
                    "ZEzVyJYBpHPS2GeImiLe"
                ],
                "start": "2025-05-13T08:49:24Z",
                "end": "2025-05-13T08:49:33Z"
            }
        },
        {
            "ZkzWyJYBpHPS2GeIeCK0": {
                "id": "ZkzWyJYBpHPS2GeIeCK0",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Z0zWyJYBpHPS2GeIeSK8",
                    "aUzWyJYBpHPS2GeIeSLa"
                ],
                "start": "2025-05-13T08:50:21Z",
                "end": "2025-05-13T08:50:30Z"
            }
        },
        {
            "cEzXyJYBpHPS2GeIuyIF": {
                "id": "cEzXyJYBpHPS2GeIuyIF",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cUzXyJYBpHPS2GeIvCIU",
                    "c0zXyJYBpHPS2GeIvCI2"
                ],
                "start": "2025-05-13T08:51:43Z",
                "end": "2025-05-13T08:51:53Z"
            }
        },
        {
            "a0zXyJYBpHPS2GeIXCLf": {
                "id": "a0zXyJYBpHPS2GeIXCLf",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bEzXyJYBpHPS2GeIXSLr",
                    "bkzXyJYBpHPS2GeIXiII"
                ],
                "start": "2025-05-13T08:51:19Z",
                "end": "2025-05-13T08:51:29Z"
            }
        },
        {
            "a0xOyZYBpHPS2GeIXSNM": {
                "id": "a0xOyZYBpHPS2GeIXSNM",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VUxJyZYBpHPS2GeIvSN9": {
                "id": "VUxJyZYBpHPS2GeIvSN9",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    START -> S1M -> S2M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> S2B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    // V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "VkxJyZYBpHPS2GeIwiOK",
                    "WkxKyZYBpHPS2GeItyNY",
                    "XkxLyZYBpHPS2GeIniOg",
                    "X0xLyZYBpHPS2GeI1iOM",
                    "Y0xMyZYBpHPS2GeIvSMa",
                    "Z0xNyZYBpHPS2GeIriM1",
                    "aExOyZYBpHPS2GeIBiPJ",
                    "aUxOyZYBpHPS2GeISyOm"
                ],
                "start": "2025-05-13T12:56:13Z",
                "end": "2025-05-13T13:02:17Z"
            }
        },
        {
            "MEwYyZYBpHPS2GeIBSP6": {
                "id": "MEwYyZYBpHPS2GeIBSP6",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    START -> S1M -> S2M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> S2B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    // V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MUwYyZYBpHPS2GeICiN6",
                    "NUwYyZYBpHPS2GeI5iPH",
                    "OUwZyZYBpHPS2GeI4yPa"
                ],
                "start": "2025-05-13T12:01:55Z"
            }
        },
        {
            "OkwbyZYBpHPS2GeIMCPt": {
                "id": "OkwbyZYBpHPS2GeIMCPt",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "O0wbyZYBpHPS2GeIMSP4",
                    "PUwbyZYBpHPS2GeIMiMS"
                ],
                "start": "2025-05-13T10:05:24Z",
                "end": "2025-05-13T10:05:34Z"
            }
        },
        {
            "80wXyZYBpHPS2GeI2yIQ": {
                "id": "80wXyZYBpHPS2GeI2yIQ",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "9EwXyZYBpHPS2GeI2yLG",
                    "_kwXyZYBpHPS2GeI3SJx",
                    "CEwXyZYBpHPS2GeI3iPz",
                    "EkwXyZYBpHPS2GeI4SMW",
                    "HEwXyZYBpHPS2GeI4iO_",
                    "JkwXyZYBpHPS2GeI5CNq"
                ],
                "start": "2025-05-13T13:01:45Z",
                "end": "2025-05-13T13:06:44Z"
            }
        },
        {
            "nUwNyZYBpHPS2GeIriIu": {
                "id": "nUwNyZYBpHPS2GeIriIu",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "nkwNyZYBpHPS2GeIsiJu",
                    "okwOyZYBpHPS2GeIlSJO",
                    "pkwPyZYBpHPS2GeIdyLI"
                ],
                "start": "2025-05-13T11:50:38Z"
            }
        },
        {
            "qkwSyZYBpHPS2GeICSKE": {
                "id": "qkwSyZYBpHPS2GeICSKE",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "q0wSyZYBpHPS2GeIDiJH",
                    "r0wSyZYBpHPS2GeI7iKN",
                    "s0wTyZYBpHPS2GeI3SJ8"
                ],
                "start": "2025-05-13T11:55:23Z"
            }
        },
        {
            "bEyiyZYBpHPS2GeI7COB": {
                "id": "bEyiyZYBpHPS2GeI7COB",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 2;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bUyiyZYBpHPS2GeI7SO-",
                    "b0yiyZYBpHPS2GeI7SPh"
                ],
                "start": "2025-05-13T12:33:40Z",
                "end": "2025-05-13T12:33:49Z"
            }
        },
        {
            "cUymyZYBpHPS2GeI6CP3": {
                "id": "cUymyZYBpHPS2GeI6CP3",
                "name": "uc5_test",
                "model": "package binary;\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadData -> TrainModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n}\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModel {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModel {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModel {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n    //Automated\n    // S1 -> S2 -> S3;\n    //START -> S1 -> S2 -> S3 -> END;\n    START -> S1 -> END;\n  }\n\n  space S1 of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2 of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3 of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n}\n\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "t0wWyZYBpHPS2GeI6yLa": {
                "id": "t0wWyZYBpHPS2GeI6yLa",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,7,2);\n        param gamma = range(1,4,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "uEwWyZYBpHPS2GeI7CKI",
                    "wkwWyZYBpHPS2GeI7iIe",
                    "zEwWyZYBpHPS2GeI7yKX",
                    "1kwWyZYBpHPS2GeI8SIx",
                    "4EwWyZYBpHPS2GeI8yIB",
                    "6kwWyZYBpHPS2GeI9CKI"
                ],
                "start": "2025-05-13T13:00:44Z"
            }
        },
        {
            "8EwXyZYBpHPS2GeISiIY": {
                "id": "8EwXyZYBpHPS2GeISiIY",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 3, 2);\n        param n_estimators = range(5, 5, 5);\n        param min_child_weight = range(1,1,2);\n        param gamma = range(1,4,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [],
                "start": "2025-05-13T13:01:08Z",
                "end": "2025-05-13T13:01:10Z"
            }
        },
        {
            "8UwXyZYBpHPS2GeIdiL2": {
                "id": "8UwXyZYBpHPS2GeIdiL2",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 5, 2);\n        param n_estimators = range(5, 5, 5);\n        param min_child_weight = range(1,1,2);\n        param gamma = range(1,4,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [],
                "start": "2025-05-13T13:01:20Z",
                "end": "2025-05-13T13:01:21Z"
            }
        },
        {
            "8kwXyZYBpHPS2GeIkSLT": {
                "id": "8kwXyZYBpHPS2GeIkSLT",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 5, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,1,2);\n        param gamma = range(1,4,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [],
                "start": "2025-05-13T13:01:27Z",
                "end": "2025-05-13T13:01:28Z"
            }
        },
        {
            "d0yvyZYBpHPS2GeIdSPI": {
                "id": "d0yvyZYBpHPS2GeIdSPI",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "eEyxyZYBpHPS2GeIhSMB": {
                "id": "eEyxyZYBpHPS2GeIhSMB",
                "name": "uc5_bestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"UC5.binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"uc5_binary_DataPreprocessing\";\n  }\n\n  \n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"UC5.multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"uc5_multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"UC5/machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"UC5.V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    //V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "dkyvyZYBpHPS2GeIFSPa": {
                "id": "dkyvyZYBpHPS2GeIFSPa",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "l0y5yZYBpHPS2GeI2CM7": {
                "id": "l0y5yZYBpHPS2GeI2CM7",
                "name": "uc5_bestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"UC5.binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"uc5_binary_DataPreprocessing\";\n  }\n\n  \n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"UC5.multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"uc5_multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"UC5/machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"UC5.V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    //V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"UC5.T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"UC5.T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"UC5.I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "mUy5yZYBpHPS2GeI3SOZ",
                    "p0y6yZYBpHPS2GeIYiOU",
                    "q0y6yZYBpHPS2GeI5SP_",
                    "r0y7yZYBpHPS2GeIaSPa"
                ],
                "start": "2025-05-13T12:58:43Z"
            }
        },
        {
            "fUy2yZYBpHPS2GeI7SNM": {
                "id": "fUy2yZYBpHPS2GeI7SNM",
                "name": "UC1_building_seg",
                "model": "package CS_building_segmentation;\n\nworkflow CS_building_segmentation_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> BuildingSegmentation -> EvaluateModel -> END;\n\n    task ReadData {\n        implementation \"UC1.building_seg.ReadData\";\n    }\n\n    task BuildingSegmentation;\n\n    task EvaluateModel {\n        implementation \"UC1.building_seg.EvaluateModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/clipped.las\";\n    }\n\n    define output data TrainedModelFolder;\n\n    ReadData.POINTS --> BuildingSegmentation.POINTS;\n    BuildingSegmentation.POLYGONS --> EvaluateModel.POLYGONS;\n}\n\nworkflow BuildingSegmentation_Alpha from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_Alpha\";\n  }\n}\n\nworkflow BuildingSegmentation_DBSCAN from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_DBSCAN\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of BuildingSegmentation_Alpha {\n        strategy gridsearch;\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param epsilon_vp = enum(200);\n        task BuildingSegmentation {\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n        }\n    }\n\n    space S2 of BuildingSegmentation_DBSCAN {\n        strategy gridsearch;\n        param epsilon_vp = enum(0.5);\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param min_samples_vp = enum(5);\n        task BuildingSegmentation {\n            param epsilon = epsilon_vp;\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n            param min_samples = min_samples_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fky2yZYBpHPS2GeI7iO8",
                    "jEy4yZYBpHPS2GeIHiPP"
                ],
                "start": "2025-05-13T12:55:31Z",
                "end": "2025-05-13T12:57:56Z"
            }
        },
        {
            "ckyqyZYBpHPS2GeILSMv": {
                "id": "ckyqyZYBpHPS2GeILSMv",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "c0yqyZYBpHPS2GeILiM2"
                ],
                "start": "2025-05-13T12:41:35Z",
                "end": "2025-05-13T12:41:43Z"
            }
        },
        {
            "dUysyZYBpHPS2GeIKSOt": {
                "id": "dUysyZYBpHPS2GeIKSOt",
                "name": "uc5_bestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"UC5.binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"uc5_binary_DataPreprocessing\";\n  }\n\n  \n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"UC5.multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"uc5_multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"UC5/machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "sEy9yZYBpHPS2GeIYCP9": {
                "id": "sEy9yZYBpHPS2GeIYCP9",
                "name": "UC1_surrogate_model",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> TrainModelAPI -> ReadMetrics -> END;\n\n    task TrainModelAPI;\n\n    task ReadMetrics {\n        implementation \"UC1.surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"UC1/nimes_2005_STAC.json\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModelAPI.FileToRead;\n    TrainModelAPI.jobID --> ReadMetrics.jobID;\n    TrainModelAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"UC1.surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "eUy0yZYBpHPS2GeI0SPU": {
                "id": "eUy0yZYBpHPS2GeI0SPU",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "eky0yZYBpHPS2GeI0iPg"
                ],
                "start": "2025-05-13T12:53:13Z",
                "end": "2025-05-13T12:53:21Z"
            }
        },
        {
            "fEy1yZYBpHPS2GeIeyMS": {
                "id": "fEy1yZYBpHPS2GeIeyMS",
                "name": "uc5_bestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"UC5.binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"uc5_binary_DataPreprocessing\";\n  }\n\n  \n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"UC5.multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"uc5_multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"UC5/machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"UC5.V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    S1B -> S2B -> S3B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    //V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "mEy5yZYBpHPS2GeI3CMu": {
                "id": "mEy5yZYBpHPS2GeI3CMu",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "nUy5yZYBpHPS2GeI6SNf"
                ],
                "start": "2025-05-13T12:58:42Z",
                "end": "2025-05-13T12:58:53Z"
            }
        },
        {
            "gUy3yZYBpHPS2GeIPyMm": {
                "id": "gUy3yZYBpHPS2GeIPyMm",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "gky3yZYBpHPS2GeIQSP5"
                ],
                "start": "2025-05-13T12:55:51Z",
                "end": "2025-05-13T12:55:59Z"
            }
        },
        {
            "2EzMyZYBpHPS2GeI4iMk": {
                "id": "2EzMyZYBpHPS2GeI4iMk",
                "name": "UC1_surrogate_model",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> TrainModelAPI -> ReadMetrics -> END;\n\n    task TrainModelAPI;\n\n    task ReadMetrics {\n        implementation \"UC1.surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"UC1/**\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModelAPI.FileToRead;\n    TrainModelAPI.jobID --> ReadMetrics.jobID;\n    TrainModelAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"UC1.surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "00zMyZYBpHPS2GeIoyNh": {
                "id": "00zMyZYBpHPS2GeIoyNh",
                "name": "uc5_bestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"UC5.binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"uc5_binary_DataPreprocessing\";\n  }\n\n  \n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"UC5.multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"uc5_multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"UC5/machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"UC5.V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    //START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    START -> S1M -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    //S1B -> S2B -> S3B -> V1;\n    S1B -> S2B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    //V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"UC5.T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"UC5.T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"UC5.I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "1EzMyZYBpHPS2GeIqCNw",
                    "2UzNyZYBpHPS2GeINSMX",
                    "3UzNyZYBpHPS2GeIzSOk"
                ],
                "start": "2025-05-13T13:19:15Z"
            }
        },
        {
            "z0zKyZYBpHPS2GeItiNm": {
                "id": "z0zKyZYBpHPS2GeItiNm",
                "name": "UC1_building_seg",
                "model": "package CS_building_segmentation;\n\nworkflow CS_building_segmentation_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> BuildingSegmentation -> EvaluateModel -> END;\n\n    task ReadData {\n        implementation \"UC1.building_seg.ReadData\";\n    }\n\n    task BuildingSegmentation;\n\n    task EvaluateModel {\n        implementation \"UC1.building_seg.EvaluateModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    ReadData.POINTS --> BuildingSegmentation.POINTS;\n    BuildingSegmentation.POLYGONS --> EvaluateModel.POLYGONS;\n}\n\nworkflow BuildingSegmentation_Alpha from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_Alpha\";\n  }\n}\n\nworkflow BuildingSegmentation_DBSCAN from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_DBSCAN\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of BuildingSegmentation_Alpha {\n        strategy gridsearch;\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param epsilon_vp = enum(200);\n        task BuildingSegmentation {\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n        }\n    }\n\n    space S2 of BuildingSegmentation_DBSCAN {\n        strategy gridsearch;\n        param epsilon_vp = enum(0.5);\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param min_samples_vp = enum(5);\n        task BuildingSegmentation {\n            param epsilon = epsilon_vp;\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n            param min_samples = min_samples_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "0EzKyZYBpHPS2GeIuCMB",
                    "30zOyZYBpHPS2GeIfyPY"
                ],
                "start": "2025-05-13T13:17:07Z",
                "end": "2025-05-13T13:25:25Z"
            }
        },
        {
            "3kzNyZYBpHPS2GeI2SN5": {
                "id": "3kzNyZYBpHPS2GeI2SN5",
                "name": "UC1_surrogate_model",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> TrainModelAPI -> ReadMetrics -> END;\n\n    task TrainModelAPI;\n\n    task ReadMetrics {\n        implementation \"UC1.surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"UC1/**\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModelAPI.FileToRead;\n    TrainModelAPI.jobID --> ReadMetrics.jobID;\n    TrainModelAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"UC1.surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "4kzPyZYBpHPS2GeIViMl": {
                "id": "4kzPyZYBpHPS2GeIViMl",
                "name": "UC1_surrogate_model",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> TrainModelAPI -> ReadMetrics -> END;\n\n    task TrainModelAPI;\n\n    task ReadMetrics {\n        implementation \"UC1.surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"UC1/**\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModelAPI.FileToRead;\n    TrainModelAPI.jobID --> ReadMetrics.jobID;\n    TrainModelAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"UC1.surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "40zSyZYBpHPS2GeIuiPf": {
                "id": "40zSyZYBpHPS2GeIuiPf",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5EzSyZYBpHPS2GeIviN7"
                ],
                "start": "2025-05-13T13:25:53Z",
                "end": "2025-05-13T13:26:01Z"
            }
        },
        {
            "7kzTyZYBpHPS2GeIiiMf": {
                "id": "7kzTyZYBpHPS2GeIiiMf",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n  PlacesData --> ValidateInput.Places;\n  GeofenceData --> ValidateInput.Geofence;\n  MobilitiesData --> ValidateInput.UserMobilities;\n  PredictionsData --> ValidateInput.Predictions;\n  AccuracyThreshold --> ValidateInput.AccThreshold;\n  TimelineMode --> ValidateInput.TimelineMode;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ykzIyZYBpHPS2GeI4SPY": {
                "id": "ykzIyZYBpHPS2GeI4SPY",
                "name": "uc5_bestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"UC5.binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"uc5_binary_DataPreprocessing\";\n  }\n\n  \n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"UC5.multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"uc5_multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"UC5/machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"UC5.V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    //START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    START -> S1M -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    //S1B -> S2B -> S3B -> V1;\n    S1B -> S2B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    //V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"UC5.T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"UC5.T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"UC5.I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "y0zIyZYBpHPS2GeI5yNF"
                ],
                "start": "2025-05-13T13:15:08Z"
            }
        },
        {
            "w0zByZYBpHPS2GeI7SOG": {
                "id": "w0zByZYBpHPS2GeI7SOG",
                "name": "UC1_building_seg",
                "model": "package CS_building_segmentation;\n\nworkflow CS_building_segmentation_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> BuildingSegmentation -> EvaluateModel -> END;\n\n    task ReadData {\n        implementation \"UC1.building_seg.ReadData\";\n    }\n\n    task BuildingSegmentation;\n\n    task EvaluateModel {\n        implementation \"UC1.building_seg.EvaluateModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    ReadData.POINTS --> BuildingSegmentation.POINTS;\n    BuildingSegmentation.POLYGONS --> EvaluateModel.POLYGONS;\n}\n\nworkflow BuildingSegmentation_Alpha from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_Alpha\";\n  }\n}\n\nworkflow BuildingSegmentation_DBSCAN from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_DBSCAN\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of BuildingSegmentation_Alpha {\n        strategy gridsearch;\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param epsilon_vp = enum(200);\n        task BuildingSegmentation {\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n        }\n    }\n\n    space S2 of BuildingSegmentation_DBSCAN {\n        strategy gridsearch;\n        param epsilon_vp = enum(0.5);\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param min_samples_vp = enum(5);\n        task BuildingSegmentation {\n            param epsilon = epsilon_vp;\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n            param min_samples = min_samples_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "xEzByZYBpHPS2GeI7iPu",
                    "x0zFyZYBpHPS2GeIqSMC"
                ],
                "start": "2025-05-13T13:07:32Z"
            }
        },
        {
            "sUy_yZYBpHPS2GeIOSPM": {
                "id": "sUy_yZYBpHPS2GeIOSPM",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "sky_yZYBpHPS2GeIPCOK"
                ],
                "start": "2025-05-13T13:04:34Z",
                "end": "2025-05-13T13:04:42Z"
            }
        },
        {
            "vEy_yZYBpHPS2GeIpSNw": {
                "id": "vEy_yZYBpHPS2GeIpSNw",
                "name": "UC1_building_seg",
                "model": "package CS_building_segmentation;\n\nworkflow CS_building_segmentation_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> BuildingSegmentation -> EvaluateModel -> END;\n\n    task ReadData {\n        implementation \"UC1.building_seg.ReadData\";\n    }\n\n    task BuildingSegmentation;\n\n    task EvaluateModel {\n        implementation \"UC1.building_seg.EvaluateModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/clipped.las\";\n    }\n\n    define output data TrainedModelFolder;\n\n    ReadData.POINTS --> BuildingSegmentation.POINTS;\n    BuildingSegmentation.POLYGONS --> EvaluateModel.POLYGONS;\n}\n\nworkflow BuildingSegmentation_Alpha from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_Alpha\";\n  }\n}\n\nworkflow BuildingSegmentation_DBSCAN from CS_building_segmentation_main {\n  task BuildingSegmentation {\n    implementation \"UC1.building_seg.BuildingSeg_DBSCAN\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of BuildingSegmentation_Alpha {\n        strategy gridsearch;\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param epsilon_vp = enum(200);\n        task BuildingSegmentation {\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n        }\n    }\n\n    space S2 of BuildingSegmentation_DBSCAN {\n        strategy gridsearch;\n        param epsilon_vp = enum(0.5);\n        param m2_thresh_vp = enum(5);\n        param rdp_epsilon_vp = enum(1.2);\n        param min_samples_vp = enum(5);\n        task BuildingSegmentation {\n            param epsilon = epsilon_vp;\n            param m2_thresh = m2_thresh_vp;\n            param rdp_epsilon = rdp_epsilon_vp;\n            param min_samples = min_samples_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "vUy_yZYBpHPS2GeIpyMF",
                    "wEy_yZYBpHPS2GeIviPd"
                ],
                "start": "2025-05-13T13:05:02Z",
                "end": "2025-05-13T13:06:32Z"
            }
        },
        {
            "S0xFypYBpHPS2GeIBCQU": {
                "id": "S0xFypYBpHPS2GeIBCQU",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> BaselineRun -> END;\n  }\n  space S1 of SingleTaskAW2 {\n       strategy randomsearch;\n     runs = 1;\n    }\n}\n\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "TExFypYBpHPS2GeIEiQu": {
                "id": "TExFypYBpHPS2GeIEiQu",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "TUxHypYBpHPS2GeIVCRy": {
                "id": "TUxHypYBpHPS2GeIVCRy",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "RUw8ypYBpHPS2GeIzCS_": {
                "id": "RUw8ypYBpHPS2GeIzCS_",
                "name": "uc5_binary_DataPreprocessing",
                "model": "workflow binary_DataPreprocessing {\n\n  // Task CONNECTIONS\n  START -> AddPadding -> SplitData -> END;\n\n  task SplitData {\n    implementation \"UC5.binary.SplitData\";\n  }\n\n  task AddPadding {\n    implementation \"UC5.binary.AddPadding\";\n  }\n\n  define input data X2;\n  define input data Y;\n  define input data IndicatorList;\n  X2 --> AddPadding.X42;\n  Y --> AddPadding.Y;\n  IndicatorList --> AddPadding.IndicatorList;\n\n  AddPadding.XPad --> SplitData.XPad;\n  AddPadding.YPad --> SplitData.YPad;\n\n  define output data Timestamps42;\n  define output data Features;\n  define output data XTrain;\n  define output data XTest;\n  define output data YTrain;\n  define output data YTest;\n  define output data XPad;\n  define output data YPad;\n\n  SplitData.Timestamps41 --> Timestamps42;\n  SplitData.Features --> Features;\n  SplitData.XTrain --> XTrain;\n  SplitData.XTest --> XTest;\n  SplitData.YTrain --> YTrain;\n  SplitData.YTest --> YTest;\n  SplitData.XPad --> XPad;\n  SplitData.YPad --> YPad;\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Rkw9ypYBpHPS2GeIOiRE": {
                "id": "Rkw9ypYBpHPS2GeIOiRE",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "R0xAypYBpHPS2GeIDyTZ": {
                "id": "R0xAypYBpHPS2GeIDyTZ",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SExAypYBpHPS2GeI2CTc": {
                "id": "SExAypYBpHPS2GeI2CTc",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SUxBypYBpHPS2GeIKiRY": {
                "id": "SUxBypYBpHPS2GeIKiRY",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "REw4ypYBpHPS2GeIACQh": {
                "id": "REw4ypYBpHPS2GeIACQh",
                "name": "uc4_pythia_mode_detection_advanced",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // Input connections\n  LocationsData --> ValidateInput.Locations;\n\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaOptimization {\n  intent OptimizeSegmentation;\n  control {\n    START -> BaselineRun -> KalmanNoiseOptimization -> \n             AccuracyThresholdOptimization -> PlaceDistanceOptimization -> END;\n  }\n  \n  space BaselineRun of pythia_mobility_segmentation {\n    strategy singlerun;\n  }\n  \n  space KalmanNoiseOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param kalman_noise_vp = enum(0.01, 0.05, 0.1);\n    \n    task ApplyKalmanFilter {\n      param process_noise_scale = kalman_noise_vp;\n    }\n  }\n  \n  space AccuracyThresholdOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param accuracy_threshold_vp = enum(30, 50, 70);\n    \n    task ValidateInput {\n      param max_acc_threshold = accuracy_threshold_vp;\n    }\n  }\n  \n  space PlaceDistanceOptimization of pythia_mobility_segmentation {\n    strategy gridsearch;\n    param place_distance_vp = enum(30, 50, 70);\n    \n    task EnforceTimelineContinuity {\n      param place_distance_threshold = place_distance_vp;\n    }\n    \n    task MergeStationarySegments {\n      param max_distance_threshold = place_distance_vp;\n    }\n  }\n  \n  task EvaluationMetrics {\n    implementation \"UC4/improved_mode_detection/ProduceMetrics\";\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SkxEypYBpHPS2GeIKSTL": {
                "id": "SkxEypYBpHPS2GeIKSTL",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n\n  // Configure data\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  \n  define output data FinalSegments;\n\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n  }\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> BaselineRun -> END;\n  }\n  space S1 of SingleTaskAW2 {\n       strategy randomsearch;\n     runs = 1;\n    }\n}\n\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "LkwhypYBpHPS2GeIjyQ7": {
                "id": "LkwhypYBpHPS2GeIjyQ7",
                "name": "uc5_bestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"UC5.binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"UC5.binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"uc5_binary_DataPreprocessing\";\n  }\n\n  \n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"UC5.multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"UC5.multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"uc5_multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"UC5/training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"UC5/machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"UC5.binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"UC5.multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"UC5.V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    //START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    START -> S1M -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    //S1B -> S2B -> S3B -> V1;\n    S1B -> S2B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    //V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(100);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(70);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"UC5.T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"UC5.T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"UC5.I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "L0whypYBpHPS2GeIlCSR",
                    "M0wiypYBpHPS2GeIPyTQ",
                    "N0wiypYBpHPS2GeI5yRa",
                    "OEwiypYBpHPS2GeI_iSn",
                    "PEwjypYBpHPS2GeImSRc",
                    "QEwkypYBpHPS2GeITiTz",
                    "QUwkypYBpHPS2GeIcSQr",
                    "QkwkypYBpHPS2GeImCT-"
                ],
                "start": "2025-05-13T14:52:00Z",
                "end": "2025-05-13T14:56:02Z"
            }
        },
        {
            "-kzTyZYBpHPS2GeI8yMy": {
                "id": "-kzTyZYBpHPS2GeI8yMy",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> BestModelEvaluator;\n    START -> S1M -> S2M -> BestModelEvaluator;\n    // START -> S2M -> BestModelEvaluator;\n\n    // The model is good, we keep the multiclass\n    BestModelEvaluator ?-> V1 { condition \"check_results_more_than 0.75\"};\n\n    // If the condition meet, go to the BINARY ONE\n    BestModelEvaluator ?-> S1B { condition \"check_results_less_or_equal_than 0.75\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> S2B -> V1;\n    // S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    V1 -> SelectBestModelAndPassMachineFiles -> I1 -> END;\n    // V1 -> SelectBestModelAndPassMachineFiles -> END;\n\n    // BestModelEvaluator es best_model_evaluator\n    // V1 es MachineDataLoad\n    // SelectBestModelAndPassMachineFiles es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    //param epochs_vp = range(2, 4);\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(40);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(256);\n    param filters_2_vp = enum(256);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n\n    param epochs_vp = enum(20);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task BestModelEvaluator {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task SelectBestModelAndPassMachineFiles {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-0zTyZYBpHPS2GeI9yOD",
                    "_0zUyZYBpHPS2GeI2iN2",
                    "A0zVyZYBpHPS2GeI1iSe",
                    "BEzWyZYBpHPS2GeIDiSJ",
                    "E0zWyZYBpHPS2GeI-iQ_",
                    "F0zXyZYBpHPS2GeI5yTU",
                    "GEzYyZYBpHPS2GeIRCRT",
                    "GUzYyZYBpHPS2GeIkSRR"
                ],
                "start": "2025-05-13T15:27:11Z"
            }
        },
        {
            "I0zcyZYBpHPS2GeIViQl": {
                "id": "I0zcyZYBpHPS2GeIViQl",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "JEzcyZYBpHPS2GeIWSQI"
                ],
                "start": "2025-05-13T13:36:22Z",
                "end": "2025-05-13T13:37:41Z"
            }
        },
        {
            "CEzWyZYBpHPS2GeIGiTv": {
                "id": "CEzWyZYBpHPS2GeIGiTv",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CUzWyZYBpHPS2GeIHiSZ"
                ],
                "start": "2025-05-13T13:29:34Z",
                "end": "2025-05-13T13:30:14Z"
            }
        },
        {
            "70zTyZYBpHPS2GeIwyO0": {
                "id": "70zTyZYBpHPS2GeIwyO0",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "8EzTyZYBpHPS2GeIxyNQ"
                ],
                "start": "2025-05-13T13:27:00Z",
                "end": "2025-05-13T13:27:08Z"
            }
        },
        {
            "T0zOzZYBpHPS2GeIqCSy": {
                "id": "T0zOzZYBpHPS2GeIqCSy",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FinalSegments --> FinalSegments;\n    configure data FormatOutput {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n    }\n\n\n  \n\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> BaselineRun -> END;\n  }\n  space S1 of SingleTaskAW2 {\n       strategy randomsearch;\n     runs = 1;\n    }\n}\n\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "UEzQzZYBpHPS2GeIjCTj": {
                "id": "UEzQzZYBpHPS2GeIjCTj",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FinalSegments --> FinalSegments;\n    configure data FormatOutput {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n    }\n\n\n  \n\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> S1 -> END;\n  }\n  space S1 of pythia_mobility_segmentation_simple {\n       strategy randomsearch;\n     runs = 1;\n    }\n}\n\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WUzSzpYBpHPS2GeIkSQA": {
                "id": "WUzSzpYBpHPS2GeIkSQA",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WkzWzpYBpHPS2GeItyRV": {
                "id": "WkzWzpYBpHPS2GeItyRV",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "V0zMzpYBpHPS2GeIWSSH": {
                "id": "V0zMzpYBpHPS2GeIWSSH",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n    // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // Task definitions with implementations\n\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n\n  define input data LocationDataSingleDayUser;\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalSegments;\n  FormatOutput.FormattedSegments --> FinalSegments;\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/**\";\n  }\n\n  // Data connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> S1 -> END;\n  }\n  space S1 of pythia_mobility_segmentation_simple {\n    strategy randomsearch;\n    runs = 1;\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WEzNzpYBpHPS2GeI3SRS": {
                "id": "WEzNzpYBpHPS2GeI3SRS",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n    // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // Task definitions with implementations\n\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n\n  define input data LocationDataSingleDayUser;\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalSegments;\n  FormatOutput.FormattedSegments --> FinalSegments;\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/**\";\n  }\n\n  // Data connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> S1 -> END;\n  }\n  space S1 of pythia_mobility_segmentation_simple {\n    strategy randomsearch;\n    runs = 1;\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "TkzJzZYBpHPS2GeITyTD": {
                "id": "TkzJzZYBpHPS2GeITyTD",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FinalSegments --> FinalSegments;\n    configure data FormatOutput {\n    path \"UC4/PythiaOutputs/Giannis015_output.csv\";\n    }\n\n\n  \n\n\n  // Data connections\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n\n  // User context and validation connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n\n  // Final output connection\n  FormatOutput.FormattedSegments --> FinalSegments;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\n\nworkflow pythia_with_extended_kalman from pythia_mobility_segmentation {\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n}\n\nworkflow pythia_with_enhanced_mode_detection from pythia_mobility_segmentation {\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> BaselineRun -> END;\n  }\n  space S1 of SingleTaskAW2 {\n       strategy randomsearch;\n     runs = 1;\n    }\n}\n\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "W0zizpYBpHPS2GeIZiSN": {
                "id": "W0zizpYBpHPS2GeIZiSN",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "UUzWzZYBpHPS2GeI5yQM": {
                "id": "UUzWzZYBpHPS2GeI5yQM",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalSegments;\n  FormatOutput.FormattedSegments --> FinalSegments;\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/**\";\n  }\n\n  // Data connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> S1 -> END;\n  }\n  space S1 of pythia_mobility_segmentation_simple {\n    strategy randomsearch;\n    runs = 1;\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VUzjzZYBpHPS2GeIWyTB": {
                "id": "VUzjzZYBpHPS2GeIWyTB",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/task.py\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext/task.py\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/task.py\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/task.py\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/task.py\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/task.py\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing/task.py\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection/task.py\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix/task.py\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments/task.py\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips/task.py\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints/task.py\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps/task.py\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences/task.py\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection/task.py\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity/task.py\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments/task.py\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips/task.py\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving/task.py\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance/task.py\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps/task.py\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline/task.py\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput/task.py\";\n  }\n\n  // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n  define input data LocationDataSingleDayUser;\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalSegments;\n  FormatOutput.FormattedSegments --> FinalSegments;\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/**\";\n  }\n\n  // Data connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> S1 -> END;\n  }\n  space S1 of pythia_mobility_segmentation_simple {\n    strategy randomsearch;\n    runs = 1;\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VkzMzpYBpHPS2GeIECQW": {
                "id": "VkzMzpYBpHPS2GeIECQW",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n\n    // Task CONNECTIONS - Full Linear Pipeline\n  START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // Task definitions with implementations\n\n  task ValidateInput {\n    implementation \"UC4/improved_mode_detection/ValidateInput/\";\n  }\n\n  task ProcessUserContext {\n    implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n  }\n\n  task PreprocessData {\n    implementation \"UC4/improved_mode_detection/PreprocessData/\";\n  }\n\n  task ApplyKalmanFilter {\n    implementation \"UC4/improved_mode_detection/ApplyKalmanFilter/\";\n  }\n\n  task HybridSegmentation {\n    implementation \"UC4/improved_mode_detection/HybridSegmentation/\";\n  }\n\n  task FixFalsePositives {\n    implementation \"UC4/improved_mode_detection/FixFalsePositives/\";\n  }\n\n  task SyndetiresProcessing {\n    implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n  }\n\n  task ModeDetection {\n    implementation \"UC4/improved_mode_detection/ModeDetection\";\n  }\n\n  task ColdStartFix {\n    implementation \"UC4/improved_mode_detection/ColdStartFix\";\n  }\n\n  task MergeMovingSegments {\n    implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n  }\n\n  task DeleteSmallRoundTrips {\n    implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n  }\n\n  task ApplyMobilityConstraints {\n    implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n  }\n\n  task MergeMissingTripGaps {\n    implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n  }\n\n  task ApplyGeofences {\n    implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n  }\n\n  task PlaceCorrection {\n    implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n  }\n\n  task EnforceTimelineContinuity {\n    implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n  }\n\n  task MergeStationarySegments {\n    implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n  }\n\n  task DetectMissingTrips {\n    implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n  }\n\n  task RemoveShortMoving {\n    implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n  }\n\n  task CalculateDistance {\n    implementation \"UC4/improved_mode_detection/CalculateDistance\";\n  }\n\n  task ResolveOverlaps {\n    implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n  }\n\n  task EnsureCompleteTimeline {\n    implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n  }\n\n  task FormatOutput {\n    implementation \"UC4/improved_mode_detection/FormatOutput\";\n  }\n\n  // *** INPUT/OUTPUT DATA DEFINITIONS ***\n\n  define input data LocationDataSingleDayUser;\n  LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n  configure data LocationDataSingleDayUser {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalSegments;\n  FormatOutput.FormattedSegments --> FinalSegments;\n  configure data FinalSegments {\n    path \"UC4/PythiaOutputs/**\";\n  }\n\n  // Data connections\n  ValidateInput.UserId --> ProcessUserContext.UserId;\n  ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n  ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n  ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n  ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n  ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n  ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n  ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n  // Data processing connections\n  PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n  ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n  HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n  FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n  // Core segmentation result connections\n  SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n  SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n  SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n  // User context connections for refinement\n  ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n  ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n  ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n  ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n  ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n  ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n  ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n  ProcessUserContext.UserId --> FormatOutput.UserId;\n\n  // Refinement pipeline connections\n  ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n  ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n  MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n  DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n  ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n  MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n  ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n  PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n  EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n  MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n  DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n  RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n  CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n  ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n  EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n  control {\n    START -> S1 -> END;\n  }\n  space S1 of pythia_mobility_segmentation_simple {\n    strategy randomsearch;\n    runs = 1;\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "UkzXzZYBpHPS2GeIESRl": {
                "id": "UkzXzZYBpHPS2GeIESRl",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "U0zXzZYBpHPS2GeIEiSA"
                ],
                "start": "2025-05-14T08:09:06Z",
                "end": "2025-05-14T08:09:15Z"
            }
        },
        {
            "ZUztzpYBpHPS2GeIlSR-": {
                "id": "ZUztzpYBpHPS2GeIlSR-",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ZkzxzpYBpHPS2GeITyS9": {
                "id": "ZkzxzpYBpHPS2GeITyS9",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "a0wQz5YBpHPS2GeIfCQD": {
                "id": "a0wQz5YBpHPS2GeIfCQD",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bEwRz5YBpHPS2GeIAyTf": {
                "id": "bEwRz5YBpHPS2GeIAyTf",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "YEzlzpYBpHPS2GeIMyQj": {
                "id": "YEzlzpYBpHPS2GeIMyQj",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YUzlzpYBpHPS2GeINCRs"
                ],
                "start": "2025-05-14T13:04:09Z",
                "end": "2025-05-14T13:04:18Z"
            }
        },
        {
            "Y0zlzpYBpHPS2GeIniSF": {
                "id": "Y0zlzpYBpHPS2GeIniSF",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ZEzmzpYBpHPS2GeIpCSO": {
                "id": "ZEzmzpYBpHPS2GeIpCSO",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Z0wFz5YBpHPS2GeICiS6": {
                "id": "Z0wFz5YBpHPS2GeICiS6",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "aEwFz5YBpHPS2GeICyTL"
                ],
                "start": "2025-05-14T13:38:56Z",
                "end": "2025-05-14T13:39:04Z"
            }
        },
        {
            "akwIz5YBpHPS2GeICCTC": {
                "id": "akwIz5YBpHPS2GeICCTC",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "XEzjzpYBpHPS2GeIFiR4": {
                "id": "XEzjzpYBpHPS2GeIFiR4",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "XUzjzpYBpHPS2GeIsCR5": {
                "id": "XUzjzpYBpHPS2GeIsCR5",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XkzjzpYBpHPS2GeIsSSI"
                ],
                "start": "2025-05-14T13:02:30Z",
                "end": "2025-05-14T13:02:40Z"
            }
        },
        {
            "nExw55YBpHPS2GeIoST2": {
                "id": "nExw55YBpHPS2GeIoST2",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "nUxx55YBpHPS2GeIxyQo": {
                "id": "nUxx55YBpHPS2GeIxyQo",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "nkxx55YBpHPS2GeI7iSO": {
                "id": "nkxx55YBpHPS2GeI7iSO",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "n0x255YBpHPS2GeI0iSj": {
                "id": "n0x255YBpHPS2GeI0iSj",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "oEx255YBpHPS2GeI1CRO"
                ],
                "start": "2025-05-19T07:34:06Z",
                "end": "2025-05-19T07:34:35Z"
            }
        },
        {
            "bUzu0pYBpHPS2GeIqCQB": {
                "id": "bUzu0pYBpHPS2GeIqCQB",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bkzu0pYBpHPS2GeIqiQa"
                ],
                "start": "2025-05-15T07:52:58Z",
                "end": "2025-05-15T07:53:31Z"
            }
        },
        {
            "jkwX05YBpHPS2GeIOiSL": {
                "id": "jkwX05YBpHPS2GeIOiSL",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "j0wX05YBpHPS2GeIPCQ1"
                ],
                "start": "2025-05-15T08:37:17Z",
                "end": "2025-05-15T08:37:46Z"
            }
        },
        {
            "eEwT05YBpHPS2GeIJiRJ": {
                "id": "eEwT05YBpHPS2GeIJiRJ",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "eUwT05YBpHPS2GeIJyTy"
                ],
                "start": "2025-05-15T08:32:49Z",
                "end": "2025-05-15T08:33:18Z"
            }
        },
        {
            "g0wW05YBpHPS2GeIRCTU": {
                "id": "g0wW05YBpHPS2GeIRCTU",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "hEwW05YBpHPS2GeIRiSE"
                ],
                "start": "2025-05-15T08:36:14Z",
                "end": "2025-05-15T08:36:42Z"
            }
        },
        {
            "tkx855YBpHPS2GeIYySJ": {
                "id": "tkx855YBpHPS2GeIYySJ",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "t0x955YBpHPS2GeIlCTR": {
                "id": "t0x955YBpHPS2GeIlCTR",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "uEyL55YBpHPS2GeIcyRx": {
                "id": "uEyL55YBpHPS2GeIcyRx",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "uUyO55YBpHPS2GeI-yR1": {
                "id": "uUyO55YBpHPS2GeI-yR1",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ukyS55YBpHPS2GeIZSSY": {
                "id": "ukyS55YBpHPS2GeIZSSY",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "qkx455YBpHPS2GeIuyQA": {
                "id": "qkx455YBpHPS2GeIuyQA",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "q0x455YBpHPS2GeIvCSk"
                ],
                "start": "2025-05-19T07:36:11Z",
                "end": "2025-05-19T07:36:39Z"
            }
        },
        {
            "tUx855YBpHPS2GeINiTo": {
                "id": "tUx855YBpHPS2GeINiTo",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "u0yV55YBpHPS2GeI8CSh": {
                "id": "u0yV55YBpHPS2GeI8CSh",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "vEyV55YBpHPS2GeI8STW"
                ],
                "start": "2025-05-19T08:08:05Z",
                "end": "2025-05-19T08:08:17Z"
            }
        },
        {
            "-0yd55YBpHPS2GeIvSS1": {
                "id": "-0yd55YBpHPS2GeIvSS1",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 8);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "_Eyd55YBpHPS2GeIviRp",
                    "Bkyd55YBpHPS2GeIvyX4",
                    "EEyd55YBpHPS2GeIwSWP"
                ],
                "start": "2025-05-19T11:16:37Z",
                "end": "2025-05-19T11:19:05Z"
            }
        },
        {
            "vkyY55YBpHPS2GeI5yQc": {
                "id": "vkyY55YBpHPS2GeI5yQc",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "v0yY55YBpHPS2GeI5yTQ",
                    "yUyY55YBpHPS2GeI6SRT",
                    "00yY55YBpHPS2GeI6yRR",
                    "3UyY55YBpHPS2GeI7CTo",
                    "50yY55YBpHPS2GeI7iSK",
                    "8UyY55YBpHPS2GeI8CQh"
                ],
                "start": "2025-05-19T11:11:20Z",
                "end": "2025-05-19T11:16:15Z"
            }
        },
        {
            "HEyx55YBpHPS2GeIMSWc": {
                "id": "HEyx55YBpHPS2GeIMSWc",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "HUyx55YBpHPS2GeINCX2": {
                "id": "HUyx55YBpHPS2GeINCX2",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Hkyx55YBpHPS2GeIOCVO": {
                "id": "Hkyx55YBpHPS2GeIOCVO",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "H0yy55YBpHPS2GeIUiVH": {
                "id": "H0yy55YBpHPS2GeIUiVH",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Gkyq55YBpHPS2GeIeCU6": {
                "id": "Gkyq55YBpHPS2GeIeCU6",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"tasks/UC2.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"tasks/UC2.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"tasks/UC2.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"tasks/UC2.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"tasks/UC2.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"datasets/UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"tasks/UC2.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "G0yx55YBpHPS2GeIJyWu": {
                "id": "G0yx55YBpHPS2GeIJyWu",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "IEy555YBpHPS2GeIXiU-": {
                "id": "IEy555YBpHPS2GeIXiU-",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "QUy_55YBpHPS2GeIJSVf": {
                "id": "QUy_55YBpHPS2GeIJSVf",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Qky_55YBpHPS2GeIriWK": {
                "id": "Qky_55YBpHPS2GeIriWK",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "QEy855YBpHPS2GeIXCUj": {
                "id": "QEy855YBpHPS2GeIXCUj",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Q0zE55YBpHPS2GeINSXh": {
                "id": "Q0zE55YBpHPS2GeINSXh",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "REzE55YBpHPS2GeINyVc",
                    "S0zE55YBpHPS2GeINyXB",
                    "UkzE55YBpHPS2GeIOCUq",
                    "WUzE55YBpHPS2GeIOCV1",
                    "YEzE55YBpHPS2GeIOCXE",
                    "Z0zE55YBpHPS2GeIOSUQ"
                ],
                "start": "2025-05-19T08:58:38Z"
            }
        },
        {
            "tkzS55YBpHPS2GeIHyXH": {
                "id": "tkzS55YBpHPS2GeIHyXH",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "t0zS55YBpHPS2GeIIiVh",
                    "vkzS55YBpHPS2GeIIiXP"
                ],
                "start": "2025-05-19T09:13:50Z",
                "end": "2025-05-19T09:21:53Z"
            }
        },
        {
            "1Eze55YBpHPS2GeI5yUl": {
                "id": "1Eze55YBpHPS2GeI5yUl",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "1Uze55YBpHPS2GeI6SUD",
                    "3Eze55YBpHPS2GeI6SV0"
                ],
                "start": "2025-05-19T09:27:47Z",
                "end": "2025-05-19T09:35:38Z"
            }
        },
        {
            "AUyz6JYBpHPS2GeI3iYm": {
                "id": "AUyz6JYBpHPS2GeI3iYm",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Akyz6JYBpHPS2GeI3yb5",
                    "CUyz6JYBpHPS2GeI4CZL"
                ],
                "start": "2025-05-19T13:20:24Z",
                "end": "2025-05-19T13:26:58Z"
            }
        },
        {
            "mUzL55YBpHPS2GeI5CUM": {
                "id": "mUzL55YBpHPS2GeI5CUM",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "mkzL55YBpHPS2GeI5yVc",
                    "oUzL55YBpHPS2GeI6CUh",
                    "qEzL55YBpHPS2GeI6CWT",
                    "r0zL55YBpHPS2GeI6CX2"
                ],
                "start": "2025-05-19T09:07:01Z",
                "end": "2025-05-19T09:11:01Z"
            }
        },
        {
            "bkzI55YBpHPS2GeIGyWr": {
                "id": "bkzI55YBpHPS2GeIGyWr",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "b0zI55YBpHPS2GeIHSWo",
                    "dkzI55YBpHPS2GeIHiUd",
                    "fUzI55YBpHPS2GeIHiWS",
                    "hEzI55YBpHPS2GeIHiXt",
                    "i0zI55YBpHPS2GeIHyVB",
                    "kkzI55YBpHPS2GeIHyWN"
                ],
                "start": "2025-05-19T09:02:53Z",
                "end": "2025-05-19T09:07:46Z"
            }
        },
        {
            "IUy555YBpHPS2GeIcCVb": {
                "id": "IUy555YBpHPS2GeIcCVb",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 8);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "workflow_ids": [
                    "Iky555YBpHPS2GeIcSUa",
                    "LEy555YBpHPS2GeIciWX",
                    "Nky555YBpHPS2GeIdCVO"
                ],
                "start": "2025-05-19T11:46:52Z",
                "end": "2025-05-19T12:11:00Z"
            }
        },
        {
            "40yp6JYBpHPS2GeIkiVl": {
                "id": "40yp6JYBpHPS2GeIkiVl",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5Eyp6JYBpHPS2GeIlCVW",
                    "60yp6JYBpHPS2GeIlCWg"
                ],
                "start": "2025-05-19T13:09:09Z",
                "end": "2025-05-19T13:13:40Z"
            }
        },
        {
            "xUzZ55YBpHPS2GeItiXh": {
                "id": "xUzZ55YBpHPS2GeItiXh",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "xkzZ55YBpHPS2GeIuSVB",
                    "zUzZ55YBpHPS2GeIuSWb"
                ],
                "start": "2025-05-19T09:22:07Z",
                "end": "2025-05-19T09:24:00Z"
            }
        },
        {
            "EEy-6JYBpHPS2GeI5yaW": {
                "id": "EEy-6JYBpHPS2GeI5yaW",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "EUy-6JYBpHPS2GeI6Sag",
                    "GEy-6JYBpHPS2GeI6Sb4"
                ],
                "start": "2025-05-19T13:32:27Z",
                "end": "2025-05-19T13:34:59Z"
            }
        },
        {
            "8kyv6JYBpHPS2GeIviXu": {
                "id": "8kyv6JYBpHPS2GeIviXu",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "80yv6JYBpHPS2GeIwCXE",
                    "-kyv6JYBpHPS2GeIwSUc"
                ],
                "start": "2025-05-19T13:15:54Z",
                "end": "2025-05-19T13:16:36Z"
            }
        },
        {
            "H0zJ6JYBpHPS2GeImSbB": {
                "id": "H0zJ6JYBpHPS2GeImSbB",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "IEzJ6JYBpHPS2GeImyad",
                    "J0zJ6JYBpHPS2GeInCYP"
                ],
                "start": "2025-05-19T13:44:08Z",
                "end": "2025-05-19T13:50:33Z"
            }
        },
        {
            "LkzQ6JYBpHPS2GeIIya-": {
                "id": "LkzQ6JYBpHPS2GeIIya-",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "L0zQ6JYBpHPS2GeIJSbn",
                    "NkzQ6JYBpHPS2GeIJiZJ"
                ],
                "start": "2025-05-19T13:51:17Z",
                "end": "2025-05-19T13:57:37Z"
            }
        },
        {
            "TExS7ZYBpHPS2GeIrSZ5": {
                "id": "TExS7ZYBpHPS2GeIrSZ5",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "TUxS7ZYBpHPS2GeIryZY",
                    "VExS7ZYBpHPS2GeIrybD"
                ],
                "start": "2025-05-20T10:52:21Z",
                "end": "2025-05-20T10:58:53Z"
            }
        },
        {
            "bUxe7ZYBpHPS2GeISyYT": {
                "id": "bUxe7ZYBpHPS2GeISyYT",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bkxe7ZYBpHPS2GeITSYm",
                    "dUxe7ZYBpHPS2GeITSaT"
                ],
                "start": "2025-05-20T11:05:02Z",
                "end": "2025-05-20T11:11:34Z"
            }
        },
        {
            "PUzi6JYBpHPS2GeI7ybU": {
                "id": "PUzi6JYBpHPS2GeI7ybU",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Pkzi6JYBpHPS2GeI8Sat",
                    "RUzi6JYBpHPS2GeI8iYt"
                ],
                "start": "2025-05-19T14:11:49Z",
                "end": "2025-05-19T14:14:41Z"
            }
        },
        {
            "Xkxa7ZYBpHPS2GeIuya2": {
                "id": "Xkxa7ZYBpHPS2GeIuya2",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "X0xa7ZYBpHPS2GeIvSaN",
                    "Zkxa7ZYBpHPS2GeIvSbw"
                ],
                "start": "2025-05-20T11:01:09Z",
                "end": "2025-05-20T11:04:33Z"
            }
        },
        {
            "fExk7ZYBpHPS2GeIZCbV": {
                "id": "fExk7ZYBpHPS2GeIZCbV",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fUxk7ZYBpHPS2GeIZias",
                    "hExk7ZYBpHPS2GeIZyYJ"
                ],
                "start": "2025-05-20T11:11:42Z",
                "end": "2025-05-20T11:14:31Z"
            }
        },
        {
            "W0xZ7ZYBpHPS2GeIJSbz": {
                "id": "W0xZ7ZYBpHPS2GeIJSbz",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XExZ7ZYBpHPS2GeIJyaw"
                ],
                "start": "2025-05-20T10:59:25Z",
                "end": "2025-05-20T10:59:35Z"
            }
        },
        {
            "5EyH7ZYBpHPS2GeIvSb7": {
                "id": "5EyH7ZYBpHPS2GeIvSb7",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5UyH7ZYBpHPS2GeIvybm",
                    "6UyI7ZYBpHPS2GeIJyay"
                ],
                "start": "2025-05-20T14:50:19Z",
                "end": "2025-05-20T14:50:51Z"
            }
        },
        {
            "2Ex37ZYBpHPS2GeIlyZ-": {
                "id": "2Ex37ZYBpHPS2GeIlyZ-",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "2Ux37ZYBpHPS2GeImSaO",
                    "3Ux37ZYBpHPS2GeIpCZZ"
                ],
                "start": "2025-05-20T14:32:40Z",
                "end": "2025-05-20T14:32:44Z"
            }
        },
        {
            "3kyF7ZYBpHPS2GeIySbC": {
                "id": "3kyF7ZYBpHPS2GeIySbC",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "30yF7ZYBpHPS2GeIyybQ",
                    "40yF7ZYBpHPS2GeI1iaE"
                ],
                "start": "2025-05-20T14:48:10Z",
                "end": "2025-05-20T14:48:14Z"
            }
        },
        {
            "6kzZ7ZYBpHPS2GeI_iZU": {
                "id": "6kzZ7ZYBpHPS2GeI_iZU",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "60za7ZYBpHPS2GeIACZd",
                    "8kza7ZYBpHPS2GeIACbW"
                ],
                "start": "2025-05-20T13:20:09Z",
                "end": "2025-05-20T13:25:07Z"
            }
        },
        {
            "uUxu7ZYBpHPS2GeIOSYD": {
                "id": "uUxu7ZYBpHPS2GeIOSYD",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 8);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "ukxu7ZYBpHPS2GeIOSap",
                    "xExu7ZYBpHPS2GeIOyZR",
                    "zkxu7ZYBpHPS2GeIPCbu"
                ],
                "start": "2025-05-20T14:22:26Z"
            }
        },
        {
            "i0xn7ZYBpHPS2GeIzCb2": {
                "id": "i0xn7ZYBpHPS2GeIzCb2",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jExn7ZYBpHPS2GeIzibM",
                    "k0xn7ZYBpHPS2GeIzyY5"
                ],
                "start": "2025-05-20T11:15:25Z",
                "end": "2025-05-20T11:16:51Z"
            }
        },
        {
            "-Uze7ZYBpHPS2GeIzSZj": {
                "id": "-Uze7ZYBpHPS2GeIzSZj",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-kze7ZYBpHPS2GeIzyY3",
                    "AUze7ZYBpHPS2GeIzye2"
                ],
                "start": "2025-05-20T13:25:24Z",
                "end": "2025-05-20T13:29:57Z"
            }
        },
        {
            "CEzj7ZYBpHPS2GeIBycB": {
                "id": "CEzj7ZYBpHPS2GeIBycB",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CUzj7ZYBpHPS2GeICScS",
                    "EEzj7ZYBpHPS2GeICSdr"
                ],
                "start": "2025-05-20T13:30:01Z",
                "end": "2025-05-20T13:30:59Z"
            }
        },
        {
            "mkxs7ZYBpHPS2GeIfSbc": {
                "id": "mkxs7ZYBpHPS2GeIfSbc",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task split_dataset {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"I2CAT.Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"I2CAT.WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 2);\n        param n_estimators = range(5, 11, 8);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "running",
                "workflow_ids": [
                    "m0xs7ZYBpHPS2GeIfiaK",
                    "pUxs7ZYBpHPS2GeIfyb3",
                    "r0xs7ZYBpHPS2GeIgSZ8"
                ],
                "start": "2025-05-20T14:20:33Z"
            }
        },
        {
            "F0zk7ZYBpHPS2GeI2Cfr": {
                "id": "F0zk7ZYBpHPS2GeI2Cfr",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GEzk7ZYBpHPS2GeI2ifC",
                    "H0zk7ZYBpHPS2GeI2yc6"
                ],
                "start": "2025-05-20T13:32:00Z",
                "end": "2025-05-20T13:37:57Z"
            }
        },
        {
            "Jkzx7ZYBpHPS2GeIzCdy": {
                "id": "Jkzx7ZYBpHPS2GeIzCdy",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "J0zx7ZYBpHPS2GeIzid3",
                    "Lkzx7ZYBpHPS2GeIzifl"
                ],
                "start": "2025-05-20T13:46:09Z",
                "end": "2025-05-20T13:54:56Z"
            }
        },
        {
            "U0wP7pYBpHPS2GeITicT": {
                "id": "U0wP7pYBpHPS2GeITicT",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "VEwP7pYBpHPS2GeITyfz",
                    "W0wP7pYBpHPS2GeIUCd0"
                ],
                "start": "2025-05-20T14:18:23Z",
                "end": "2025-05-20T14:21:43Z"
            }
        },
        {
            "REwM7pYBpHPS2GeICCfn": {
                "id": "REwM7pYBpHPS2GeICCfn",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "RUwM7pYBpHPS2GeICifZ",
                    "TEwM7pYBpHPS2GeICydS"
                ],
                "start": "2025-05-20T14:14:48Z",
                "end": "2025-05-20T14:15:51Z"
            }
        },
        {
            "YkwT7pYBpHPS2GeIxycK": {
                "id": "YkwT7pYBpHPS2GeIxycK",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Y0wT7pYBpHPS2GeIyCfd",
                    "akwT7pYBpHPS2GeIySc8"
                ],
                "start": "2025-05-20T14:23:16Z",
                "end": "2025-05-20T14:26:17Z"
            }
        },
        {
            "NUwC7pYBpHPS2GeI1Sdq": {
                "id": "NUwC7pYBpHPS2GeI1Sdq",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "NkwC7pYBpHPS2GeI1ydG",
                    "PUwC7pYBpHPS2GeI1yex"
                ],
                "start": "2025-05-20T14:04:45Z",
                "end": "2025-05-20T14:04:54Z"
            }
        },
        {
            "cUwa7pYBpHPS2GeIbycB": {
                "id": "cUwa7pYBpHPS2GeIbycB",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ckwa7pYBpHPS2GeIcScA",
                    "eUwa7pYBpHPS2GeIcSdk"
                ],
                "start": "2025-05-20T14:30:32Z",
                "end": "2025-05-20T14:34:33Z"
            }
        },
        {
            "gEwe7pYBpHPS2GeIpCdu": {
                "id": "gEwe7pYBpHPS2GeIpCdu",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "gUwe7pYBpHPS2GeIpidH",
                    "iEwe7pYBpHPS2GeIpiea"
                ],
                "start": "2025-05-20T14:35:08Z",
                "end": "2025-05-20T14:37:59Z"
            }
        },
        {
            "9UxH8pYBpHPS2GeIESdK": {
                "id": "9UxH8pYBpHPS2GeIESdK",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 16, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,1);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9kxH8pYBpHPS2GeIEycq",
                    "_UxH8pYBpHPS2GeIEyeV",
                    "BExH8pYBpHPS2GeIEyjm",
                    "C0xH8pYBpHPS2GeIFCg4",
                    "EkxH8pYBpHPS2GeIFCiJ",
                    "GUxH8pYBpHPS2GeIFCja",
                    "IExH8pYBpHPS2GeIFShD",
                    "J0xH8pYBpHPS2GeIFSiw",
                    "LkxH8pYBpHPS2GeIFigQ",
                    "NUxH8pYBpHPS2GeIFih-",
                    "PExH8pYBpHPS2GeIFijO",
                    "Q0xH8pYBpHPS2GeIFygl"
                ],
                "start": "2025-05-21T09:57:46Z",
                "end": "2025-05-21T10:10:03Z"
            }
        },
        {
            "oEwv8pYBpHPS2GeIlye7": {
                "id": "oEwv8pYBpHPS2GeIlye7",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 16, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,1);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "oUwv8pYBpHPS2GeImSeP",
                    "qEwv8pYBpHPS2GeImSfs",
                    "r0wv8pYBpHPS2GeImidf",
                    "tkwv8pYBpHPS2GeImifZ",
                    "vUwv8pYBpHPS2GeImyc4",
                    "xEwv8pYBpHPS2GeImyew",
                    "y0wv8pYBpHPS2GeInCcn",
                    "0kwv8pYBpHPS2GeInCef",
                    "2Uwv8pYBpHPS2GeInScc",
                    "4Ewv8pYBpHPS2GeInSeA",
                    "50wv8pYBpHPS2GeInSfa",
                    "7kwv8pYBpHPS2GeInic5"
                ],
                "start": "2025-05-21T09:32:07Z",
                "end": "2025-05-21T09:44:15Z"
            }
        },
        {
            "WUx68pYBpHPS2GeIxijg": {
                "id": "WUx68pYBpHPS2GeIxijg",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Wkx78pYBpHPS2GeIrSjg": {
                "id": "Wkx78pYBpHPS2GeIrSjg",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "W0x-8pYBpHPS2GeIjygl": {
                "id": "W0x-8pYBpHPS2GeIjygl",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WEx68pYBpHPS2GeIOyhZ": {
                "id": "WEx68pYBpHPS2GeIOyhZ",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Vkx38pYBpHPS2GeI-ig1": {
                "id": "Vkx38pYBpHPS2GeI-ig1",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "V0x48pYBpHPS2GeI-yjp": {
                "id": "V0x48pYBpHPS2GeI-yjp",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "H0zB8pYBpHPS2GeItSmz": {
                "id": "H0zB8pYBpHPS2GeItSmz",
                "name": "uc4_pythia_mode_detection_simple",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task ProcessUserContext {\n        implementation \"UC4/improved_mode_detection/ProcessUserContext\";\n    }\n\n    task PreprocessData {\n        implementation \"UC4/improved_mode_detection/PreprocessData\";\n    }\n\n    task ApplyKalmanFilter {\n        implementation \"UC4/improved_mode_detection/ApplyKalmanFilter\";\n    }\n\n    task HybridSegmentation {\n        implementation \"UC4/improved_mode_detection/HybridSegmentation\";\n    }\n\n    task FixFalsePositives {\n        implementation \"UC4/improved_mode_detection/FixFalsePositives\";\n    }\n\n    task SyndetiresProcessing {\n        implementation \"UC4/improved_mode_detection/SyndetiresProcessing\";\n    }\n\n    task ModeDetection {\n        implementation \"UC4/improved_mode_detection/ModeDetection\";\n    }\n\n    task ColdStartFix {\n        implementation \"UC4/improved_mode_detection/ColdStartFix\";\n    }\n\n    task MergeMovingSegments {\n        implementation \"UC4/improved_mode_detection/MergeMovingSegments\";\n    }\n\n    task DeleteSmallRoundTrips {\n        implementation \"UC4/improved_mode_detection/DeleteSmallRoundTrips\";\n    }\n\n    task ApplyMobilityConstraints {\n        implementation \"UC4/improved_mode_detection/ApplyMobilityConstraints\";\n    }\n\n    task MergeMissingTripGaps {\n        implementation \"UC4/improved_mode_detection/MergeMissingTripGaps\";\n    }\n\n    task ApplyGeofences {\n        implementation \"UC4/improved_mode_detection/ApplyGeofences\";\n    }\n\n    task PlaceCorrection {\n        implementation \"UC4/improved_mode_detection/PlaceCorrection\";\n    }\n\n    task EnforceTimelineContinuity {\n        implementation \"UC4/improved_mode_detection/EnforceTimelineContinuity\";\n    }\n\n    task MergeStationarySegments {\n        implementation \"UC4/improved_mode_detection/MergeStationarySegments\";\n    }\n\n    task DetectMissingTrips {\n        implementation \"UC4/improved_mode_detection/DetectMissingTrips\";\n    }\n\n    task RemoveShortMoving {\n        implementation \"UC4/improved_mode_detection/RemoveShortMoving\";\n    }\n\n    task CalculateDistance {\n        implementation \"UC4/improved_mode_detection/CalculateDistance\";\n    }\n\n    task ResolveOverlaps {\n        implementation \"UC4/improved_mode_detection/ResolveOverlaps\";\n    }\n\n    task EnsureCompleteTimeline {\n        implementation \"UC4/improved_mode_detection/EnsureCompleteTimeline\";\n    }\n\n    task FormatOutput {\n        implementation \"UC4/improved_mode_detection/FormatOutput\";\n    }\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput -> ProcessUserContext -> PreprocessData -> ApplyKalmanFilter -> \n           HybridSegmentation -> FixFalsePositives -> SyndetiresProcessing -> \n           ModeDetection -> ColdStartFix -> MergeMovingSegments -> DeleteSmallRoundTrips -> \n           ApplyMobilityConstraints -> MergeMissingTripGaps ->\n           ApplyGeofences -> PlaceCorrection -> EnforceTimelineContinuity ->\n           MergeStationarySegments -> DetectMissingTrips -> RemoveShortMoving -> \n           CalculateDistance -> ResolveOverlaps -> EnsureCompleteTimeline -> \n           FormatOutput -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n    define output data FinalSegments;\n    FormatOutput.FormattedSegments --> FinalSegments;\n    configure data FinalSegments {\n        path \"UC4/PythiaOutputs/**\";\n    }\n\n    // Data connections\n    ValidateInput.UserId --> ProcessUserContext.UserId;\n    ValidateInput.ValidatedLocations --> PreprocessData.Locations;\n    ValidateInput.ValidatedPlaces --> ProcessUserContext.Places;\n    ValidateInput.ValidatedGeofence --> ProcessUserContext.Geofence;\n    ValidateInput.ValidatedMobilities --> ProcessUserContext.UserMobilities;\n    ValidateInput.ValidatedPredictions --> ProcessUserContext.Predictions;\n    ValidateInput.ValidatedAccThreshold --> PreprocessData.AccuracyThreshold;\n    ValidateInput.ValidatedTimelineMode --> EnsureCompleteTimeline.TimelineMode;\n\n    // Data processing connections\n    PreprocessData.Preprocessed --> ApplyKalmanFilter.Preprocessed;\n    ApplyKalmanFilter.Filtered --> HybridSegmentation.Filtered;\n    HybridSegmentation.Segmented --> FixFalsePositives.Segmented;\n    FixFalsePositives.FixedSegmented --> SyndetiresProcessing.Segmented;\n\n    // Core segmentation result connections\n    SyndetiresProcessing.TripsDf --> ModeDetection.TripsDf;\n    SyndetiresProcessing.DicSegments --> ModeDetection.DicSegments;\n    SyndetiresProcessing.DfSyndetiresOutput --> ModeDetection.DfForMode;\n  \n    // User context connections for refinement\n    ProcessUserContext.MobilitiesList --> ModeDetection.Mobilities;\n    ProcessUserContext.GeofenceEvents --> ModeDetection.GeofenceEvents;\n    ProcessUserContext.MobilitiesList --> ApplyMobilityConstraints.Mobilities;\n    ProcessUserContext.ProcessedPlaces --> ApplyGeofences.Places;\n    ProcessUserContext.ProcessedPlaces --> PlaceCorrection.Places;\n    ProcessUserContext.ProcessedPlaces --> EnsureCompleteTimeline.Places;\n    ProcessUserContext.GeofenceEvents --> ApplyGeofences.GeofenceEvents;\n    ProcessUserContext.UserId --> FormatOutput.UserId;\n\n    // Refinement pipeline connections\n    ModeDetection.RefinedDicTrips --> ColdStartFix.DicTrips;\n    ColdStartFix.FixedDicTrips --> MergeMovingSegments.DicTrips;\n    MergeMovingSegments.MergedDicTrips --> DeleteSmallRoundTrips.DicTrips;\n    DeleteSmallRoundTrips.FilteredDicTrips --> ApplyMobilityConstraints.DicTrips;\n    ApplyMobilityConstraints.ConstrainedDicTrips --> MergeMissingTripGaps.DicTrips;\n    MergeMissingTripGaps.MergedDicTrips --> ApplyGeofences.DicTrips;\n    ApplyGeofences.GeofencedDicTrips --> PlaceCorrection.DicTrips;\n    PlaceCorrection.CorrectedDicTrips --> EnforceTimelineContinuity.DicTrips;\n    EnforceTimelineContinuity.ContinuousDicTrips --> MergeStationarySegments.DicTrips;\n    MergeStationarySegments.MergedDicTrips --> DetectMissingTrips.DicTrips;\n    DetectMissingTrips.CompletedDicTrips --> RemoveShortMoving.DicTrips;\n    RemoveShortMoving.CleanedDicTrips --> CalculateDistance.DicTrips;\n    CalculateDistance.DistanceCalculatedDicTrips --> ResolveOverlaps.DicTrips;\n    ResolveOverlaps.OverlapResolvedDicTrips --> EnsureCompleteTimeline.DicTrips;\n    EnsureCompleteTimeline.CompletedDicTrips --> FormatOutput.CompletedDicTrips;\n}\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "XEyU8pYBpHPS2GeIBSiu": {
                "id": "XEyU8pYBpHPS2GeIBSiu",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 16, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,1);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XUyU8pYBpHPS2GeIByh4",
                    "ZEyU8pYBpHPS2GeIByjc",
                    "a0yU8pYBpHPS2GeICCgy",
                    "ckyU8pYBpHPS2GeICCiI",
                    "eUyU8pYBpHPS2GeICCjz",
                    "gEyU8pYBpHPS2GeICShj",
                    "h0yU8pYBpHPS2GeICSjM",
                    "jkyU8pYBpHPS2GeICig7",
                    "lUyU8pYBpHPS2GeICiim",
                    "nEyU8pYBpHPS2GeICygL",
                    "o0yU8pYBpHPS2GeICyh5",
                    "qkyU8pYBpHPS2GeICyjm"
                ],
                "start": "2025-05-21T11:21:49Z",
                "end": "2025-05-21T11:33:39Z"
            }
        },
        {
            "vkyi8pYBpHPS2GeI7ig1": {
                "id": "vkyi8pYBpHPS2GeI7ig1",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 16, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,1);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "v0yi8pYBpHPS2GeI8Cgi",
                    "xkyi8pYBpHPS2GeI8CiA",
                    "zUyi8pYBpHPS2GeI8Cjj",
                    "1Eyi8pYBpHPS2GeI8ShC",
                    "20yi8pYBpHPS2GeI8Siu",
                    "4kyi8pYBpHPS2GeI8igN",
                    "6Uyi8pYBpHPS2GeI8ihd",
                    "8Eyi8pYBpHPS2GeI8iit",
                    "90yi8pYBpHPS2GeI8ygE",
                    "_kyi8pYBpHPS2GeI8yhR",
                    "BUyi8pYBpHPS2GeI8ymh",
                    "DEyi8pYBpHPS2GeI8yn7"
                ],
                "start": "2025-05-21T11:38:06Z",
                "end": "2025-05-21T11:49:51Z"
            }
        },
        {
            "RUzh8pYBpHPS2GeIdimg": {
                "id": "RUzh8pYBpHPS2GeIdimg",
                "name": "uc4_pythia_mode_detection_simple_test",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation_simple_1 {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput ->  END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation_simple_1 {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple_1 {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "vUyg8pYBpHPS2GeInShU": {
                "id": "vUyg8pYBpHPS2GeInShU",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "LUzZ8pYBpHPS2GeIuikD": {
                "id": "LUzZ8pYBpHPS2GeIuikD",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "LkzZ8pYBpHPS2GeIuinR"
                ],
                "start": "2025-05-21T15:37:59Z",
                "end": "2025-05-21T15:38:02Z"
            }
        },
        {
            "OEza8pYBpHPS2GeILCnE": {
                "id": "OEza8pYBpHPS2GeILCnE",
                "name": "uc4_pythia_mode_detection_simple_test",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation_simple_1 {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput ->  END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation_simple_1 {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple_1 {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "OUza8pYBpHPS2GeIbymo": {
                "id": "OUza8pYBpHPS2GeIbymo",
                "name": "uc4_pythia_mode_detection_simple_test",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation_simple_1 {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput ->  END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation_simple_1 {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple_1 {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "IUzW8pYBpHPS2GeIOSnH": {
                "id": "IUzW8pYBpHPS2GeIOSnH",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "IkzW8pYBpHPS2GeIPCmO"
                ],
                "start": "2025-05-21T12:34:09Z",
                "end": "2025-05-21T12:34:44Z"
            }
        },
        {
            "LEzY8pYBpHPS2GeIJyng": {
                "id": "LEzY8pYBpHPS2GeIJyng",
                "name": "uc4_pythia_mode_detection_simple_test",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation_simple_1 {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput ->  END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation_simple_1 {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple_1 {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "IEzO8pYBpHPS2GeIRCmp": {
                "id": "IEzO8pYBpHPS2GeIRCmp",
                "name": "uc4_pythia_mode_detection_simple_test",
                "model": "// ************************************************************************************************\n// * PYTHIA MOBILITY SEGMENTATION PIPELINE\n// ************************************************************************************************\n\nworkflow pythia_mobility_segmentation_simple_1 {\n    // Task definitions with implementations first\n    task ValidateInput {\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n\n\n    // Task CONNECTIONS - Full Linear Pipeline (after task definitions)\n    START -> ValidateInput ->  END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS ***\n    define input data LocationDataSingleDayUser;\n    LocationDataSingleDayUser --> ValidateInput.LocationDataSingleDayUser;\n    configure data LocationDataSingleDayUser {\n        path \"UC4/RawTraces/Giannis/Giannis015.json\";\n    }\n\n\n// ************************************************************************************************\n// * WORKFLOW VARIANTS\n// ************************************************************************************************\nworkflow pythia_mobility_segmentation_simple from pythia_mobility_segmentation_simple_1 {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT\n// ************************************************************************************************\n\nexperiment PythiaSimple {\n    control {\n        START -> S1 -> END;\n    }\n    space S1 of pythia_mobility_segmentation_simple_1 {\n        strategy randomsearch;\n        runs = 1;\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Okzc8pYBpHPS2GeIXSmd": {
                "id": "Okzc8pYBpHPS2GeIXSmd",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "O0zc8pYBpHPS2GeIXili"
                ],
                "start": "2025-05-21T15:40:51Z",
                "end": "2025-05-21T15:41:16Z"
            }
        },
        {
            "j0wi7pYBpHPS2GeIUyfx": {
                "id": "j0wi7pYBpHPS2GeIUyfx",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        param n_estimators = range(5, 11, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,2);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "kEwi7pYBpHPS2GeIVSfw",
                    "l0wi7pYBpHPS2GeIVidK"
                ],
                "start": "2025-05-20T14:39:09Z",
                "end": "2025-05-20T14:42:16Z"
            }
        },
        {
            "aEz28pYBpHPS2GeI-ClL": {
                "id": "aEz28pYBpHPS2GeI-ClL",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "aUz38pYBpHPS2GeIDin5": {
                "id": "aUz38pYBpHPS2GeIDin5",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "akz58pYBpHPS2GeI3ym1": {
                "id": "akz58pYBpHPS2GeI3ym1",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VEzp8pYBpHPS2GeIaCmU": {
                "id": "VEzp8pYBpHPS2GeIaCmU",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VUzr8pYBpHPS2GeIByna": {
                "id": "VUzr8pYBpHPS2GeIByna",
                "name": "uc4_pythia_minimal_test",
                "model": "// ************************************************************************************************\n// * MINIMAL TEST WORKFLOW & EXPERIMENT\n// ************************************************************************************************\n\nworkflow minimal_pipeline {\n    // Task definitions with implementations\n    task ValidateInput {\n        // Path to the FOLDER containing ValidateInput's task.xxp and task.py\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task DummyProcessTask {\n        // Path to the FOLDER containing DummyProcessTask's task.xxp and task.py\n        implementation \"UC4/dummy_tasks/DummyProcessTask\";\n    }\n\n    // Task CONNECTIONS (Control Flow)\n    START -> ValidateInput -> DummyProcessTask -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS for the WORKFLOW ***\n    define input data RawLocationData;\n    RawLocationData --> ValidateInput.LocationDataSingleDayUser; // Connect workflow input to ValidateInput's input port\n    configure data RawLocationData {\n        // Path to your test JSON file\n        path \"UC4/RawTraces/minimal_test_input.json\";\n    }\n\n    define output data FinalMessageFromDummy;\n    DummyProcessTask.ProcessingMessage --> FinalMessageFromDummy; // Connect DummyProcessTask's output port to workflow output\n    configure data FinalMessageFromDummy {\n        // Where the final output of this workflow will be saved\n        path \"output/minimal_test_output/final_message_content.txt\";\n    }\n\n    // *** DATA CONNECTIONS BETWEEN TASKS ***\n    ValidateInput.UserId --> DummyProcessTask.InputUserID;\n    ValidateInput.ValidatedLocations --> DummyProcessTask.InputLocations;\n}\n\n// ************************************************************************************************\n// * MINIMAL EXPERIMENT\n// ************************************************************************************************\n\nexperiment MinimalTestExperiment {\n    control {\n        START -> S_Minimal -> END;\n    }\n\n    space S_Minimal of minimal_pipeline {\n        strategy gridsearch; // For a single run, this is fine.\n        // runs = 1; // Often implied if no parameters define a grid.\n\n        // --- Optional: Test parameter passing to ValidateInput ---\n        // 1. Uncomment these param definitions.\n        // 2. Ensure ValidateInput.xxp has these params defined.\n        // 3. Ensure ValidateInput/task.py uses these params.\n        /*\n        param min_records_vp = enum(2); // Test with a different value\n        param max_acc_threshold_vp = enum(120); // Test with a different value\n\n        task ValidateInput { // Specify which task in minimal_pipeline to parameterize\n            param min_records = min_records_vp;\n            param max_acc_threshold = max_acc_threshold_vp;\n        }\n        */\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Ukzn8pYBpHPS2GeIxCni": {
                "id": "Ukzn8pYBpHPS2GeIxCni",
                "name": "uc4_pythia_minimal_test",
                "model": "// ************************************************************************************************\n// * MINIMAL TEST WORKFLOW & EXPERIMENT\n// ************************************************************************************************\n\nworkflow minimal_pipeline {\n    // Task definitions with implementations\n    task ValidateInput {\n        // Path to the FOLDER containing ValidateInput's task.xxp and task.py\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task DummyProcessTask {\n        // Path to the FOLDER containing DummyProcessTask's task.xxp and task.py\n        implementation \"UC4/dummy_tasks/DummyProcessTask\";\n    }\n\n    // Task CONNECTIONS (Control Flow)\n    START -> ValidateInput -> DummyProcessTask -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS for the WORKFLOW ***\n    define input data RawLocationData;\n    RawLocationData --> ValidateInput.LocationDataSingleDayUser; // Connect workflow input to ValidateInput's input port\n    configure data RawLocationData {\n        // Path to your test JSON file\n        path \"UC4/RawTraces/minimal_test_input.json\";\n    }\n\n    define output data FinalMessageFromDummy;\n    DummyProcessTask.ProcessingMessage --> FinalMessageFromDummy; // Connect DummyProcessTask's output port to workflow output\n    configure data FinalMessageFromDummy {\n        // Where the final output of this workflow will be saved\n        path \"output/minimal_test_output/final_message_content.txt\";\n    }\n\n    // *** DATA CONNECTIONS BETWEEN TASKS ***\n    ValidateInput.UserId --> DummyProcessTask.InputUserID;\n    ValidateInput.ValidatedLocations --> DummyProcessTask.InputLocations;\n}\n\n// ************************************************************************************************\n// * MINIMAL EXPERIMENT\n// ************************************************************************************************\n\nexperiment MinimalTestExperiment {\n    control {\n        START -> S_Minimal -> END;\n    }\n\n    space S_Minimal of minimal_pipeline {\n        strategy gridsearch; // For a single run, this is fine.\n        // runs = 1; // Often implied if no parameters define a grid.\n\n        // --- Optional: Test parameter passing to ValidateInput ---\n        // 1. Uncomment these param definitions.\n        // 2. Ensure ValidateInput.xxp has these params defined.\n        // 3. Ensure ValidateInput/task.py uses these params.\n        /*\n        param min_records_vp = enum(2); // Test with a different value\n        param max_acc_threshold_vp = enum(120); // Test with a different value\n\n        task ValidateInput { // Specify which task in minimal_pipeline to parameterize\n            param min_records = min_records_vp;\n            param max_acc_threshold = max_acc_threshold_vp;\n        }\n        */\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "U0zo8pYBpHPS2GeI8ikm": {
                "id": "U0zo8pYBpHPS2GeI8ikm",
                "name": "uc4_pythia_minimal_test",
                "model": "// ************************************************************************************************\n// * MINIMAL TEST WORKFLOW & EXPERIMENT\n// ************************************************************************************************\n\nworkflow minimal_pipeline {\n    // Task definitions with implementations\n    task ValidateInput {\n        // Path to the FOLDER containing ValidateInput's task.xxp and task.py\n        implementation \"UC4/improved_mode_detection/ValidateInput\";\n    }\n\n    task DummyProcessTask {\n        // Path to the FOLDER containing DummyProcessTask's task.xxp and task.py\n        implementation \"UC4/dummy_tasks/DummyProcessTask\";\n    }\n\n    // Task CONNECTIONS (Control Flow)\n    START -> ValidateInput -> DummyProcessTask -> END;\n\n    // *** INPUT/OUTPUT DATA DEFINITIONS for the WORKFLOW ***\n    define input data RawLocationData;\n    RawLocationData --> ValidateInput.LocationDataSingleDayUser; // Connect workflow input to ValidateInput's input port\n    configure data RawLocationData {\n        // Path to your test JSON file\n        path \"UC4/RawTraces/minimal_test_input.json\";\n    }\n\n    define output data FinalMessageFromDummy;\n    DummyProcessTask.ProcessingMessage --> FinalMessageFromDummy; // Connect DummyProcessTask's output port to workflow output\n    configure data FinalMessageFromDummy {\n        // Where the final output of this workflow will be saved\n        path \"output/minimal_test_output/final_message_content.txt\";\n    }\n\n    // *** DATA CONNECTIONS BETWEEN TASKS ***\n    ValidateInput.UserId --> DummyProcessTask.InputUserID;\n    ValidateInput.ValidatedLocations --> DummyProcessTask.InputLocations;\n}\n\n// ************************************************************************************************\n// * MINIMAL EXPERIMENT\n// ************************************************************************************************\n\nexperiment MinimalTestExperiment {\n    control {\n        START -> S_Minimal -> END;\n    }\n\n    space S_Minimal of minimal_pipeline {\n        strategy gridsearch; // For a single run, this is fine.\n        // runs = 1; // Often implied if no parameters define a grid.\n\n        // --- Optional: Test parameter passing to ValidateInput ---\n        // 1. Uncomment these param definitions.\n        // 2. Ensure ValidateInput.xxp has these params defined.\n        // 3. Ensure ValidateInput/task.py uses these params.\n        /*\n        param min_records_vp = enum(2); // Test with a different value\n        param max_acc_threshold_vp = enum(120); // Test with a different value\n\n        task ValidateInput { // Specify which task in minimal_pipeline to parameterize\n            param min_records = min_records_vp;\n            param max_acc_threshold = max_acc_threshold_vp;\n        }\n        */\n    }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Wkzw8pYBpHPS2GeIvCmK": {
                "id": "Wkzw8pYBpHPS2GeIvCmK",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "W0zw8pYBpHPS2GeIvikK"
                ],
                "start": "2025-05-21T16:03:06Z",
                "end": "2025-05-21T16:03:47Z"
            }
        },
        {
            "ZUzx8pYBpHPS2GeIpikx": {
                "id": "ZUzx8pYBpHPS2GeIpikx",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Zkz18pYBpHPS2GeIxino": {
                "id": "Zkz18pYBpHPS2GeIxino",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WEzv8pYBpHPS2GeI5ik6": {
                "id": "WEzv8pYBpHPS2GeI5ik6",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.tasks.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.tasks.DummyProcess\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data RawUC4LocationData;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  RawUC4LocationData --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data RawUC4LocationData {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WUzv8pYBpHPS2GeI_yn9": {
                "id": "WUzv8pYBpHPS2GeI_yn9",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.tasks.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.tasks.DummyProcess\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data RawUC4LocationData;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  RawUC4LocationData --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data RawUC4LocationData {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Z0z28pYBpHPS2GeI3inx": {
                "id": "Z0z28pYBpHPS2GeI3inx",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "V0zv8pYBpHPS2GeIKilF": {
                "id": "V0zv8pYBpHPS2GeIKilF",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.tasks.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.tasks.DummyProcess\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data RawUC4LocationData;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  RawUC4LocationData --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data RawUC4LocationData {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/minimal_test_input.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"output/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Vkzs8pYBpHPS2GeILCkg": {
                "id": "Vkzs8pYBpHPS2GeILCkg",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Rkzj8pYBpHPS2GeI8yk9": {
                "id": "Rkzj8pYBpHPS2GeI8yk9",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "R0zj8pYBpHPS2GeI9Cnc"
                ],
                "start": "2025-05-21T15:49:09Z",
                "end": "2025-05-21T15:49:48Z"
            }
        },
        {
            "UUzn8pYBpHPS2GeINSkm": {
                "id": "UUzn8pYBpHPS2GeINSkm",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "eUwD85YBpHPS2GeIrikZ": {
                "id": "eUwD85YBpHPS2GeIrikZ",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ekwE85YBpHPS2GeIBSkZ": {
                "id": "ekwE85YBpHPS2GeIBSkZ",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "e0wG85YBpHPS2GeIASlt": {
                "id": "e0wG85YBpHPS2GeIASlt",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "a0z68pYBpHPS2GeIiCns": {
                "id": "a0z68pYBpHPS2GeIiCns",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bEz78pYBpHPS2GeIIilQ": {
                "id": "bEz78pYBpHPS2GeIIilQ",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "d0wC85YBpHPS2GeI3Clv": {
                "id": "d0wC85YBpHPS2GeI3Clv",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "eEwD85YBpHPS2GeIiimD": {
                "id": "eEwD85YBpHPS2GeIiimD",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bUz78pYBpHPS2GeIJynB": {
                "id": "bUz78pYBpHPS2GeIJynB",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bkz88pYBpHPS2GeIiynS": {
                "id": "bkz88pYBpHPS2GeIiynS",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "cUz88pYBpHPS2GeI4ykL": {
                "id": "cUz88pYBpHPS2GeI4ykL",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ckz88pYBpHPS2GeI5Ckx"
                ],
                "start": "2025-05-21T13:16:21Z",
                "end": "2025-05-21T13:16:31Z"
            }
        },
        {
            "dEz98pYBpHPS2GeIRSnp": {
                "id": "dEz98pYBpHPS2GeIRSnp",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "dUwA85YBpHPS2GeICSm_": {
                "id": "dUwA85YBpHPS2GeICSm_",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "dkwB85YBpHPS2GeI9SkU": {
                "id": "dkwB85YBpHPS2GeI9SkU",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "b0z88pYBpHPS2GeIkClv": {
                "id": "b0z88pYBpHPS2GeIkClv",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "cEz88pYBpHPS2GeI0SlT": {
                "id": "cEz88pYBpHPS2GeI0SlT",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4.improved_mode_detection.ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4.dummy_tasks.DummyProcessTask\";\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "fEwU85YBpHPS2GeIVSlt": {
                "id": "fEwU85YBpHPS2GeIVSlt",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "fUwU85YBpHPS2GeIuinE": {
                "id": "fUwU85YBpHPS2GeIuinE",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "gUwc85YBpHPS2GeI0inD": {
                "id": "gUwc85YBpHPS2GeI0inD",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 16, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,1);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "gkwc85YBpHPS2GeI1Cmd",
                    "iUwc85YBpHPS2GeI1SkB",
                    "kEwc85YBpHPS2GeI1SlZ",
                    "l0wc85YBpHPS2GeI1Smt",
                    "nkwc85YBpHPS2GeI1ikP",
                    "pUwc85YBpHPS2GeI1ilz",
                    "rEwc85YBpHPS2GeI1ina",
                    "s0wc85YBpHPS2GeI1ylP",
                    "ukwc85YBpHPS2GeI1ynD",
                    "wUwc85YBpHPS2GeI2Ckc",
                    "yEwc85YBpHPS2GeI2Clr",
                    "z0wc85YBpHPS2GeI2Cnf"
                ],
                "start": "2025-05-21T13:51:15Z",
                "end": "2025-05-21T14:01:21Z"
            }
        },
        {
            "2Uyv95YBpHPS2GeIkClS": {
                "id": "2Uyv95YBpHPS2GeIkClS",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "fkwa85YBpHPS2GeIhCkL": {
                "id": "fkwa85YBpHPS2GeIhCkL",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "f0wb85YBpHPS2GeI9ymO": {
                "id": "f0wb85YBpHPS2GeI9ymO",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "1kxV85YBpHPS2GeIfynM": {
                "id": "1kxV85YBpHPS2GeIfynM",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "10zU9pYBpHPS2GeIbCkU": {
                "id": "10zU9pYBpHPS2GeIbCkU",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "2Ezh9pYBpHPS2GeIIinw": {
                "id": "2Ezh9pYBpHPS2GeIIinw",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "gEwc85YBpHPS2GeItym4": {
                "id": "gEwc85YBpHPS2GeItym4",
                "name": "UC1_surrogate_model",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> TrainModelAPI -> ReadMetrics -> END;\n\n    task TrainModelAPI;\n\n    task ReadMetrics {\n        implementation \"UC1.surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"UC1/**\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModelAPI.FileToRead;\n    TrainModelAPI.jobID --> ReadMetrics.jobID;\n    TrainModelAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"UC1.surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Ikxf-JYBpHPS2GeI0ip4": {
                "id": "Ikxf-JYBpHPS2GeI0ip4",
                "name": "main_BestClassifier",
                "model": "\n// ************************************************************************************************\n// * MAIN BINARY\n// ************************************************************************************************\n\nworkflow main_binary {\n\n  // Task CONNECTIONS\n  START -> ReadDataBinary -> PrepareDataBinary -> TrainModelBinary -> EvaluateModelBinary -> END;\n\n  task TrainModelBinary;\n\n  task ReadDataBinary {\n    implementation \"binary.ReadData\";\n  }\n\n  task EvaluateModelBinary {\n    implementation \"binary.EvaluateModel\";\n  }\n\n  task PrepareDataBinary {\n    subworkflow \"binary_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFileBinary;\n\n  // DATA CONNECTIONS\n  ExternalDataFileBinary --> ReadDataBinary.FileToRead;\n\n  configure data ExternalDataFileBinary {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolderBinary;\n\n  TrainModelBinary.OutputFolder --> TrainedModelFolderBinary;\n\n  configure data TrainedModelFolderBinary {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadDataBinary.X41 --> PrepareDataBinary.X2;\n    ReadDataBinary.Y --> PrepareDataBinary.Y;\n    ReadDataBinary.IndicatorList --> PrepareDataBinary.IndicatorList;\n\n    PrepareDataBinary.Timestamps42 --> TrainModelBinary.Timestamps;\n    PrepareDataBinary.Features --> TrainModelBinary.Features;\n    PrepareDataBinary.XTrain --> TrainModelBinary.XTrain;\n    PrepareDataBinary.XTest --> TrainModelBinary.XTest;\n    PrepareDataBinary.YTrain --> TrainModelBinary.YTrain;\n    PrepareDataBinary.YTest --> TrainModelBinary.YTest;\n    PrepareDataBinary.XPad --> TrainModelBinary.XPad;\n    PrepareDataBinary.YPad --> TrainModelBinary.YPad;\n\n    TrainModelBinary.OutputFolder --> EvaluateModelBinary.OutputFolder;\n    TrainModelBinary.XTest --> EvaluateModelBinary.XTest;\n    TrainModelBinary.YTest --> EvaluateModelBinary.YTest;\n    TrainModelBinary.XPad --> EvaluateModelBinary.XPad;\n    TrainModelBinary.YPad --> EvaluateModelBinary.YPad;\n    TrainModelBinary.Timestamps --> EvaluateModelBinary.Timestamps2;\n    TrainModelBinary.Features --> EvaluateModelBinary.Features;\n    TrainModelBinary.model_path --> EvaluateModelBinary.model_path;\n\n}\n// ************************************************************************************************\n// * MAIN MUTICLASS\n// ************************************************************************************************\n\nworkflow main_multiclass {\n\n  // Task CONNECTIONS\n  START -> ReadData -> PrepareData -> TrainModel -> EvaluateModel -> END;\n\n  task TrainModel;\n\n  task ReadData {\n    implementation \"multiclass.ReadData\";\n  }\n\n  task EvaluateModel {\n    implementation \"multiclass.EvaluateModel\";\n  }\n\n  task PrepareData {\n    subworkflow \"multiclass_DataPreprocessing\";\n  }\n\n  // DATA\n  define input data ExternalDataFile;\n\n  // DATA CONNECTIONS\n  ExternalDataFile --> ReadData.FileToRead;\n\n  configure data ExternalDataFile {\n    path \"training/**\";\n  }\n\n  define output data TrainedModelFolder;\n\n  TrainModel.OutputFolder --> TrainedModelFolder;\n\n  configure data TrainedModelFolder {\n    path \"output/trained_model/**\";\n    type \"generated-ML-model\";\n  }\n\n    ReadData.X41 --> PrepareData.X2;\n    ReadData.Y --> PrepareData.Y;\n    ReadData.IndicatorList --> PrepareData.IndicatorList;\n\n    PrepareData.Timestamps42 --> TrainModel.Timestamps;\n    PrepareData.Features --> TrainModel.Features;\n    PrepareData.n_classes --> TrainModel.n_classes;\n    PrepareData.XTrain --> TrainModel.XTrain;\n    PrepareData.XTest --> TrainModel.XTest;\n    PrepareData.YTrain --> TrainModel.YTrain;\n    PrepareData.YTest --> TrainModel.YTest;\n    PrepareData.XPad --> TrainModel.XPad;\n    PrepareData.YPad --> TrainModel.YPad;\n\n    TrainModel.OutputFolder --> EvaluateModel.OutputFolder;\n    TrainModel.XTest --> EvaluateModel.XTest;\n    TrainModel.YTest --> EvaluateModel.YTest;\n    TrainModel.XPad --> EvaluateModel.XPad;\n    TrainModel.YPad --> EvaluateModel.YPad;\n    TrainModel.Timestamps --> EvaluateModel.Timestamps2;\n    TrainModel.Features --> EvaluateModel.Features;\n    TrainModel.n_classes --> EvaluateModel.n_classes;\n\n}\n\nworkflow machine_files_load {\n\n  // Task CONNECTIONS\n  START -> MachineData -> END;\n\n  task MachineData;\n\n  // DATA VALIDATION\n  define input data MachineDataFiles;\n\n  configure data MachineDataFiles {\n    path \"machine_files/**\";\n  }\n\n  MachineDataFiles --> MachineData.FileToValidate;\n}\n\n\n// ************************************************************************************************\n// * TRAINMODELS BINARY\n// ************************************************************************************************\n\nworkflow TrainModelNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_B from main_binary {\n  task TrainModelBinary {\n    implementation \"binary.TrainModelCNN\";\n  }\n}\n\nworkflow TrainModelNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelNN\";\n  }\n}\n\nworkflow TrainModelRNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelRNN\";\n  }\n}\n\nworkflow TrainModelCNN_M from main_multiclass {\n  task TrainModel {\n    implementation \"multiclass.TrainModelCNN\";\n  }\n}\n\nworkflow MachineDataLoad from machine_files_load {\n  task MachineData {\n    implementation \"V1_MachineDataLoad\";\n  }\n}\n\n\n// ************************************************************************************************\n// * EXPERIMENTO\n// ************************************************************************************************\n\nexperiment EXP {\n  intent FindBestClassifier;\n  control {\n\n\n\n    // Run all the MULTICLASS spaces, get the best model and save it\n    // START -> S1M -> S2M -> S3M -> T1;\n    START -> S2M -> T1;\n\n    // The model is good, we keep the multiclass\n    T1 ?-> V1 { condition \"check_results_more_than 0.4\"};\n\n    // If the condition meet, go to the BINARY ONE\n    T1 ?-> S1B { condition \"check_results_less_or_equal_than 0.4\"};\n\n    // Get the best model for the BINARY\n    // S1B -> S2B -> S3B -> V1;\n    S1B -> V1;\n\n    // We have the best model, pass the machiles and files and run the user interaction task\n    //V1 -> T2 -> I1 -> END;\n    V1 -> T2 -> END;\n\n    // T1 es best_model_evaluator\n    // V1 es MachineDataLoad\n    // T2 es Coger el mejor y PassMachineFilesThroughTheBestModel\n    // I2 es user interaction\n\n  }\n\n  // ************************************************************************************************\n  // * SPACES BINARY\n  // ************************************************************************************************\n\n  space S1B of TrainModelNN_B {\n    strategy gridsearch;\n    param epochs_vp = range(2, 4);\n    //param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(100);\n    param units_2_vp = enum(100);\n    param units_3_vp = enum(100);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2B of TrainModelCNN_B {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(64);\n    param filters_2_vp = enum(64);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3B of TrainModelRNN_B {\n    strategy gridsearch;\n    //param epochs_vp = enum(2);\n    param epochs_vp = range(2, 4);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModelBinary {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n\n\n  // ************************************************************************************************\n  // * SPACES MULTICLASS\n  // ************************************************************************************************\n\n  space S1M of TrainModelNN_M {\n    strategy gridsearch;\n    // param epochs_vp = range(2, 4);\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(64);\n    param units_1_vp = enum(10);\n    param units_2_vp = enum(10);\n    param units_3_vp = enum(10);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param units_1 = units_1_vp;\n      param units_2 = units_2_vp;\n      param units_3 = units_3_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S2M of TrainModelCNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param filters_1_vp = enum(16);\n    param filters_2_vp = enum(8);\n    param kernel_size_vp = enum(3);\n    param pool_size_vp = enum(2);\n    param activation_function_vp = enum(\"relu\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param filters_1 = filters_1_vp;\n      param filters_2 = filters_2_vp;\n      param kernel_size = kernel_size_vp;\n      param pool_size = pool_size_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space S3M of TrainModelRNN_M {\n    strategy gridsearch;\n    param epochs_vp = enum(2);\n    param batch_size_vp = enum(32);\n    param hidden_units_1_vp = enum(64);\n    param hidden_units_2_vp = enum(64);\n    param activation_function_vp = enum(\"tanh\");\n    task TrainModel {\n      param epochs = epochs_vp;\n      param batch_size = batch_size_vp;\n      param hidden_units_1 = hidden_units_1_vp;\n      param hidden_units_2 = hidden_units_2_vp;\n      param activation_function = activation_function_vp;\n    }\n  }\n\n  space V1 of MachineDataLoad {\n    strategy gridsearch;\n    param random = enum(1);\n    task MachineData {\n      param random = random;\n    }\n\n  }\n\n  task T1 {\n    implementation \"T1_BestModelEvaluator\";\n  }\n\n  task T2 {\n    implementation \"T2_SelectBestModelAndPassMachineFiles\";\n  }\n\n  task I1 {\n    implementation \"I1_UserInteraction\";\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "I0xf-JYBpHPS2GeI1Cqa",
                    "J0xg-JYBpHPS2GeIOipB"
                ],
                "start": "2025-05-22T17:22:32Z",
                "end": "2025-05-22T17:23:04Z"
            }
        },
        {
            "2ky095YBpHPS2GeIzSky": {
                "id": "2ky095YBpHPS2GeIzSky",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "20y095YBpHPS2GeIzilD",
                    "4ky495YBpHPS2GeIWim7",
                    "6Uy495YBpHPS2GeIWynV",
                    "8Ey495YBpHPS2GeIXCnA",
                    "90y495YBpHPS2GeIXSmt"
                ],
                "start": "2025-05-22T14:15:45Z",
                "end": "2025-05-22T14:34:09Z"
            }
        },
        {
            "KEwt-ZYBpHPS2GeIgCpj": {
                "id": "KEwt-ZYBpHPS2GeIgCpj",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "KUwt-ZYBpHPS2GeIgyp7"
                ],
                "start": "2025-05-22T20:07:12Z"
            }
        },
        {
            "K0ww-ZYBpHPS2GeIHyoA": {
                "id": "K0ww-ZYBpHPS2GeIHyoA",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "_kxX-JYBpHPS2GeIIymz": {
                "id": "_kxX-JYBpHPS2GeIIymz",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_0xX-JYBpHPS2GeIJSka",
                    "Bkxa-JYBpHPS2GeIkCrn",
                    "DUxa-JYBpHPS2GeIkSrW",
                    "FExa-JYBpHPS2GeIkiq-",
                    "G0xa-JYBpHPS2GeIkyqa"
                ],
                "start": "2025-05-22T17:13:03Z"
            }
        },
        {
            "NkxD-ZYBpHPS2GeIniq9": {
                "id": "NkxD-ZYBpHPS2GeIniq9",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "N0xD-ZYBpHPS2GeIoCrD"
                ],
                "start": "2025-05-22T20:31:21Z",
                "end": "2025-05-22T20:32:53Z"
            }
        },
        {
            "MExB-ZYBpHPS2GeIaiqR": {
                "id": "MExB-ZYBpHPS2GeIaiqR",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MUxB-ZYBpHPS2GeIbSo7"
                ],
                "start": "2025-05-22T20:28:57Z"
            }
        },
        {
            "LEww-ZYBpHPS2GeIbipS": {
                "id": "LEww-ZYBpHPS2GeIbipS",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "LUww-ZYBpHPS2GeIcSrN"
                ],
                "start": "2025-05-22T20:10:24Z",
                "end": "2025-05-22T20:10:49Z"
            }
        },
        {
            "Lkw--ZYBpHPS2GeIcCoR": {
                "id": "Lkw--ZYBpHPS2GeIcCoR",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "L0w--ZYBpHPS2GeIcyp2"
                ],
                "start": "2025-05-22T20:25:42Z",
                "end": "2025-05-22T20:26:18Z"
            }
        },
        {
            "M0xC-ZYBpHPS2GeIiCoU": {
                "id": "M0xC-ZYBpHPS2GeIiCoU",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "NExC-ZYBpHPS2GeIiioR"
                ],
                "start": "2025-05-22T20:30:10Z",
                "end": "2025-05-22T20:31:02Z"
            }
        },
        {
            "UEyp-ZYBpHPS2GeIxCrA": {
                "id": "UEyp-ZYBpHPS2GeIxCrA",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SkxQ-ZYBpHPS2GeIDSr9": {
                "id": "SkxQ-ZYBpHPS2GeIDSr9",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "S0xQ-ZYBpHPS2GeIECpQ"
                ],
                "start": "2025-05-22T20:44:56Z",
                "end": "2025-05-22T20:45:27Z"
            }
        },
        {
            "TExQ-ZYBpHPS2GeIuSoO": {
                "id": "TExQ-ZYBpHPS2GeIuSoO",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "TUxQ-ZYBpHPS2GeIuypk"
                ],
                "start": "2025-05-22T20:45:40Z",
                "end": "2025-05-22T20:46:03Z"
            }
        },
        {
            "P0xL-ZYBpHPS2GeIFSr_": {
                "id": "P0xL-ZYBpHPS2GeIFSr_",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "QExL-ZYBpHPS2GeIFyr6"
                ],
                "start": "2025-05-22T20:39:31Z"
            }
        },
        {
            "QkxL-ZYBpHPS2GeItyoH": {
                "id": "QkxL-ZYBpHPS2GeItyoH",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Q0xL-ZYBpHPS2GeIuSoI"
                ],
                "start": "2025-05-22T20:40:12Z"
            }
        },
        {
            "RUxM-ZYBpHPS2GeIiip-": {
                "id": "RUxM-ZYBpHPS2GeIiip-",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "RkxM-ZYBpHPS2GeIjCqA"
                ],
                "start": "2025-05-22T20:41:06Z"
            }
        },
        {
            "SExO-ZYBpHPS2GeIJypD": {
                "id": "SExO-ZYBpHPS2GeIJypD",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "SUxO-ZYBpHPS2GeIKSrn"
                ],
                "start": "2025-05-22T20:42:52Z",
                "end": "2025-05-22T20:43:16Z"
            }
        },
        {
            "O0xI-ZYBpHPS2GeI4iqt": {
                "id": "O0xI-ZYBpHPS2GeI4iqt",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "PExI-ZYBpHPS2GeI5SoO"
                ],
                "start": "2025-05-22T20:37:06Z",
                "end": "2025-05-22T20:37:30Z"
            }
        },
        {
            "TkxR-ZYBpHPS2GeITyrd": {
                "id": "TkxR-ZYBpHPS2GeITyrd",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "T0xR-ZYBpHPS2GeIUio2"
                ],
                "start": "2025-05-22T20:46:19Z",
                "end": "2025-05-22T20:46:49Z"
            }
        },
        {
            "PUxK-ZYBpHPS2GeIIiq0": {
                "id": "PUxK-ZYBpHPS2GeIIiq0",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "PkxK-ZYBpHPS2GeIJSoH"
                ],
                "start": "2025-05-22T20:38:28Z",
                "end": "2025-05-22T20:38:52Z"
            }
        },
        {
            "OUxF-ZYBpHPS2GeIgyr8": {
                "id": "OUxF-ZYBpHPS2GeIgyr8",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "OkxF-ZYBpHPS2GeIhirL"
                ],
                "start": "2025-05-22T20:33:26Z",
                "end": "2025-05-22T20:33:49Z"
            }
        },
        {
            "uEwu_JYBpHPS2GeI9SrU": {
                "id": "uEwu_JYBpHPS2GeI9SrU",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "uUwu_JYBpHPS2GeI-So4",
                    "ukwv_JYBpHPS2GeIXyrr"
                ],
                "start": "2025-05-23T10:07:40Z",
                "end": "2025-05-23T10:08:30Z"
            }
        },
        {
            "U0wi_JYBpHPS2GeIBiq1": {
                "id": "U0wi_JYBpHPS2GeIBiq1",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 9, 3);\n        param n_estimators = range(5, 16, 5);\n        param min_child_weight = range(1,3,2);\n        param gamma = range(1,3,1);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "VEwi_JYBpHPS2GeICCqX",
                    "W0wi_JYBpHPS2GeICCr0",
                    "Ykwi_JYBpHPS2GeICSpZ",
                    "aUwi_JYBpHPS2GeICSqp",
                    "cEwi_JYBpHPS2GeICSr2",
                    "d0wi_JYBpHPS2GeICipM",
                    "fkwi_JYBpHPS2GeICiqn",
                    "hUwi_JYBpHPS2GeICirz",
                    "jEwi_JYBpHPS2GeICypF",
                    "k0wi_JYBpHPS2GeICyqk",
                    "mkwi_JYBpHPS2GeICyr2",
                    "oUwi_JYBpHPS2GeIDCpL"
                ],
                "start": "2025-05-23T07:53:30Z",
                "end": "2025-05-23T08:05:21Z"
            }
        },
        {
            "skws_JYBpHPS2GeIvCpr": {
                "id": "skws_JYBpHPS2GeIvCpr",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "s0ws_JYBpHPS2GeIvyra",
                    "tEwt_JYBpHPS2GeIKSr6"
                ],
                "start": "2025-05-23T10:05:14Z",
                "end": "2025-05-23T10:06:04Z"
            }
        },
        {
            "qkwl_JYBpHPS2GeIhSqG": {
                "id": "qkwl_JYBpHPS2GeIhSqG",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "q0wl_JYBpHPS2GeIiip9"
                ],
                "start": "2025-05-23T09:57:21Z",
                "end": "2025-05-23T09:58:06Z"
            }
        },
        {
            "qEwi_JYBpHPS2GeIdCro": {
                "id": "qEwi_JYBpHPS2GeIdCro",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "qUwi_JYBpHPS2GeIeCrO"
                ],
                "start": "2025-05-23T09:54:00Z",
                "end": "2025-05-23T09:54:28Z"
            }
        },
        {
            "rEwm_JYBpHPS2GeIjSqj": {
                "id": "rEwm_JYBpHPS2GeIjSqj",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "rUwm_JYBpHPS2GeIkSrq",
                    "rkwn_JYBpHPS2GeICCpu"
                ],
                "start": "2025-05-23T09:58:29Z",
                "end": "2025-05-23T09:59:24Z"
            }
        },
        {
            "r0wo_JYBpHPS2GeIqyqo": {
                "id": "r0wo_JYBpHPS2GeIqyqo",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "sEwo_JYBpHPS2GeIsCoU",
                    "sUwp_JYBpHPS2GeIHyqQ"
                ],
                "start": "2025-05-23T10:00:48Z",
                "end": "2025-05-23T10:01:40Z"
            }
        },
        {
            "tUwu_JYBpHPS2GeIEiqb": {
                "id": "tUwu_JYBpHPS2GeIEiqb",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "tkwu_JYBpHPS2GeIFiqb",
                    "t0wu_JYBpHPS2GeIfCrn"
                ],
                "start": "2025-05-23T10:06:42Z",
                "end": "2025-05-23T10:07:31Z"
            }
        },
        {
            "UUwh_JYBpHPS2GeIgCqE": {
                "id": "UUwh_JYBpHPS2GeIgCqE",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        // START -> S1 -> I1 -> END;\n        START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Ukwh_JYBpHPS2GeIhSrl"
                ],
                "start": "2025-05-23T09:52:58Z",
                "end": "2025-05-23T09:53:52Z"
            }
        },
        {
            "yUwz_JYBpHPS2GeIIyqy": {
                "id": "yUwz_JYBpHPS2GeIIyqy",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ykwz_JYBpHPS2GeIKiom",
                    "y0wz_JYBpHPS2GeI5CqN"
                ],
                "start": "2025-05-23T10:12:14Z",
                "end": "2025-05-23T10:14:39Z"
            }
        },
        {
            "wUwy_JYBpHPS2GeIvyrG": {
                "id": "wUwy_JYBpHPS2GeIvyrG",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "wkwy_JYBpHPS2GeIwSqI"
                ],
                "start": "2025-05-23T08:11:46Z",
                "end": "2025-05-23T08:14:53Z"
            }
        },
        {
            "zkw2_JYBpHPS2GeItSqJ": {
                "id": "zkw2_JYBpHPS2GeItSqJ",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "z0w2_JYBpHPS2GeItyp2"
                ],
                "start": "2025-05-23T08:16:06Z",
                "end": "2025-05-23T08:19:09Z"
            }
        },
        {
            "10w5_JYBpHPS2GeIbyp5": {
                "id": "10w5_JYBpHPS2GeIbyp5",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "2Ew5_JYBpHPS2GeIcirn",
                    "2Uw5_JYBpHPS2GeI3yr9"
                ],
                "start": "2025-05-23T10:19:06Z",
                "end": "2025-05-23T10:20:18Z"
            }
        },
        {
            "3Uw__JYBpHPS2GeIBiqQ": {
                "id": "3Uw__JYBpHPS2GeIBiqQ",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "3kw__JYBpHPS2GeICirT"
                ],
                "start": "2025-05-23T10:25:13Z",
                "end": "2025-05-23T10:26:25Z"
            }
        },
        {
            "2kw7_JYBpHPS2GeIxipP": {
                "id": "2kw7_JYBpHPS2GeIxipP",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "20w7_JYBpHPS2GeIySrh",
                    "3Ew8_JYBpHPS2GeINyrc"
                ],
                "start": "2025-05-23T10:21:39Z",
                "end": "2025-05-23T10:23:30Z"
            }
        },
        {
            "u0ww_JYBpHPS2GeIYSq6": {
                "id": "u0ww_JYBpHPS2GeIYSq6",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "vEww_JYBpHPS2GeIZSo_",
                    "vUww_JYBpHPS2GeIzCr9"
                ],
                "start": "2025-05-23T10:09:13Z",
                "end": "2025-05-23T10:10:28Z"
            }
        },
        {
            "vkwx_JYBpHPS2GeIqCo9": {
                "id": "vkwx_JYBpHPS2GeIqCo9",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "v0wx_JYBpHPS2GeIqyqY",
                    "wEwy_JYBpHPS2GeIEyq7"
                ],
                "start": "2025-05-23T10:10:36Z",
                "end": "2025-05-23T10:12:00Z"
            }
        },
        {
            "zEw2_JYBpHPS2GeIXypk": {
                "id": "zEw2_JYBpHPS2GeIXypk",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "zUw2_JYBpHPS2GeIYyqX",
                    "1kw2_JYBpHPS2GeI1yqL"
                ],
                "start": "2025-05-23T10:15:46Z",
                "end": "2025-05-23T10:17:37Z"
            }
        },
        {
            "8UxH_JYBpHPS2GeIqSpi": {
                "id": "8UxH_JYBpHPS2GeIqSpi",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "8kxH_JYBpHPS2GeIrSq8",
                    "80xI_JYBpHPS2GeIHyrJ"
                ],
                "start": "2025-05-23T10:34:39Z",
                "end": "2025-05-23T10:35:51Z"
            }
        },
        {
            "6UxG_JYBpHPS2GeIbSp9": {
                "id": "6UxG_JYBpHPS2GeIbSp9",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 6, 3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6kxG_JYBpHPS2GeIbypS"
                ],
                "start": "2025-05-23T08:33:16Z",
                "end": "2025-05-23T08:36:23Z"
            }
        },
        {
            "BkxQ_JYBpHPS2GeI-iu7": {
                "id": "BkxQ_JYBpHPS2GeI-iu7",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "B0xQ_JYBpHPS2GeI_ivf"
                ],
                "start": "2025-05-23T10:44:49Z"
            }
        },
        {
            "90xO_JYBpHPS2GeIxyrW": {
                "id": "90xO_JYBpHPS2GeIxyrW",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-ExO_JYBpHPS2GeIySqd",
                    "_0xO_JYBpHPS2GeIySr-"
                ],
                "start": "2025-05-23T08:42:23Z",
                "end": "2025-05-23T08:45:26Z"
            }
        },
        {
            "9ExN_JYBpHPS2GeIfypn": {
                "id": "9ExN_JYBpHPS2GeIfypn",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9UxN_JYBpHPS2GeIgiqn"
                ],
                "start": "2025-05-23T10:41:01Z"
            }
        },
        {
            "40xE_JYBpHPS2GeIEyo4": {
                "id": "40xE_JYBpHPS2GeIEyo4",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5ExE_JYBpHPS2GeIFioZ"
                ],
                "start": "2025-05-23T10:30:43Z"
            }
        },
        {
            "CUxT_JYBpHPS2GeIuCtu": {
                "id": "CUxT_JYBpHPS2GeIuCtu",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CkxT_JYBpHPS2GeIuyt8"
                ],
                "start": "2025-05-23T10:47:48Z",
                "end": "2025-05-23T10:49:05Z"
            }
        },
        {
            "4ExB_JYBpHPS2GeIxSpP": {
                "id": "4ExB_JYBpHPS2GeIxSpP",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "4UxB_JYBpHPS2GeIyCqo"
                ],
                "start": "2025-05-23T10:28:12Z",
                "end": "2025-05-23T10:29:11Z"
            }
        },
        {
            "5kxF_JYBpHPS2GeIviqL": {
                "id": "5kxF_JYBpHPS2GeIviqL",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "50xF_JYBpHPS2GeIwiqW",
                    "6ExG_JYBpHPS2GeIRiqc"
                ],
                "start": "2025-05-23T10:32:33Z",
                "end": "2025-05-23T10:33:53Z"
            }
        },
        {
            "IUyE_JYBpHPS2GeIHSsc": {
                "id": "IUyE_JYBpHPS2GeIHSsc",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "JUya_JYBpHPS2GeIeSto": {
                "id": "JUya_JYBpHPS2GeIeSto",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Jkya_JYBpHPS2GeIeytG",
                    "LUye_JYBpHPS2GeIZyuO",
                    "NEye_JYBpHPS2GeIaSvR",
                    "O0ye_JYBpHPS2GeIaivx",
                    "Qkye_JYBpHPS2GeIayvs"
                ],
                "start": "2025-05-23T13:05:05Z",
                "end": "2025-05-23T13:24:24Z"
            }
        },
        {
            "DExd_JYBpHPS2GeImSsY": {
                "id": "DExd_JYBpHPS2GeImSsY",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DUxd_JYBpHPS2GeImysO",
                    "FExd_JYBpHPS2GeImyuK"
                ],
                "start": "2025-05-23T08:58:35Z",
                "end": "2025-05-23T09:01:34Z"
            }
        },
        {
            "WEyo_JYBpHPS2GeI6is9": {
                "id": "WEyo_JYBpHPS2GeI6is9",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "WUyo_JYBpHPS2GeI7Csk",
                    "YEyo_JYBpHPS2GeI7CuT"
                ],
                "start": "2025-05-23T10:20:51Z",
                "end": "2025-05-23T10:23:48Z"
            }
        },
        {
            "IkyU_JYBpHPS2GeI0CvN": {
                "id": "IkyU_JYBpHPS2GeI0CvN",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "I0yU_JYBpHPS2GeI1StL"
                ],
                "start": "2025-05-23T11:58:55Z",
                "end": "2025-05-23T12:00:40Z"
            }
        },
        {
            "G0xx_JYBpHPS2GeIfCtr": {
                "id": "G0xx_JYBpHPS2GeIfCtr",
                "name": "user_interaction_in_workflow",
                "model": "workflow UserInteraction {\n\n    START -> Task1 -> Task2 -> Task3 -> END;\n\n    task Task1 {\n        implementation \"Task1\";\n    }\n\n    task Task2 {\n        implementation \"Task2\";\n    }\n\n    task Task3 {\n        implementation \"Task3\";\n    }\n\n}\n\nworkflow AssembledUserInteraction from UserInteraction {\n\n}\n\nexperiment UserInteractionExp {\n\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of AssembledUserInteraction {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "HExx_JYBpHPS2GeIfysY"
                ],
                "start": "2025-05-23T11:20:19Z",
                "end": "2025-05-23T11:22:09Z"
            }
        },
        {
            "HkyB_JYBpHPS2GeIhSse": {
                "id": "HkyB_JYBpHPS2GeIhSse",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n        // START -> S1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "H0yB_JYBpHPS2GeIiysu",
                    "IEyC_JYBpHPS2GeIQSvz"
                ],
                "start": "2025-05-23T11:37:51Z",
                "end": "2025-05-23T11:40:25Z"
            }
        },
        {
            "SUyf_JYBpHPS2GeIISsD": {
                "id": "SUyf_JYBpHPS2GeIISsD",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Skyf_JYBpHPS2GeIIivC",
                    "UUyf_JYBpHPS2GeIIysY"
                ],
                "start": "2025-05-23T10:10:09Z",
                "end": "2025-05-23T10:13:25Z"
            }
        },
        {
            "aUyw_JYBpHPS2GeIOSuS": {
                "id": "aUyw_JYBpHPS2GeIOSuS",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        param min_child_weight = range(1,3,2);\n        // param min_child_weight = enum(3);\n        param gamma = range(1,3,1);\n        // param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "akyw_JYBpHPS2GeIOytn",
                    "cUyw_JYBpHPS2GeIOyvL",
                    "eEyw_JYBpHPS2GeIPCs1",
                    "f0yw_JYBpHPS2GeIPCuj",
                    "hkyw_JYBpHPS2GeIPCv_",
                    "jUyw_JYBpHPS2GeIPStz",
                    "lEyw_JYBpHPS2GeIPSvY",
                    "m0yw_JYBpHPS2GeIPis4",
                    "okyw_JYBpHPS2GeIPiuT",
                    "qUyw_JYBpHPS2GeIPysE",
                    "sEyw_JYBpHPS2GeIPytl",
                    "t0yw_JYBpHPS2GeIPyvG"
                ],
                "start": "2025-05-23T10:28:50Z",
                "end": "2025-05-23T10:35:02Z"
            }
        },
        {
            "MUzW_JYBpHPS2GeIvywQ": {
                "id": "MUzW_JYBpHPS2GeIvywQ",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MkzW_JYBpHPS2GeIxCwY",
                    "OUza_JYBpHPS2GeIJCzQ",
                    "QEza_JYBpHPS2GeIJSzR",
                    "R0za_JYBpHPS2GeIJiy9",
                    "Tkza_JYBpHPS2GeIJyyi"
                ],
                "start": "2025-05-23T14:10:55Z"
            }
        },
        {
            "VUzb_JYBpHPS2GeIbCwW": {
                "id": "VUzb_JYBpHPS2GeIbCwW",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "6UzK_JYBpHPS2GeIeCsW": {
                "id": "6UzK_JYBpHPS2GeIeCsW",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6kzK_JYBpHPS2GeIeivB",
                    "8UzN_JYBpHPS2GeI-yuS",
                    "-EzN_JYBpHPS2GeI_Ct8",
                    "_0zN_JYBpHPS2GeI_StZ",
                    "BkzN_JYBpHPS2GeI_ixG"
                ],
                "start": "2025-05-23T13:57:31Z"
            }
        },
        {
            "DUzR_JYBpHPS2GeI2Cx8": {
                "id": "DUzR_JYBpHPS2GeI2Cx8",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DkzR_JYBpHPS2GeI2Sxp",
                    "FUzV_JYBpHPS2GeICSzv",
                    "HEzV_JYBpHPS2GeICizY",
                    "I0zV_JYBpHPS2GeICyyt",
                    "KkzV_JYBpHPS2GeIDCx_"
                ],
                "start": "2025-05-23T14:05:34Z"
            }
        },
        {
            "vky__JYBpHPS2GeIiSvv": {
                "id": "vky__JYBpHPS2GeIiSvv",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "v0y__JYBpHPS2GeIiyvg",
                    "xky__JYBpHPS2GeIjCtX",
                    "zUy__JYBpHPS2GeIjCvF",
                    "1Ey__JYBpHPS2GeIjSso",
                    "20y__JYBpHPS2GeIjSt-",
                    "4ky__JYBpHPS2GeIjSvJ"
                ],
                "start": "2025-05-23T10:45:33Z",
                "end": "2025-05-23T10:45:52Z"
            }
        },
        {
            "q0zk_JYBpHPS2GeIFCzn": {
                "id": "q0zk_JYBpHPS2GeIFCzn",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        param min_child_weight = range(1,3,2);\n        // param min_child_weight = enum(3);\n        param gamma = range(1,3,1);\n        // param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "rEzk_JYBpHPS2GeIFizM",
                    "s0zk_JYBpHPS2GeIFywR",
                    "ukzk_JYBpHPS2GeIFyxb",
                    "wUzk_JYBpHPS2GeIFyys",
                    "yEzk_JYBpHPS2GeIFyz6",
                    "z0zk_JYBpHPS2GeIGCxF",
                    "1kzk_JYBpHPS2GeIGCyN",
                    "3Uzk_JYBpHPS2GeIGCzR",
                    "5Ezk_JYBpHPS2GeIGSwP",
                    "60zk_JYBpHPS2GeIGSxV",
                    "8kzk_JYBpHPS2GeIGSyX",
                    "-Uzk_JYBpHPS2GeIGSzZ"
                ],
                "start": "2025-05-23T11:25:28Z",
                "end": "2025-05-23T11:26:05Z"
            }
        },
        {
            "Vkzh_JYBpHPS2GeIOSwg": {
                "id": "Vkzh_JYBpHPS2GeIOSwg",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        param min_child_weight = range(1,3,2);\n        // param min_child_weight = enum(3);\n        param gamma = range(1,3,1);\n        // param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "V0zh_JYBpHPS2GeIOiz8",
                    "Xkzh_JYBpHPS2GeIOyxZ",
                    "ZUzh_JYBpHPS2GeIOyy8",
                    "bEzh_JYBpHPS2GeIPCwX",
                    "c0zh_JYBpHPS2GeIPCyI",
                    "ekzh_JYBpHPS2GeIPCzs",
                    "gUzh_JYBpHPS2GeIPSxA",
                    "iEzh_JYBpHPS2GeIPSyY",
                    "j0zh_JYBpHPS2GeIPiwE",
                    "lkzh_JYBpHPS2GeIPixr",
                    "nUzh_JYBpHPS2GeIPizC",
                    "pEzh_JYBpHPS2GeIPywP"
                ],
                "start": "2025-05-23T11:22:21Z",
                "end": "2025-05-23T11:22:59Z"
            }
        },
        {
            "YUzk_pYBpHPS2GeIfy3s": {
                "id": "YUzk_pYBpHPS2GeIfy3s",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    /*\n    param min_records_vp = enum(2);\n    param max_acc_thresh_vp = enum(120);\n\n    task UC4ValidateInput { // Target the task within the 'uc4_minimal_pipeline'\n      param min_records = min_records_vp;\n      param max_acc_threshold = max_acc_thresh_vp;\n    }\n    */\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Ykzm_pYBpHPS2GeIBS3z": {
                "id": "Ykzm_pYBpHPS2GeIBS3z",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bEz3_pYBpHPS2GeIxi17": {
                "id": "bEz3_pYBpHPS2GeIxi17",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bUz3_pYBpHPS2GeIyS1g"
                ],
                "start": "2025-05-23T23:06:14Z",
                "end": "2025-05-23T23:06:48Z"
            }
        },
        {
            "ZEzp_pYBpHPS2GeIMi1y": {
                "id": "ZEzp_pYBpHPS2GeIMi1y",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ZUzp_pYBpHPS2GeINC12"
                ],
                "start": "2025-05-23T22:50:19Z",
                "end": "2025-05-23T22:50:54Z"
            }
        },
        {
            "Y0zn_pYBpHPS2GeIQC0Q": {
                "id": "Y0zn_pYBpHPS2GeIQC0Q",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-23T22:48:12Z",
                "end": "2025-05-23T22:48:13Z"
            }
        },
        {
            "Zkzx_pYBpHPS2GeIvy1I": {
                "id": "Zkzx_pYBpHPS2GeIvy1I",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Z0zx_pYBpHPS2GeIwi1p"
                ],
                "start": "2025-05-23T22:59:40Z",
                "end": "2025-05-23T23:00:17Z"
            }
        },
        {
            "aEzz_pYBpHPS2GeIcy0E": {
                "id": "aEzz_pYBpHPS2GeIcy0E",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "aUzz_pYBpHPS2GeIdS0s"
                ],
                "start": "2025-05-23T23:01:31Z",
                "end": "2025-05-23T23:01:51Z"
            }
        },
        {
            "akz2_pYBpHPS2GeIJy0R": {
                "id": "akz2_pYBpHPS2GeIJy0R",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "a0z2_pYBpHPS2GeIKS0x"
                ],
                "start": "2025-05-23T23:04:28Z",
                "end": "2025-05-23T23:04:47Z"
            }
        },
        {
            "DEz5_JYBpHPS2GeIdy2e": {
                "id": "DEz5_JYBpHPS2GeIdy2e",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        param min_child_weight = range(1,3,2);\n        // param min_child_weight = enum(3);\n        param gamma = range(1,3,1);\n        // param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DUz5_JYBpHPS2GeIeS1v",
                    "FEz5_JYBpHPS2GeIeS3T",
                    "G0z5_JYBpHPS2GeIei03",
                    "Ikz5_JYBpHPS2GeIei2d",
                    "KUz5_JYBpHPS2GeIey0K",
                    "MEz5_JYBpHPS2GeIey14",
                    "N0z5_JYBpHPS2GeIey3i",
                    "Pkz5_JYBpHPS2GeIfC1C",
                    "RUz5_JYBpHPS2GeIfC2q",
                    "TEz5_JYBpHPS2GeIfS0P",
                    "U0z5_JYBpHPS2GeIfS1r",
                    "Wkz5_JYBpHPS2GeIfS3G"
                ],
                "start": "2025-05-23T11:48:50Z",
                "end": "2025-05-23T11:49:28Z"
            }
        },
        {
            "dkwG_5YBpHPS2GeIoi2T": {
                "id": "dkwG_5YBpHPS2GeIoi2T",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "d0wG_5YBpHPS2GeIpS2i"
                ],
                "start": "2025-05-23T23:22:28Z",
                "end": "2025-05-23T23:23:05Z"
            }
        },
        {
            "eEwH_5YBpHPS2GeIUi0A": {
                "id": "eEwH_5YBpHPS2GeIUi0A",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "eUwH_5YBpHPS2GeIVC0g"
                ],
                "start": "2025-05-23T23:23:13Z",
                "end": "2025-05-23T23:23:33Z"
            }
        },
        {
            "cEz7_pYBpHPS2GeIYS1r": {
                "id": "cEz7_pYBpHPS2GeIYS1r",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cUz7_pYBpHPS2GeIYy2n"
                ],
                "start": "2025-05-23T23:10:11Z",
                "end": "2025-05-23T23:10:30Z"
            }
        },
        {
            "fEwT_5YBpHPS2GeIEC27": {
                "id": "fEwT_5YBpHPS2GeIEC27",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fUwT_5YBpHPS2GeIEy3K"
                ],
                "start": "2025-05-23T23:36:03Z",
                "end": "2025-05-23T23:36:42Z"
            }
        },
        {
            "bkz5_pYBpHPS2GeIii2W": {
                "id": "bkz5_pYBpHPS2GeIii2W",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "b0z5_pYBpHPS2GeIjC3H"
                ],
                "start": "2025-05-23T23:08:10Z",
                "end": "2025-05-23T23:08:31Z"
            }
        },
        {
            "ekwQ_5YBpHPS2GeIDy3-": {
                "id": "ekwQ_5YBpHPS2GeIDy3-",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "e0wQ_5YBpHPS2GeIEi1L"
                ],
                "start": "2025-05-23T23:32:46Z",
                "end": "2025-05-23T23:33:10Z"
            }
        },
        {
            "ckwD_5YBpHPS2GeIQy2u": {
                "id": "ckwD_5YBpHPS2GeIQy2u",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "c0wD_5YBpHPS2GeIRi0i"
                ],
                "start": "2025-05-23T23:18:47Z",
                "end": "2025-05-23T23:19:07Z"
            }
        },
        {
            "dEwE_5YBpHPS2GeIYi07": {
                "id": "dEwE_5YBpHPS2GeIYi07",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "dUwE_5YBpHPS2GeIZC2L"
                ],
                "start": "2025-05-23T23:20:01Z",
                "end": "2025-05-23T23:20:21Z"
            }
        },
        {
            "1UxIAZcBpHPS2GeILC2S": {
                "id": "1UxIAZcBpHPS2GeILC2S",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "1kxIAZcBpHPS2GeILi1N"
                ],
                "start": "2025-05-24T09:53:17Z",
                "end": "2025-05-24T09:53:37Z"
            }
        },
        {
            "y0wxAZcBpHPS2GeIUS3J": {
                "id": "y0wxAZcBpHPS2GeIUS3J",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-24T09:28:20Z",
                "end": "2025-05-24T09:28:21Z"
            }
        },
        {
            "zEwxAZcBpHPS2GeIjS1Z": {
                "id": "zEwxAZcBpHPS2GeIjS1Z",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-24T09:28:35Z",
                "end": "2025-05-24T09:28:36Z"
            }
        },
        {
            "zUwyAZcBpHPS2GeILi2y": {
                "id": "zUwyAZcBpHPS2GeILi2y",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "zkwyAZcBpHPS2GeIMC2q"
                ],
                "start": "2025-05-24T09:29:16Z"
            }
        },
        {
            "x0wwAZcBpHPS2GeIES0X": {
                "id": "x0wwAZcBpHPS2GeIES0X",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-24T09:26:58Z",
                "end": "2025-05-24T09:26:59Z"
            }
        },
        {
            "yEwwAZcBpHPS2GeIbi1y": {
                "id": "yEwwAZcBpHPS2GeIbi1y",
                "name": "user_interaction_in_experiment",
                "model": "workflow ComplexControlW {\n\n    task Task1 {\n        implementation \"Testcase_task1\";\n    }\n\n    task Task2;\n\n     START -> Task1 -> Task2 -> END;\n\n}\n\nworkflow ComplexControlAW1 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v1\";\n    }\n}\n\nworkflow ComplexControlAW2 from ComplexControlW {\n    task Task2 {\n        implementation \"Testcase_task2_v2\";\n    }\n}\n\nexperiment ComplexControlExp1 {\n    intent testComplexControl;\n\n    control {\n        START -> S1 -> I1 -> END;\n    }\n\n    space S1 of ComplexControlAW1 {\n        strategy randomsearch;\n        runs = 1;\n        param param1_vp = enum(2, 5, 7);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S2 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(3, 4);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    space S3 of ComplexControlAW2 {\n        strategy gridsearch;\n        param param1_vp = range(4, 5);\n        task Task1 {\n            param param1 = param1_vp;\n        }\n    }\n\n    task T1 {\n        implementation \"ExpTask1\";\n    }\n\n    task T2 {\n        implementation \"ExpTask1\";\n    }\n\n    interaction I1 {\n        implementation \"ExpInteraction1\";\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "yUwwAZcBpHPS2GeIcS2T",
                    "ykwwAZcBpHPS2GeI8i0w"
                ],
                "start": "2025-05-24T09:27:22Z"
            }
        },
        {
            "fkwt_5YBpHPS2GeI-S1y": {
                "id": "fkwt_5YBpHPS2GeI-S1y",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "f0wt_5YBpHPS2GeI-i35",
                    "hkwx_5YBpHPS2GeIoC2t",
                    "jUwx_5YBpHPS2GeIoS3r",
                    "lEwx_5YBpHPS2GeIoi39",
                    "m0wx_5YBpHPS2GeIpC1K"
                ],
                "start": "2025-05-24T01:05:26Z"
            }
        },
        {
            "okw0_5YBpHPS2GeIsC22": {
                "id": "okw0_5YBpHPS2GeIsC22",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"CS_surrogate_model/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "o0w0_5YBpHPS2GeIsi1J",
                    "qkw6_5YBpHPS2GeIOy05",
                    "sUw6_5YBpHPS2GeIPC23",
                    "uEw6_5YBpHPS2GeIPS33",
                    "v0w6_5YBpHPS2GeIPy06"
                ],
                "start": "2025-05-24T01:12:46Z"
            }
        },
        {
            "z0wzAZcBpHPS2GeIBi1M": {
                "id": "z0wzAZcBpHPS2GeIBi1M",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "0EwzAZcBpHPS2GeICC0Q"
                ],
                "start": "2025-05-24T09:30:11Z",
                "end": "2025-05-24T09:30:30Z"
            }
        },
        {
            "xkwuAZcBpHPS2GeIiS0Y": {
                "id": "xkwuAZcBpHPS2GeIiS0Y",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy randomsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-24T09:25:17Z",
                "end": "2025-05-24T09:25:18Z"
            }
        },
        {
            "0UxEAZcBpHPS2GeIFC0O": {
                "id": "0UxEAZcBpHPS2GeIFC0O",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "0kxEAZcBpHPS2GeIFi1B"
                ],
                "start": "2025-05-24T09:48:49Z",
                "end": "2025-05-24T09:49:22Z"
            }
        },
        {
            "00xFAZcBpHPS2GeIpi37": {
                "id": "00xFAZcBpHPS2GeIpi37",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "1ExFAZcBpHPS2GeIqS0h"
                ],
                "start": "2025-05-24T09:50:32Z",
                "end": "2025-05-24T09:50:52Z"
            }
        },
        {
            "2UxUAZcBpHPS2GeIaC0c": {
                "id": "2UxUAZcBpHPS2GeIaC0c",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "2kxUAZcBpHPS2GeIai2K"
                ],
                "start": "2025-05-24T10:06:39Z",
                "end": "2025-05-24T10:07:28Z"
            }
        },
        {
            "20xcAZcBpHPS2GeI3C0D": {
                "id": "20xcAZcBpHPS2GeI3C0D",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "3ExcAZcBpHPS2GeI3i0c"
                ],
                "start": "2025-05-24T10:15:53Z",
                "end": "2025-05-24T10:16:29Z"
            }
        },
        {
            "50xtAZcBpHPS2GeIci1z": {
                "id": "50xtAZcBpHPS2GeIci1z",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6ExtAZcBpHPS2GeIdC1J"
                ],
                "start": "2025-05-24T10:34:00Z",
                "end": "2025-05-24T10:34:36Z"
            }
        },
        {
            "10xKAZcBpHPS2GeIRC2I": {
                "id": "10xKAZcBpHPS2GeIRC2I",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"TestZenoh\";\n    }\n\n    START -> Task1 -> END;\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "2ExKAZcBpHPS2GeIRi1L"
                ],
                "start": "2025-05-24T09:55:35Z",
                "end": "2025-05-24T09:55:58Z"
            }
        },
        {
            "3UxmAZcBpHPS2GeI-i0B": {
                "id": "3UxmAZcBpHPS2GeI-i0B",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "3kxmAZcBpHPS2GeI_C3d"
                ],
                "start": "2025-05-24T10:26:56Z",
                "end": "2025-05-24T10:27:00Z"
            }
        },
        {
            "30xnAZcBpHPS2GeIqS2V": {
                "id": "30xnAZcBpHPS2GeIqS2V",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "4ExnAZcBpHPS2GeIqy2A"
                ],
                "start": "2025-05-24T10:27:41Z",
                "end": "2025-05-24T10:27:45Z"
            }
        },
        {
            "4UxoAZcBpHPS2GeIdC2V": {
                "id": "4UxoAZcBpHPS2GeIdC2V",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "4kxoAZcBpHPS2GeIdi2E"
                ],
                "start": "2025-05-24T10:28:33Z",
                "end": "2025-05-24T10:29:25Z"
            }
        },
        {
            "40xqAZcBpHPS2GeIQS2f": {
                "id": "40xqAZcBpHPS2GeIQS2f",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5ExqAZcBpHPS2GeIQy22"
                ],
                "start": "2025-05-24T10:30:31Z",
                "end": "2025-05-24T10:31:06Z"
            }
        },
        {
            "5UxrAZcBpHPS2GeI3C29": {
                "id": "5UxrAZcBpHPS2GeI3C29",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5kxrAZcBpHPS2GeI3i3N"
                ],
                "start": "2025-05-24T10:32:17Z",
                "end": "2025-05-24T10:32:52Z"
            }
        },
        {
            "90y1AZcBpHPS2GeIuC1c": {
                "id": "90y1AZcBpHPS2GeIuC1c",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-Ey1AZcBpHPS2GeIuy0B"
                ],
                "start": "2025-05-24T11:52:57Z",
                "end": "2025-05-24T11:53:46Z"
            }
        },
        {
            "8UykAZcBpHPS2GeIYC2Q": {
                "id": "8UykAZcBpHPS2GeIYC2Q",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "8kykAZcBpHPS2GeIYy0o"
                ],
                "start": "2025-05-24T11:34:00Z",
                "end": "2025-05-24T11:34:50Z"
            }
        },
        {
            "70x-AZcBpHPS2GeICy2k": {
                "id": "70x-AZcBpHPS2GeICy2k",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "8Ex-AZcBpHPS2GeIDS1_"
                ],
                "start": "2025-05-24T10:52:08Z",
                "end": "2025-05-24T10:52:44Z"
            }
        },
        {
            "6Ux5AZcBpHPS2GeIYy1v": {
                "id": "6Ux5AZcBpHPS2GeIYy1v",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6kx5AZcBpHPS2GeIZS3k"
                ],
                "start": "2025-05-24T10:47:03Z",
                "end": "2025-05-24T10:47:51Z"
            }
        },
        {
            "60x6AZcBpHPS2GeIWS0h": {
                "id": "60x6AZcBpHPS2GeIWS0h",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "7Ex6AZcBpHPS2GeIWy0D"
                ],
                "start": "2025-05-24T10:48:06Z",
                "end": "2025-05-24T10:48:41Z"
            }
        },
        {
            "7Ux8AZcBpHPS2GeIOS0T": {
                "id": "7Ux8AZcBpHPS2GeIOS0T",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "7kx8AZcBpHPS2GeIOy2u"
                ],
                "start": "2025-05-24T10:50:09Z",
                "end": "2025-05-24T10:50:57Z"
            }
        },
        {
            "80yxAZcBpHPS2GeIJC0I": {
                "id": "80yxAZcBpHPS2GeIJC0I",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9EyxAZcBpHPS2GeIJi2e"
                ],
                "start": "2025-05-24T11:47:57Z",
                "end": "2025-05-24T11:48:45Z"
            }
        },
        {
            "9UyyAZcBpHPS2GeIWC1g": {
                "id": "9UyyAZcBpHPS2GeIWC1g",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9kyyAZcBpHPS2GeIWi1f"
                ],
                "start": "2025-05-24T11:49:16Z",
                "end": "2025-05-24T11:49:51Z"
            }
        },
        {
            "-Uy4AZcBpHPS2GeIIC3n": {
                "id": "-Uy4AZcBpHPS2GeIIC3n",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n//    define input data InputFile;\n//    InputFile --> Task1.InputFile;\n//    configure data InputFile {\n//        path \"demo_datasets/demo_data.txt\";\n//    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-ky4AZcBpHPS2GeIIi3b"
                ],
                "start": "2025-05-24T11:55:35Z",
                "end": "2025-05-24T11:56:10Z"
            }
        },
        {
            "-0y6AZcBpHPS2GeIei0z": {
                "id": "-0y6AZcBpHPS2GeIei0z",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        path \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "CEzuAZcBpHPS2GeIqC4s": {
                "id": "CEzuAZcBpHPS2GeIqC4s",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CUzuAZcBpHPS2GeIqi4v"
                ],
                "start": "2025-05-24T12:55:08Z",
                "end": "2025-05-24T12:55:48Z"
            }
        },
        {
            "AEzOAZcBpHPS2GeI2S6j": {
                "id": "AEzOAZcBpHPS2GeI2S6j",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "AUzOAZcBpHPS2GeI3C4H"
                ],
                "start": "2025-05-24T12:20:24Z",
                "end": "2025-05-24T12:21:11Z"
            }
        },
        {
            "AkznAZcBpHPS2GeIIC7h": {
                "id": "AkznAZcBpHPS2GeIIC7h",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "A0znAZcBpHPS2GeIJC42"
                ],
                "start": "2025-05-24T12:46:55Z",
                "end": "2025-05-24T12:47:58Z"
            }
        },
        {
            "_Ey8AZcBpHPS2GeIUS0C": {
                "id": "_Ey8AZcBpHPS2GeIUS0C",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        path \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_Uy8AZcBpHPS2GeIUi3i"
                ],
                "start": "2025-05-24T12:00:09Z",
                "end": "2025-05-24T12:00:44Z"
            }
        },
        {
            "_kzJAZcBpHPS2GeIxC27": {
                "id": "_kzJAZcBpHPS2GeIxC27",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_0zJAZcBpHPS2GeIxy02"
                ],
                "start": "2025-05-24T12:14:51Z",
                "end": "2025-05-24T12:15:39Z"
            }
        },
        {
            "BEzrAZcBpHPS2GeIay4e": {
                "id": "BEzrAZcBpHPS2GeIay4e",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "BUzrAZcBpHPS2GeIbS6_"
                ],
                "start": "2025-05-24T12:51:36Z",
                "end": "2025-05-24T12:52:35Z"
            }
        },
        {
            "BkzsAZcBpHPS2GeI-C6i": {
                "id": "BkzsAZcBpHPS2GeI-C6i",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "B0zsAZcBpHPS2GeI-i6Y"
                ],
                "start": "2025-05-24T12:53:18Z",
                "end": "2025-05-24T12:53:57Z"
            }
        },
        {
            "GEwIApcBpHPS2GeIxC5C": {
                "id": "GEwIApcBpHPS2GeIxC5C",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GUwIApcBpHPS2GeIxy6h"
                ],
                "start": "2025-05-24T13:23:40Z",
                "end": "2025-05-24T13:24:20Z"
            }
        },
        {
            "FEwAApcBpHPS2GeIqi45": {
                "id": "FEwAApcBpHPS2GeIqi45",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "FUwAApcBpHPS2GeIrC6G"
                ],
                "start": "2025-05-24T13:14:49Z",
                "end": "2025-05-24T13:15:28Z"
            }
        },
        {
            "Ekz8AZcBpHPS2GeINy58": {
                "id": "Ekz8AZcBpHPS2GeINy58",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "E0z8AZcBpHPS2GeIOi4C"
                ],
                "start": "2025-05-24T13:09:57Z",
                "end": "2025-05-24T13:10:37Z"
            }
        },
        {
            "CkzyAZcBpHPS2GeIDi7n": {
                "id": "CkzyAZcBpHPS2GeIDi7n",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "C0zyAZcBpHPS2GeIEC73"
                ],
                "start": "2025-05-24T12:58:51Z",
                "end": "2025-05-24T12:59:30Z"
            }
        },
        {
            "DEz1AZcBpHPS2GeIWy5d": {
                "id": "DEz1AZcBpHPS2GeIWy5d",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DUz1AZcBpHPS2GeIXS6G"
                ],
                "start": "2025-05-24T13:02:27Z",
                "end": "2025-05-24T13:03:08Z"
            }
        },
        {
            "Dkz4AZcBpHPS2GeI7C7G": {
                "id": "Dkz4AZcBpHPS2GeI7C7G",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "D0z4AZcBpHPS2GeI7y72"
                ],
                "start": "2025-05-24T13:06:22Z",
                "end": "2025-05-24T13:07:19Z"
            }
        },
        {
            "EEz6AZcBpHPS2GeISi5M": {
                "id": "EEz6AZcBpHPS2GeISi5M",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "EUz6AZcBpHPS2GeITC5Y"
                ],
                "start": "2025-05-24T13:07:51Z",
                "end": "2025-05-24T13:08:30Z"
            }
        },
        {
            "FkwHApcBpHPS2GeINS4o": {
                "id": "FkwHApcBpHPS2GeINS4o",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "F0wHApcBpHPS2GeIOC4i"
                ],
                "start": "2025-05-24T13:21:58Z",
                "end": "2025-05-24T13:22:56Z"
            }
        },
        {
            "JEw1ApcBpHPS2GeI9C6p": {
                "id": "JEw1ApcBpHPS2GeI9C6p",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "JUw1ApcBpHPS2GeI-S4p"
                ],
                "start": "2025-05-24T14:13:02Z",
                "end": "2025-05-24T14:14:03Z"
            }
        },
        {
            "IEwgApcBpHPS2GeIYS5m": {
                "id": "IEwgApcBpHPS2GeIYS5m",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "IUwgApcBpHPS2GeIZS7N"
                ],
                "start": "2025-05-24T13:49:28Z",
                "end": "2025-05-24T13:50:31Z"
            }
        },
        {
            "HkwPApcBpHPS2GeIvy6E": {
                "id": "HkwPApcBpHPS2GeIvy6E",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "H0wPApcBpHPS2GeIwS6B"
                ],
                "start": "2025-05-24T13:31:17Z",
                "end": "2025-05-24T13:31:56Z"
            }
        },
        {
            "IkwiApcBpHPS2GeIzi4o": {
                "id": "IkwiApcBpHPS2GeIzi4o",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "I0wiApcBpHPS2GeI0S7w"
                ],
                "start": "2025-05-24T13:52:06Z",
                "end": "2025-05-24T13:53:12Z"
            }
        },
        {
            "Jkw3ApcBpHPS2GeIsC7e": {
                "id": "Jkw3ApcBpHPS2GeIsC7e",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "J0w3ApcBpHPS2GeIsy5d"
                ],
                "start": "2025-05-24T14:14:55Z",
                "end": "2025-05-24T14:15:32Z"
            }
        },
        {
            "GkwMApcBpHPS2GeIlC75": {
                "id": "GkwMApcBpHPS2GeIlC75",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "G0wMApcBpHPS2GeIly6s"
                ],
                "start": "2025-05-24T13:27:50Z",
                "end": "2025-05-24T13:28:30Z"
            }
        },
        {
            "HEwOApcBpHPS2GeIHC4R": {
                "id": "HEwOApcBpHPS2GeIHC4R",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "HUwOApcBpHPS2GeIHi6t"
                ],
                "start": "2025-05-24T13:29:30Z",
                "end": "2025-05-24T13:30:11Z"
            }
        },
        {
            "NkyGApcBpHPS2GeIcC5R": {
                "id": "NkyGApcBpHPS2GeIcC5R",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "N0yGApcBpHPS2GeIci63"
                ],
                "start": "2025-05-24T15:40:56Z",
                "end": "2025-05-24T15:41:44Z"
            }
        },
        {
            "OkyjApcBpHPS2GeIGi5p": {
                "id": "OkyjApcBpHPS2GeIGi5p",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "O0yjApcBpHPS2GeIHC7d"
                ],
                "start": "2025-05-24T16:12:14Z",
                "end": "2025-05-24T16:13:02Z"
            }
        },
        {
            "NEx_ApcBpHPS2GeIfy7G": {
                "id": "NEx_ApcBpHPS2GeIfy7G",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "NUx_ApcBpHPS2GeIgi5X"
                ],
                "start": "2025-05-24T15:33:21Z",
                "end": "2025-05-24T15:34:10Z"
            }
        },
        {
            "KEw5ApcBpHPS2GeIWy4f": {
                "id": "KEw5ApcBpHPS2GeIWy4f",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "KUw5ApcBpHPS2GeIXS7P"
                ],
                "start": "2025-05-24T14:16:44Z",
                "end": "2025-05-24T14:17:23Z"
            }
        },
        {
            "Kkw6ApcBpHPS2GeINS66": {
                "id": "Kkw6ApcBpHPS2GeINS66",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "K0w6ApcBpHPS2GeIOC7N"
                ],
                "start": "2025-05-24T14:17:40Z",
                "end": "2025-05-24T14:18:20Z"
            }
        },
        {
            "MExAApcBpHPS2GeIEi4z": {
                "id": "MExAApcBpHPS2GeIEi4z",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "MUxAApcBpHPS2GeIFS49"
                ],
                "start": "2025-05-24T14:24:04Z",
                "end": "2025-05-24T14:24:43Z"
            }
        },
        {
            "MkxAApcBpHPS2GeIyS7v": {
                "id": "MkxAApcBpHPS2GeIyS7v",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "M0xAApcBpHPS2GeIzC7Y"
                ],
                "start": "2025-05-24T14:24:51Z",
                "end": "2025-05-24T14:25:28Z"
            }
        },
        {
            "OEygApcBpHPS2GeIWy6P": {
                "id": "OEygApcBpHPS2GeIWy6P",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "OUygApcBpHPS2GeIXS56"
                ],
                "start": "2025-05-24T16:09:14Z",
                "end": "2025-05-24T16:09:49Z"
            }
        },
        {
            "LEw9ApcBpHPS2GeIfS4D": {
                "id": "LEw9ApcBpHPS2GeIfS4D",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "LUw9ApcBpHPS2GeIgC6G"
                ],
                "start": "2025-05-24T14:21:15Z",
                "end": "2025-05-24T14:21:54Z"
            }
        },
        {
            "Lkw-ApcBpHPS2GeIrC4S": {
                "id": "Lkw-ApcBpHPS2GeIrC4S",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        // zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n    configure data OutputFile {\n        path \"output/trained_model1/**\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "L0w-ApcBpHPS2GeIri5_"
                ],
                "start": "2025-05-24T14:22:32Z",
                "end": "2025-05-24T14:23:12Z"
            }
        },
        {
            "P0yrApcBpHPS2GeIoy4H": {
                "id": "P0yrApcBpHPS2GeIoy4H",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "QEzFApcBpHPS2GeIPy7v": {
                "id": "QEzFApcBpHPS2GeIPy7v",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "PEyoApcBpHPS2GeIly4Z": {
                "id": "PEyoApcBpHPS2GeIly4Z",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "PUypApcBpHPS2GeIBy4c": {
                "id": "PUypApcBpHPS2GeIBy4c",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "PkypApcBpHPS2GeICC79"
                ],
                "start": "2025-05-24T16:18:42Z",
                "end": "2025-05-24T16:19:17Z"
            }
        },
        {
            "RUzSApcBpHPS2GeIWy62": {
                "id": "RUzSApcBpHPS2GeIWy62",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "RkzSApcBpHPS2GeIXS75"
                ],
                "start": "2025-05-24T17:03:51Z"
            }
        },
        {
            "R0zTApcBpHPS2GeIFC7b": {
                "id": "R0zTApcBpHPS2GeIFC7b",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "SEzTApcBpHPS2GeIFi66"
                ],
                "start": "2025-05-24T17:04:38Z",
                "end": "2025-05-24T17:05:11Z"
            }
        },
        {
            "Q0zIApcBpHPS2GeIei5k": {
                "id": "Q0zIApcBpHPS2GeIei5k",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "REzIApcBpHPS2GeIfC5a"
                ],
                "start": "2025-05-24T16:53:04Z",
                "end": "2025-05-24T16:53:38Z"
            }
        },
        {
            "QUzFApcBpHPS2GeIrC5A": {
                "id": "QUzFApcBpHPS2GeIrC5A",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias13.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "QkzFApcBpHPS2GeIri4-"
                ],
                "start": "2025-05-24T16:50:00Z",
                "end": "2025-05-24T16:50:47Z"
            }
        },
        {
            "SUzYApcBpHPS2GeIfS4i": {
                "id": "SUzYApcBpHPS2GeIfS4i",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias14.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "SkzYApcBpHPS2GeIfy61"
                ],
                "start": "2025-05-24T17:10:33Z",
                "end": "2025-05-24T17:11:21Z"
            }
        },
        {
            "TUzpApcBpHPS2GeI-y6V": {
                "id": "TUzpApcBpHPS2GeI-y6V",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias15.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile -> Task2.TestZenohTask2InputFile\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n   // define input data InputFile2;\n   // InputFile2 --> Task2.TestZenohTask2InputFile;\n   // configure data InputFile2 {\n   //     zenoh_name \"titanic.json\";\n   //     zenoh_project \"demo_project_001\";\n   // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "UkzyApcBpHPS2GeIGS6N": {
                "id": "UkzyApcBpHPS2GeIGS6N",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias15.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n   // define input data InputFile2;\n   // InputFile2 --> Task2.TestZenohTask2InputFile;\n   // configure data InputFile2 {\n   //     zenoh_name \"titanic.json\";\n   //     zenoh_project \"demo_project_001\";\n   // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "UEzsApcBpHPS2GeIGi5h": {
                "id": "UEzsApcBpHPS2GeIGi5h",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias15.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n   // define input data InputFile2;\n   // InputFile2 --> Task2.TestZenohTask2InputFile;\n   // configure data InputFile2 {\n   //     zenoh_name \"titanic.json\";\n   //     zenoh_project \"demo_project_001\";\n   // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "UUzsApcBpHPS2GeIHC6D"
                ],
                "start": "2025-05-24T17:31:58Z",
                "end": "2025-05-24T17:32:32Z"
            }
        },
        {
            "U0z7ApcBpHPS2GeINS65": {
                "id": "U0z7ApcBpHPS2GeINS65",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias15.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n   // define input data InputFile2;\n   // InputFile2 --> Task2.TestZenohTask2InputFile;\n   // configure data InputFile2 {\n   //     zenoh_name \"titanic.json\";\n   //     zenoh_project \"demo_project_001\";\n   // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "VEz7ApcBpHPS2GeINy6f"
                ],
                "start": "2025-05-24T17:48:28Z",
                "end": "2025-05-24T17:49:02Z"
            }
        },
        {
            "TkzqApcBpHPS2GeIXy7f": {
                "id": "TkzqApcBpHPS2GeIXy7f",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias15.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n   // define input data InputFile2;\n   // InputFile2 --> Task2.TestZenohTask2InputFile;\n   // configure data InputFile2 {\n   //     zenoh_name \"titanic.json\";\n   //     zenoh_project \"demo_project_001\";\n   // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "T0zqApcBpHPS2GeIYi4W"
                ],
                "start": "2025-05-24T17:30:05Z",
                "end": "2025-05-24T17:30:50Z"
            }
        },
        {
            "S0zhApcBpHPS2GeITS4_": {
                "id": "S0zhApcBpHPS2GeITS4_",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    InputFile --> Task1.TestZenohTask1InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define input data InputFile2;\n    InputFile2 --> Task2.TestZenohTask2InputFile;\n    configure data InputFile2 {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias15.json\";\n    }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "TEzhApcBpHPS2GeITy4q"
                ],
                "start": "2025-05-24T17:20:10Z",
                "end": "2025-05-24T17:20:45Z"
            }
        },
        {
            "V0wmA5cBpHPS2GeI5S7a": {
                "id": "V0wmA5cBpHPS2GeI5S7a",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias16.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "WEwmA5cBpHPS2GeI5y7S"
                ],
                "start": "2025-05-24T18:36:11Z",
                "end": "2025-05-24T18:36:47Z"
            }
        },
        {
            "VUwjA5cBpHPS2GeIpC64": {
                "id": "VUwjA5cBpHPS2GeIpC64",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias15.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n   // define input data InputFile2;\n   // InputFile2 --> Task2.TestZenohTask2InputFile;\n   // configure data InputFile2 {\n   //     zenoh_name \"titanic.json\";\n   //     zenoh_project \"demo_project_001\";\n   // }\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "VkwjA5cBpHPS2GeIpy5B"
                ],
                "start": "2025-05-24T18:32:38Z",
                "end": "2025-05-24T18:33:26Z"
            }
        },
        {
            "YUxsA5cBpHPS2GeIHi77": {
                "id": "YUxsA5cBpHPS2GeIHi77",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "WUw4A5cBpHPS2GeILi5E": {
                "id": "WUw4A5cBpHPS2GeILi5E",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Wkw4A5cBpHPS2GeIMC7a"
                ],
                "start": "2025-05-24T18:55:04Z",
                "end": "2025-05-24T18:55:39Z"
            }
        },
        {
            "YkxsA5cBpHPS2GeI9i79": {
                "id": "YkxsA5cBpHPS2GeI9i79",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Y0xsA5cBpHPS2GeI-S5h"
                ],
                "start": "2025-05-24T19:52:43Z",
                "end": "2025-05-24T19:53:30Z"
            }
        },
        {
            "XUxGA5cBpHPS2GeIwC7q": {
                "id": "XUxGA5cBpHPS2GeIwC7q",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias16.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XkxGA5cBpHPS2GeIwy6p"
                ],
                "start": "2025-05-24T19:10:59Z",
                "end": "2025-05-24T19:11:46Z"
            }
        },
        {
            "X0xRA5cBpHPS2GeIOC76": {
                "id": "X0xRA5cBpHPS2GeIOC76",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YExRA5cBpHPS2GeIOi71"
                ],
                "start": "2025-05-24T19:22:25Z",
                "end": "2025-05-24T19:23:00Z"
            }
        },
        {
            "ZExuA5cBpHPS2GeINC5D": {
                "id": "ZExuA5cBpHPS2GeINC5D",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ZUxuA5cBpHPS2GeINi49"
                ],
                "start": "2025-05-24T19:54:05Z",
                "end": "2025-05-24T19:54:40Z"
            }
        },
        {
            "W0xFA5cBpHPS2GeIny4M": {
                "id": "W0xFA5cBpHPS2GeIny4M",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias16.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XExFA5cBpHPS2GeIoS6c"
                ],
                "start": "2025-05-24T19:09:45Z",
                "end": "2025-05-24T19:10:32Z"
            }
        },
        {
            "dEyJA5cBpHPS2GeINi4h": {
                "id": "dEyJA5cBpHPS2GeINi4h",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic2.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "dUyJA5cBpHPS2GeIOC4e"
                ],
                "start": "2025-05-24T20:23:34Z",
                "end": "2025-05-24T20:24:09Z"
            }
        },
        {
            "akxzA5cBpHPS2GeIUi45": {
                "id": "akxzA5cBpHPS2GeIUi45",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "a0xzA5cBpHPS2GeIVC5B"
                ],
                "start": "2025-05-24T19:59:40Z",
                "end": "2025-05-24T20:00:15Z"
            }
        },
        {
            "ckyHA5cBpHPS2GeI_y67": {
                "id": "ckyHA5cBpHPS2GeI_y67",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "c0yIA5cBpHPS2GeIAS6l"
                ],
                "start": "2025-05-24T20:22:15Z",
                "end": "2025-05-24T20:22:50Z"
            }
        },
        {
            "ZkxvA5cBpHPS2GeIwy4q": {
                "id": "ZkxvA5cBpHPS2GeIwy4q",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Z0xvA5cBpHPS2GeIxS42"
                ],
                "start": "2025-05-24T19:55:47Z",
                "end": "2025-05-24T19:56:22Z"
            }
        },
        {
            "aExwA5cBpHPS2GeI1y4I": {
                "id": "aExwA5cBpHPS2GeI1y4I",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "aUxwA5cBpHPS2GeI2S7O"
                ],
                "start": "2025-05-24T19:56:57Z",
                "end": "2025-05-24T19:57:46Z"
            }
        },
        {
            "bEx7A5cBpHPS2GeIpy4A": {
                "id": "bEx7A5cBpHPS2GeIpy4A",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "bUx7A5cBpHPS2GeIqi6Q"
                ],
                "start": "2025-05-24T20:08:46Z",
                "end": "2025-05-24T20:09:21Z"
            }
        },
        {
            "bkx_A5cBpHPS2GeIBC5D": {
                "id": "bkx_A5cBpHPS2GeIBC5D",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "b0x_A5cBpHPS2GeIBi7I"
                ],
                "start": "2025-05-24T20:12:26Z",
                "end": "2025-05-24T20:13:14Z"
            }
        },
        {
            "cEyDA5cBpHPS2GeIEi7s": {
                "id": "cEyDA5cBpHPS2GeIEi7s",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cUyDA5cBpHPS2GeIFC7l"
                ],
                "start": "2025-05-24T20:16:52Z",
                "end": "2025-05-24T20:17:27Z"
            }
        },
        {
            "hkybA5cBpHPS2GeIXC6j": {
                "id": "hkybA5cBpHPS2GeIXC6j",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "h0ybA5cBpHPS2GeIXi69"
                ],
                "start": "2025-05-24T20:43:24Z",
                "end": "2025-05-24T20:43:58Z"
            }
        },
        {
            "ekySA5cBpHPS2GeILC4J": {
                "id": "ekySA5cBpHPS2GeILC4J",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic2.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "e0ySA5cBpHPS2GeILS74"
                ],
                "start": "2025-05-24T20:33:22Z",
                "end": "2025-05-24T20:33:56Z"
            }
        },
        {
            "fEySA5cBpHPS2GeI8i4c": {
                "id": "fEySA5cBpHPS2GeI8i4c",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_002\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fUySA5cBpHPS2GeI9C44"
                ],
                "start": "2025-05-24T20:34:12Z",
                "end": "2025-05-24T20:34:47Z"
            }
        },
        {
            "dkyNA5cBpHPS2GeI6C6I": {
                "id": "dkyNA5cBpHPS2GeI6C6I",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic2.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "d0yNA5cBpHPS2GeI6y4N"
                ],
                "start": "2025-05-24T20:28:42Z",
                "end": "2025-05-24T20:29:29Z"
            }
        },
        {
            "eEyQA5cBpHPS2GeIey6v": {
                "id": "eEyQA5cBpHPS2GeIey6v",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic2.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "eUyQA5cBpHPS2GeIfS6Z"
                ],
                "start": "2025-05-24T20:31:31Z",
                "end": "2025-05-24T20:32:05Z"
            }
        },
        {
            "fkyVA5cBpHPS2GeIGC6z": {
                "id": "fkyVA5cBpHPS2GeIGC6z",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "f0yVA5cBpHPS2GeIGi7H"
                ],
                "start": "2025-05-24T20:36:33Z",
                "end": "2025-05-24T20:37:09Z"
            }
        },
        {
            "gEyXA5cBpHPS2GeIRi6r": {
                "id": "gEyXA5cBpHPS2GeIRi6r",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "gUyXA5cBpHPS2GeISC6S"
                ],
                "start": "2025-05-24T20:38:56Z",
                "end": "2025-05-24T20:39:31Z"
            }
        },
        {
            "gkyYA5cBpHPS2GeIki7o": {
                "id": "gkyYA5cBpHPS2GeIki7o",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "g0yYA5cBpHPS2GeIlS5H"
                ],
                "start": "2025-05-24T20:40:21Z",
                "end": "2025-05-24T20:41:09Z"
            }
        },
        {
            "hEyZA5cBpHPS2GeI9i6z": {
                "id": "hEyZA5cBpHPS2GeI9i6z",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "hUyZA5cBpHPS2GeI-S5Z"
                ],
                "start": "2025-05-24T20:41:52Z",
                "end": "2025-05-24T20:42:39Z"
            }
        },
        {
            "iEyiA5cBpHPS2GeICi4L": {
                "id": "iEyiA5cBpHPS2GeICi4L",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_001\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_ilias18.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "iUyiA5cBpHPS2GeICy79"
                ],
                "start": "2025-05-24T20:50:42Z",
                "end": "2025-05-24T20:51:18Z"
            }
        },
        {
            "ikykA5cBpHPS2GeIPy74": {
                "id": "ikykA5cBpHPS2GeIPy74",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestZenohTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestZenohTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestZenohTask1InputFile;\n    Task1.TestZenohTask1OutputFile --> Task2.TestZenohTask2InputFile;\n    Task2.TestZenohTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "i0ylA5cBpHPS2GeIUS76": {
                "id": "i0ylA5cBpHPS2GeIUS76",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jEylA5cBpHPS2GeIUy7k"
                ],
                "start": "2025-05-24T20:54:17Z",
                "end": "2025-05-24T20:54:51Z"
            }
        },
        {
            "jUyuA5cBpHPS2GeICi5N": {
                "id": "jUyuA5cBpHPS2GeICi5N",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jkyuA5cBpHPS2GeIDC6-"
                ],
                "start": "2025-05-24T21:03:48Z",
                "end": "2025-05-24T21:04:36Z"
            }
        },
        {
            "kUy4A5cBpHPS2GeI1i62": {
                "id": "kUy4A5cBpHPS2GeI1i62",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "kky4A5cBpHPS2GeI2S4-"
                ],
                "start": "2025-05-24T21:15:36Z",
                "end": "2025-05-24T21:16:23Z"
            }
        },
        {
            "k0zGA5cBpHPS2GeIdS7U": {
                "id": "k0zGA5cBpHPS2GeIdS7U",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "lEzGA5cBpHPS2GeIeC5m"
                ],
                "start": "2025-05-24T21:30:29Z",
                "end": "2025-05-24T21:31:16Z"
            }
        },
        {
            "l0zPA5cBpHPS2GeIxC7q": {
                "id": "l0zPA5cBpHPS2GeIxC7q",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "mEzPA5cBpHPS2GeIxy5K"
                ],
                "start": "2025-05-24T21:40:39Z",
                "end": "2025-05-24T21:41:13Z"
            }
        },
        {
            "j0y0A5cBpHPS2GeIQy79": {
                "id": "j0y0A5cBpHPS2GeIQy79",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "kEy0A5cBpHPS2GeIRS7w"
                ],
                "start": "2025-05-24T21:10:36Z",
                "end": "2025-05-24T21:11:11Z"
            }
        },
        {
            "lUzOA5cBpHPS2GeIqi6p": {
                "id": "lUzOA5cBpHPS2GeIqi6p",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "lkzOA5cBpHPS2GeIrC6J"
                ],
                "start": "2025-05-24T21:39:26Z",
                "end": "2025-05-24T21:40:00Z"
            }
        },
        {
            "qUzkA5cBpHPS2GeIMy7n": {
                "id": "qUzkA5cBpHPS2GeIMy7n",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "qkzkA5cBpHPS2GeINS7n"
                ],
                "start": "2025-05-24T22:02:58Z",
                "end": "2025-05-24T22:03:33Z"
            }
        },
        {
            "m0zXA5cBpHPS2GeI8y5a": {
                "id": "m0zXA5cBpHPS2GeI8y5a",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "nEzXA5cBpHPS2GeI9S5p"
                ],
                "start": "2025-05-24T21:49:35Z",
                "end": "2025-05-24T21:50:09Z"
            }
        },
        {
            "nUzYA5cBpHPS2GeI1S58": {
                "id": "nUzYA5cBpHPS2GeI1S58",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "nkzYA5cBpHPS2GeI1y5e"
                ],
                "start": "2025-05-24T21:50:33Z",
                "end": "2025-05-24T21:51:07Z"
            }
        },
        {
            "p0ziA5cBpHPS2GeI-y41": {
                "id": "p0ziA5cBpHPS2GeI-y41",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "qEziA5cBpHPS2GeI_S5q"
                ],
                "start": "2025-05-24T22:01:38Z",
                "end": "2025-05-24T22:02:12Z"
            }
        },
        {
            "o0zdA5cBpHPS2GeIaS7J": {
                "id": "o0zdA5cBpHPS2GeIaS7J",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "pEzdA5cBpHPS2GeIay62"
                ],
                "start": "2025-05-24T21:55:33Z",
                "end": "2025-05-24T21:56:08Z"
            }
        },
        {
            "pUzfA5cBpHPS2GeIkC6P": {
                "id": "pUzfA5cBpHPS2GeIkC6P",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "pkzfA5cBpHPS2GeIky4R"
                ],
                "start": "2025-05-24T21:57:54Z",
                "end": "2025-05-24T21:58:41Z"
            }
        },
        {
            "n0zZA5cBpHPS2GeI5y4A": {
                "id": "n0zZA5cBpHPS2GeI5y4A",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "oEzZA5cBpHPS2GeI6C79"
                ],
                "start": "2025-05-24T21:51:43Z",
                "end": "2025-05-24T21:52:18Z"
            }
        },
        {
            "mUzVA5cBpHPS2GeI5i7f": {
                "id": "mUzVA5cBpHPS2GeI5i7f",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "mkzVA5cBpHPS2GeI6S50"
                ],
                "start": "2025-05-24T21:47:21Z",
                "end": "2025-05-24T21:48:07Z"
            }
        },
        {
            "oUzcA5cBpHPS2GeIBy4S": {
                "id": "oUzcA5cBpHPS2GeIBy4S",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "okzcA5cBpHPS2GeICS4N"
                ],
                "start": "2025-05-24T21:54:02Z",
                "end": "2025-05-24T21:54:35Z"
            }
        },
        {
            "u0waCJcBpHPS2GeImy5F": {
                "id": "u0waCJcBpHPS2GeImy5F",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/local_test/**\";\n        // path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "t0z2A5cBpHPS2GeIqi4a": {
                "id": "t0z2A5cBpHPS2GeIqi4a",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "uEz2A5cBpHPS2GeIrC4X"
                ],
                "start": "2025-05-24T22:23:08Z",
                "end": "2025-05-24T22:23:43Z"
            }
        },
        {
            "uUwKCJcBpHPS2GeIrC49": {
                "id": "uUwKCJcBpHPS2GeIrC49",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ukwKCJcBpHPS2GeIri67"
                ],
                "start": "2025-05-25T17:23:28Z",
                "end": "2025-05-25T17:24:17Z"
            }
        },
        {
            "r0zsA5cBpHPS2GeI7C4m": {
                "id": "r0zsA5cBpHPS2GeI7C4m",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/local_test/**\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "sEzsA5cBpHPS2GeI7i7B"
                ],
                "start": "2025-05-24T22:12:29Z",
                "end": "2025-05-24T22:13:17Z"
            }
        },
        {
            "sUztA5cBpHPS2GeIvi6C": {
                "id": "sUztA5cBpHPS2GeIvi6C",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/**\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "skztA5cBpHPS2GeIwC6B"
                ],
                "start": "2025-05-24T22:13:23Z",
                "end": "2025-05-24T22:13:57Z"
            }
        },
        {
            "tUz0A5cBpHPS2GeImS6n": {
                "id": "tUz0A5cBpHPS2GeImS6n",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/local_test/**\";\n        // path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "tkz0A5cBpHPS2GeImy6c"
                ],
                "start": "2025-05-24T22:20:52Z",
                "end": "2025-05-24T22:21:27Z"
            }
        },
        {
            "s0zuA5cBpHPS2GeIgS4p": {
                "id": "s0zuA5cBpHPS2GeIgS4p",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "tEzuA5cBpHPS2GeIgy4i"
                ],
                "start": "2025-05-24T22:14:13Z",
                "end": "2025-05-24T22:14:48Z"
            }
        },
        {
            "rUzoA5cBpHPS2GeIqy5N": {
                "id": "rUzoA5cBpHPS2GeIqy5N",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/local_test/**\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "rkzoA5cBpHPS2GeIrS6O"
                ],
                "start": "2025-05-24T22:07:50Z",
                "end": "2025-05-24T22:08:25Z"
            }
        },
        {
            "q0znA5cBpHPS2GeIWC6Q": {
                "id": "q0znA5cBpHPS2GeIWC6Q",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/local_test/**\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "rEznA5cBpHPS2GeIWi6W"
                ],
                "start": "2025-05-24T22:06:24Z",
                "end": "2025-05-24T22:06:58Z"
            }
        },
        {
            "vkwiCJcBpHPS2GeIai6x": {
                "id": "vkwiCJcBpHPS2GeIai6x",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "v0wiCJcBpHPS2GeIbC6P"
                ],
                "start": "2025-05-25T17:49:24Z",
                "end": "2025-05-25T17:49:58Z"
            }
        },
        {
            "vEwbCJcBpHPS2GeILC5O": {
                "id": "vEwbCJcBpHPS2GeILC5O",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/local_test/**\";\n        // path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "vUwbCJcBpHPS2GeILi5q"
                ],
                "start": "2025-05-25T17:41:29Z",
                "end": "2025-05-25T17:42:18Z"
            }
        },
        {
            "xkwrCJcBpHPS2GeIcS5p": {
                "id": "xkwrCJcBpHPS2GeIcS5p",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "x0wrCJcBpHPS2GeIcy5a"
                ],
                "start": "2025-05-25T17:59:16Z",
                "end": "2025-05-25T17:59:52Z"
            }
        },
        {
            "yEwsCJcBpHPS2GeITi4l": {
                "id": "yEwsCJcBpHPS2GeITi4l",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "yUwsCJcBpHPS2GeIUC4y"
                ],
                "start": "2025-05-25T18:00:12Z",
                "end": "2025-05-25T18:00:47Z"
            }
        },
        {
            "ykwtCJcBpHPS2GeITS6K": {
                "id": "ykwtCJcBpHPS2GeITS6K",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "y0wtCJcBpHPS2GeITy6D"
                ],
                "start": "2025-05-25T18:01:18Z",
                "end": "2025-05-25T18:01:52Z"
            }
        },
        {
            "zEwuCJcBpHPS2GeILi6i": {
                "id": "zEwuCJcBpHPS2GeILi6i",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "zUwuCJcBpHPS2GeIMC6n"
                ],
                "start": "2025-05-25T18:02:15Z",
                "end": "2025-05-25T18:02:50Z"
            }
        },
        {
            "wEwnCJcBpHPS2GeI-C6n": {
                "id": "wEwnCJcBpHPS2GeI-C6n",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/local_test/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "wUwnCJcBpHPS2GeI-i6L"
                ],
                "start": "2025-05-25T17:55:28Z",
                "end": "2025-05-25T17:56:03Z"
            }
        },
        {
            "wkwoCJcBpHPS2GeI8i5y": {
                "id": "wkwoCJcBpHPS2GeI8i5y",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"output/titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "w0woCJcBpHPS2GeI9C7p"
                ],
                "start": "2025-05-25T17:56:32Z",
                "end": "2025-05-25T17:57:20Z"
            }
        },
        {
            "xEwqCJcBpHPS2GeIDy5w": {
                "id": "xEwqCJcBpHPS2GeIDy5w",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        // path \"output/local_test/**\";\n        path \"titanic_ilias17.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "xUwqCJcBpHPS2GeIES5Z"
                ],
                "start": "2025-05-25T17:57:45Z",
                "end": "2025-05-25T17:58:19Z"
            }
        },
        {
            "3kxUCJcBpHPS2GeI4y42": {
                "id": "3kxUCJcBpHPS2GeI4y42",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "30xUCJcBpHPS2GeI5S4g"
                ],
                "start": "2025-05-25T18:44:32Z",
                "end": "2025-05-25T18:45:06Z"
            }
        },
        {
            "0Ew3CJcBpHPS2GeILS6u": {
                "id": "0Ew3CJcBpHPS2GeILS6u",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "0Uw3CJcBpHPS2GeIMC5g"
                ],
                "start": "2025-05-25T18:12:05Z",
                "end": "2025-05-25T18:12:52Z"
            }
        },
        {
            "1ExOCJcBpHPS2GeIxS56": {
                "id": "1ExOCJcBpHPS2GeIxS56",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "1UxOCJcBpHPS2GeIxy55"
                ],
                "start": "2025-05-25T18:37:51Z",
                "end": "2025-05-25T18:37:58Z"
            }
        },
        {
            "1kxPCJcBpHPS2GeIYC5_": {
                "id": "1kxPCJcBpHPS2GeIYC5_",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "10xPCJcBpHPS2GeIYi5e"
                ],
                "start": "2025-05-25T18:38:31Z",
                "end": "2025-05-25T18:38:38Z"
            }
        },
        {
            "zkw1CJcBpHPS2GeIAy6U": {
                "id": "zkw1CJcBpHPS2GeIAy6U",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "z0w1CJcBpHPS2GeIBS5-"
                ],
                "start": "2025-05-25T18:09:43Z",
                "end": "2025-05-25T18:10:16Z"
            }
        },
        {
            "0kxNCJcBpHPS2GeIdy7Z": {
                "id": "0kxNCJcBpHPS2GeIdy7Z",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "00xNCJcBpHPS2GeIei5l"
                ],
                "start": "2025-05-25T18:36:26Z",
                "end": "2025-05-25T18:37:12Z"
            }
        },
        {
            "2ExQCJcBpHPS2GeIji5d": {
                "id": "2ExQCJcBpHPS2GeIji5d",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "2UxQCJcBpHPS2GeIkC45"
                ],
                "start": "2025-05-25T18:39:48Z",
                "end": "2025-05-25T18:40:22Z"
            }
        },
        {
            "2kxRCJcBpHPS2GeIzy6X": {
                "id": "2kxRCJcBpHPS2GeIzy6X",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "20xRCJcBpHPS2GeI0i5P"
                ],
                "start": "2025-05-25T18:41:10Z",
                "end": "2025-05-25T18:41:57Z"
            }
        },
        {
            "3ExTCJcBpHPS2GeI2y5N": {
                "id": "3ExTCJcBpHPS2GeI2y5N",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "3UxTCJcBpHPS2GeI3S4x"
                ],
                "start": "2025-05-25T18:43:24Z",
                "end": "2025-05-25T18:43:59Z"
            }
        },
        {
            "9ExmCJcBpHPS2GeIPi7Q": {
                "id": "9ExmCJcBpHPS2GeIPi7Q",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9UxmCJcBpHPS2GeIQC6e"
                ],
                "start": "2025-05-25T19:03:29Z"
            }
        },
        {
            "9kxnCJcBpHPS2GeICS6c": {
                "id": "9kxnCJcBpHPS2GeICS6c",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "90xnCJcBpHPS2GeICy6T"
                ],
                "start": "2025-05-25T19:04:21Z",
                "end": "2025-05-25T19:04:52Z"
            }
        },
        {
            "8ExlCJcBpHPS2GeIOy64": {
                "id": "8ExlCJcBpHPS2GeIOy64",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "8UxlCJcBpHPS2GeIPS6p"
                ],
                "start": "2025-05-25T19:02:23Z"
            }
        },
        {
            "8kxlCJcBpHPS2GeI8S45": {
                "id": "8kxlCJcBpHPS2GeI8S45",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "80xlCJcBpHPS2GeI8y4Z"
                ],
                "start": "2025-05-25T19:03:09Z"
            }
        },
        {
            "6kxcCJcBpHPS2GeI6y6M": {
                "id": "6kxcCJcBpHPS2GeI6y6M",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "60xcCJcBpHPS2GeI7S5g"
                ],
                "start": "2025-05-25T18:53:18Z",
                "end": "2025-05-25T18:53:35Z"
            }
        },
        {
            "7ExjCJcBpHPS2GeI_C6q": {
                "id": "7ExjCJcBpHPS2GeI_C6q",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "7UxjCJcBpHPS2GeI_y5h"
                ],
                "start": "2025-05-25T19:01:01Z",
                "end": "2025-05-25T19:01:22Z"
            }
        },
        {
            "7kxkCJcBpHPS2GeIji6h": {
                "id": "7kxkCJcBpHPS2GeIji6h",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "70xkCJcBpHPS2GeIkC6Q"
                ],
                "start": "2025-05-25T19:01:39Z"
            }
        },
        {
            "5kxbCJcBpHPS2GeIty5u": {
                "id": "5kxbCJcBpHPS2GeIty5u",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "50xbCJcBpHPS2GeIuS5Y"
                ],
                "start": "2025-05-25T18:51:59Z",
                "end": "2025-05-25T18:52:07Z"
            }
        },
        {
            "6ExcCJcBpHPS2GeIOC6S": {
                "id": "6ExcCJcBpHPS2GeIOC6S",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6UxcCJcBpHPS2GeIOi5i"
                ],
                "start": "2025-05-25T18:52:32Z",
                "end": "2025-05-25T18:52:58Z"
            }
        },
        {
            "4ExVCJcBpHPS2GeI8S7A": {
                "id": "4ExVCJcBpHPS2GeI8S7A",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "4UxVCJcBpHPS2GeI8y6l"
                ],
                "start": "2025-05-25T18:45:41Z",
                "end": "2025-05-25T18:46:15Z"
            }
        },
        {
            "4kxZCJcBpHPS2GeIUS4n": {
                "id": "4kxZCJcBpHPS2GeIUS4n",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "40xZCJcBpHPS2GeIUy4D"
                ],
                "start": "2025-05-25T18:49:22Z",
                "end": "2025-05-25T18:49:55Z"
            }
        },
        {
            "5ExaCJcBpHPS2GeIhC4V": {
                "id": "5ExaCJcBpHPS2GeIhC4V",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "5UxaCJcBpHPS2GeIhi4I"
                ],
                "start": "2025-05-25T18:50:41Z",
                "end": "2025-05-25T18:51:15Z"
            }
        },
        {
            "CEx8CJcBpHPS2GeINC8T": {
                "id": "CEx8CJcBpHPS2GeINC8T",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "CUx8CJcBpHPS2GeINi95"
                ],
                "start": "2025-05-25T19:27:28Z"
            }
        },
        {
            "Ckx8CJcBpHPS2GeIrC-S": {
                "id": "Ckx8CJcBpHPS2GeIrC-S",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "C0x8CJcBpHPS2GeIri99"
                ],
                "start": "2025-05-25T19:27:59Z"
            }
        },
        {
            "DEx8CJcBpHPS2GeI_i-l": {
                "id": "DEx8CJcBpHPS2GeI_i-l",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DUx9CJcBpHPS2GeIAC-R"
                ],
                "start": "2025-05-25T19:28:20Z"
            }
        },
        {
            "-kxpCJcBpHPS2GeIhS5L": {
                "id": "-kxpCJcBpHPS2GeIhS5L",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-0xpCJcBpHPS2GeIhy4p"
                ],
                "start": "2025-05-25T19:07:04Z",
                "end": "2025-05-25T19:07:21Z"
            }
        },
        {
            "_ExqCJcBpHPS2GeIaC4H": {
                "id": "_ExqCJcBpHPS2GeIaC4H",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_UxqCJcBpHPS2GeIaS7n"
                ],
                "start": "2025-05-25T19:08:02Z",
                "end": "2025-05-25T19:08:19Z"
            }
        },
        {
            "BEx5CJcBpHPS2GeIYC9r": {
                "id": "BEx5CJcBpHPS2GeIYC9r",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "BUx5CJcBpHPS2GeIYi9Q"
                ],
                "start": "2025-05-25T19:24:23Z"
            }
        },
        {
            "Bkx6CJcBpHPS2GeIdy8V": {
                "id": "Bkx6CJcBpHPS2GeIdy8V",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "B0x6CJcBpHPS2GeIeC_v"
                ],
                "start": "2025-05-25T19:25:34Z"
            }
        },
        {
            "_kx0CJcBpHPS2GeI5S5J": {
                "id": "_kx0CJcBpHPS2GeI5S5J",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "_0x0CJcBpHPS2GeI5y7T"
                ],
                "start": "2025-05-25T19:19:29Z",
                "end": "2025-05-25T19:19:59Z"
            }
        },
        {
            "AEx2CJcBpHPS2GeIIS8a": {
                "id": "AEx2CJcBpHPS2GeIIS8a",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "AUx2CJcBpHPS2GeIIy80"
                ],
                "start": "2025-05-25T19:20:50Z",
                "end": "2025-05-25T19:20:58Z"
            }
        },
        {
            "Akx3CJcBpHPS2GeI1y-k": {
                "id": "Akx3CJcBpHPS2GeI1y-k",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "A0x3CJcBpHPS2GeI2S-K"
                ],
                "start": "2025-05-25T19:22:42Z",
                "end": "2025-05-25T19:22:50Z"
            }
        },
        {
            "-ExnCJcBpHPS2GeIuS7m": {
                "id": "-ExnCJcBpHPS2GeIuS7m",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-UxnCJcBpHPS2GeIvC44"
                ],
                "start": "2025-05-25T19:05:06Z",
                "end": "2025-05-25T19:05:24Z"
            }
        },
        {
            "PEyZCJcBpHPS2GeIqi-G": {
                "id": "PEyZCJcBpHPS2GeIqi-G",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "PUyZCJcBpHPS2GeIry-5"
                ],
                "start": "2025-05-25T19:59:39Z",
                "end": "2025-05-25T20:00:11Z"
            }
        },
        {
            "PkyaCJcBpHPS2GeIVS-i": {
                "id": "PkyaCJcBpHPS2GeIVS-i",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "P0yaCJcBpHPS2GeIVy-V"
                ],
                "start": "2025-05-25T20:00:23Z",
                "end": "2025-05-25T20:00:53Z"
            }
        },
        {
            "Dkx9CJcBpHPS2GeIUC90": {
                "id": "Dkx9CJcBpHPS2GeIUC90",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "D0x9CJcBpHPS2GeIUi9j"
                ],
                "start": "2025-05-25T19:28:41Z",
                "end": "2025-05-25T19:29:12Z"
            }
        },
        {
            "OEyPCJcBpHPS2GeIki9t": {
                "id": "OEyPCJcBpHPS2GeIki9t",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "OUyPCJcBpHPS2GeIlC-p"
                ],
                "start": "2025-05-25T19:48:38Z",
                "end": "2025-05-25T19:49:07Z"
            }
        },
        {
            "NkyNCJcBpHPS2GeIKC-T": {
                "id": "NkyNCJcBpHPS2GeIKC-T",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "N0yNCJcBpHPS2GeIKi_5"
                ],
                "start": "2025-05-25T19:45:59Z"
            }
        },
        {
            "EEyFCJcBpHPS2GeIEy9z": {
                "id": "EEyFCJcBpHPS2GeIEy9z",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"CS_surrogate_model/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "EUyFCJcBpHPS2GeIFC9X",
                    "GkyKCJcBpHPS2GeI0y8r",
                    "IUyKCJcBpHPS2GeI1C-X",
                    "KEyKCJcBpHPS2GeI1S_P",
                    "L0yKCJcBpHPS2GeI1i_t"
                ],
                "start": "2025-05-25T20:37:09Z"
            }
        },
        {
            "OkyWCJcBpHPS2GeIxS9D": {
                "id": "OkyWCJcBpHPS2GeIxS9D",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "O0yWCJcBpHPS2GeIxy_Z"
                ],
                "start": "2025-05-25T19:56:29Z",
                "end": "2025-05-25T19:57:12Z"
            }
        },
        {
            "GEyFCJcBpHPS2GeINy-H": {
                "id": "GEyFCJcBpHPS2GeINy-H",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GUyFCJcBpHPS2GeIOS-A"
                ],
                "start": "2025-05-25T19:37:19Z",
                "end": "2025-05-25T19:37:43Z"
            }
        },
        {
            "bEyrC5cBpHPS2GeIxC8E": {
                "id": "bEyrC5cBpHPS2GeIxC8E",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "ZEymC5cBpHPS2GeIYS9l": {
                "id": "ZEymC5cBpHPS2GeIYS9l",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"CS_surrogate_model/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "ZUymC5cBpHPS2GeIYy8E"
                ],
                "start": "2025-05-26T11:12:24Z",
                "end": "2025-05-26T11:18:20Z"
            }
        },
        {
            "bUytC5cBpHPS2GeINy94": {
                "id": "bUytC5cBpHPS2GeINy94",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "new",
                "workflow_ids": []
            }
        },
        {
            "REw0CZcBpHPS2GeIAi_w": {
                "id": "REw0CZcBpHPS2GeIAi_w",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"CS_surrogate_model/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "RUw0CZcBpHPS2GeIBC87"
                ],
                "start": "2025-05-25T23:48:14Z",
                "end": "2025-05-25T23:54:06Z"
            }
        },
        {
            "WUyCC5cBpHPS2GeIoC8a": {
                "id": "WUyCC5cBpHPS2GeIoC8a",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "WkyCC5cBpHPS2GeIoi85"
                ],
                "start": "2025-05-26T07:33:21Z",
                "end": "2025-05-26T07:42:32Z"
            }
        },
        {
            "TExKC5cBpHPS2GeIOy9U": {
                "id": "TExKC5cBpHPS2GeIOy9U",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "TUxKC5cBpHPS2GeIPi9A"
                ],
                "start": "2025-05-26T08:31:45Z",
                "end": "2025-05-26T08:32:39Z"
            }
        },
        {
            "QkycCJcBpHPS2GeIai_C": {
                "id": "QkycCJcBpHPS2GeIai_C",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Q0ycCJcBpHPS2GeIbC-4"
                ],
                "start": "2025-05-25T20:02:39Z",
                "end": "2025-05-25T20:03:05Z"
            }
        },
        {
            "Tkx-C5cBpHPS2GeI8S9-": {
                "id": "Tkx-C5cBpHPS2GeI8S9-",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "T0x-C5cBpHPS2GeI9C_A"
                ],
                "start": "2025-05-26T07:29:19Z",
                "end": "2025-05-26T07:30:16Z"
            }
        },
        {
            "QEybCJcBpHPS2GeIDi-t": {
                "id": "QEybCJcBpHPS2GeIDi-t",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "QUybCJcBpHPS2GeIEC-P"
                ],
                "start": "2025-05-25T20:01:10Z",
                "end": "2025-05-25T20:01:39Z"
            }
        },
        {
            "dkzGC5cBpHPS2GeI-S_m": {
                "id": "dkzGC5cBpHPS2GeI-S_m",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"CS_surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"CS_surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"CS_surrogate_model/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.X_EVAL --> EvaluationModel.X_EVAL;\n    TrainModel.Y_EVAL --> EvaluationModel.Y_EVAL;\n    TrainModel.MASK_EVAL --> EvaluationModel.MASK_EVAL;\n    // TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    // ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    // ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    // ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "d0zGC5cBpHPS2GeI-y_R"
                ],
                "start": "2025-05-26T11:48:00Z",
                "end": "2025-05-26T11:54:16Z"
            }
        },
        {
            "gEzOC5cBpHPS2GeIRy-k": {
                "id": "gEzOC5cBpHPS2GeIRy-k",
                "name": "CS_surrogate_model",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> ReadData -> TrainModel -> EvaluationModel -> END;\n\n    task ReadData {\n        implementation \"UC1.surrogate_model.ReadData\";\n    }\n\n    task TrainModel;\n\n    task EvaluationModel {\n        implementation \"UC1.surrogate_model.EvaluationModel\";\n    }\n\n    // DATA\n    define input data ExternalDataFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFolder --> ReadData.FolderToRead;\n\n    configure data ExternalDataFolder {\n        path \"UC1/nimes_2002_v1/**\";\n    }\n\n    define output data TrainedModelFolder;\n\n    TrainModel.OutputFolder --> TrainedModelFolder;\n\n    ReadData.X_TRAIN --> TrainModel.X_TRAIN;\n    ReadData.Y_TRAIN --> TrainModel.Y_TRAIN;\n    ReadData.MASK_TRAIN --> TrainModel.MASK_TRAIN;\n    ReadData.X_VAL --> TrainModel.X_VAL;\n    ReadData.Y_VAL --> TrainModel.Y_VAL;\n    ReadData.MASK_VAL --> TrainModel.MASK_VAL;\n\n    TrainModel.OutputFolder --> EvaluationModel.OutputFolder;\n    TrainModel.TrainedModel --> EvaluationModel.TrainedModel;\n\n    ReadData.X_VAL --> EvaluationModel.X_EVAL;\n    ReadData.Y_VAL --> EvaluationModel.Y_EVAL;\n    ReadData.MASK_VAL --> EvaluationModel.MASK_EVAL;\n}\n\nworkflow TrainModelAdam from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModel\";\n  }\n}\n\nworkflow TrainModelSGD from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"UC1.surrogate_model.TrainModel\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> S2 -> END;\n    }\n\n    space S1 of TrainModelAdam {\n        strategy gridsearch;\n        param epochs_vp = enum(2);\n        param batch_size_vp = enum(8);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n\n    space S2 of TrainModelSGD {\n        strategy gridsearch;\n        param epochs_vp = range(50, 150, 50);\n        param batch_size_vp = enum(12, 64);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"SGD\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n        }\n    }\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "cEyvC5cBpHPS2GeI5S9D": {
                "id": "cEyvC5cBpHPS2GeI5S9D",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:22:47Z"
            }
        },
        {
            "cUyvC5cBpHPS2GeI5y9k": {
                "id": "cUyvC5cBpHPS2GeI5y9k",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:22:48Z"
            }
        },
        {
            "ckzGC5cBpHPS2GeINy9f": {
                "id": "ckzGC5cBpHPS2GeINy9f",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:47:10Z"
            }
        },
        {
            "c0zGC5cBpHPS2GeIOS_s": {
                "id": "c0zGC5cBpHPS2GeIOS_s",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:47:11Z"
            }
        },
        {
            "dEzGC5cBpHPS2GeI0i9g": {
                "id": "dEzGC5cBpHPS2GeI0i9g",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:47:50Z"
            }
        },
        {
            "dUzGC5cBpHPS2GeI1i80": {
                "id": "dUzGC5cBpHPS2GeI1i80",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:47:51Z"
            }
        },
        {
            "bkyuC5cBpHPS2GeILC-M": {
                "id": "bkyuC5cBpHPS2GeILC-M",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:20:55Z"
            }
        },
        {
            "b0yuC5cBpHPS2GeILy9A": {
                "id": "b0yuC5cBpHPS2GeILy9A",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T11:20:55Z"
            }
        },
        {
            "gUwEDJcBpHPS2GeIYi8H": {
                "id": "gUwEDJcBpHPS2GeIYi8H",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n        path \"output/trained_model/**\";\n        type \"generated-ML-model\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "gkwEDJcBpHPS2GeIZC_U"
                ],
                "start": "2025-05-26T09:55:04Z"
            }
        },
        {
            "4UwnDJcBpHPS2GeIMi90": {
                "id": "4UwnDJcBpHPS2GeIMi90",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "4kwnDJcBpHPS2GeINC9N"
                ],
                "start": "2025-05-26T10:33:05Z",
                "end": "2025-05-26T10:33:52Z"
            }
        },
        {
            "fkzMC5cBpHPS2GeIPy-h": {
                "id": "fkzMC5cBpHPS2GeIPy-h",
                "name": "uc4_minimal_pipeline",
                "model": "// ************************************************************************************************\n// * MINIMAL UC4 TEST (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nworkflow uc4_minimal_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START -> UC4ValidateInput -> UC4DummyProcess -> END;\n\n  // Task Definitions (using the UC5-like identifier style)\n  task UC4ValidateInput {\n    // This identifier will be resolved by the system to find:\n    // your_project_root/UC4/tasks/ValidateInput/task.xxp (or similar convention)\n    implementation \"UC4/improved_mode_detection/ValidateInput\";\n  }\n\n  task UC4DummyProcess {\n    implementation \"UC4/dummy_tasks/DummyProcess\"; // Adjusted from DummyProcessTask\n  }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data LocationDataSingleDayUser;\n\n  // DATA CONNECTIONS: Workflow Input to First Task\n  LocationDataSingleDayUser --> UC4ValidateInput.LocationDataSingleDayUser;\n\n  configure data LocationDataSingleDayUser {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n\n  define output data FinalUC4Message;\n\n  // DATA CONNECTIONS: Last Task Output to Workflow Output\n  UC4DummyProcess.ProcessingMessage --> FinalUC4Message;\n\n  configure data FinalUC4Message {\n    path \"UC4/uc4_minimal_test/final_message.txt\";\n    // type \"text-message\"; // Optional type, similar to UC5\n  }\n\n  // DATA CONNECTIONS: Between Tasks\n  UC4ValidateInput.UserId --> UC4DummyProcess.InputUserID;\n  UC4ValidateInput.ValidatedLocations --> UC4DummyProcess.InputLocations;\n}\n\nworkflow uc4_minimal_pipeline_assembled from uc4_minimal_pipeline {\n}\n\n// ************************************************************************************************\n// * EXPERIMENT (Modeled on UC5 Structure)\n// ************************************************************************************************\n\nexperiment UC4MinimalExperiment {\n  // intent \"Test basic UC4 two-task pipeline\"; // Optional intent\n\n  control {\n    START -> MinimalSpace -> END;\n  }\n\n  space MinimalSpace of uc4_minimal_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, etc.\n    // runs = 1; // Implied for gridsearch with no params\n    // --- Optional: Parameterization for UC4ValidateInput ---\n    // This mirrors how parameters were set in your UC5 example.\n    // 'param_vp' are space-level virtual parameters.\n    // param min_records_vp = enum(2);\n    // param max_acc_thresh_vp = enum(120);\n    // task UC4ValidateInput {\n    //   param min_records = min_records_vp;\n    //   param max_acc_threshold = max_acc_thresh_vp;\n    //}\n  }\n}",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "f0zMC5cBpHPS2GeIQS_p"
                ],
                "start": "2025-05-26T11:53:45Z",
                "end": "2025-05-26T11:54:07Z"
            }
        },
        {
            "jEwcDJcBpHPS2GeIvS-V": {
                "id": "jEwcDJcBpHPS2GeIvS-V",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        param min_child_weight = range(1,3,2);\n        // param min_child_weight = enum(3);\n        param gamma = range(1,3,1);\n        // param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jUwcDJcBpHPS2GeIvy-8",
                    "lEwcDJcBpHPS2GeIwC9G",
                    "m0wcDJcBpHPS2GeIwC-7",
                    "okwcDJcBpHPS2GeIwS8y",
                    "qUwcDJcBpHPS2GeIwS-r",
                    "sEwcDJcBpHPS2GeIwi8o",
                    "t0wcDJcBpHPS2GeIwi-B",
                    "vkwcDJcBpHPS2GeIwi_3",
                    "xUwcDJcBpHPS2GeIwy9y",
                    "zEwcDJcBpHPS2GeIwy_q",
                    "00wcDJcBpHPS2GeIxC9p",
                    "2kwcDJcBpHPS2GeIxC_l"
                ],
                "start": "2025-05-26T10:21:40Z",
                "end": "2025-05-26T10:25:00Z"
            }
        },
        {
            "BExzDJcBpHPS2GeIwjCW": {
                "id": "BExzDJcBpHPS2GeIwjCW",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "BUxzDJcBpHPS2GeIxDCk"
                ],
                "start": "2025-05-26T11:56:42Z",
                "end": "2025-05-26T11:59:31Z"
            }
        },
        {
            "DEx1DJcBpHPS2GeIvjDU": {
                "id": "DEx1DJcBpHPS2GeIvjDU",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "DUx1DJcBpHPS2GeIwDCu"
                ],
                "start": "2025-05-26T11:58:53Z",
                "end": "2025-05-26T12:02:00Z"
            }
        },
        {
            "_0xoDJcBpHPS2GeIbi_a": {
                "id": "_0xoDJcBpHPS2GeIbi_a",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "AkxqDJcBpHPS2GeIPDCM": {
                "id": "AkxqDJcBpHPS2GeIPDCM",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T13:46:19Z"
            }
        },
        {
            "A0xqDJcBpHPS2GeIPjBO": {
                "id": "A0xqDJcBpHPS2GeIPjBO",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T13:46:20Z"
            }
        },
        {
            "AExoDJcBpHPS2GeI6DD9": {
                "id": "AExoDJcBpHPS2GeI6DD9",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T13:44:53Z"
            }
        },
        {
            "AUxoDJcBpHPS2GeI6jDz": {
                "id": "AUxoDJcBpHPS2GeI6jDz",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "running",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [],
                "start": "2025-05-26T13:44:53Z"
            }
        },
        {
            "9ExgDJcBpHPS2GeIdS_b": {
                "id": "9ExgDJcBpHPS2GeIdS_b",
                "name": "CS_surrogate_model_API",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> TrainModel -> ReadMetrics -> END;\n\n    task TrainModel;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModel.FileToRead;\n    TrainModel.jobID --> ReadMetrics.jobID;\n    TrainModel.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainModelAPI from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainModelAPI {\n        strategy gridsearch;\n        param epochs_vp = enum(20);\n        param batch_size_vp = enum(2);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "9UxgDJcBpHPS2GeIdi-f"
                ],
                "start": "2025-05-26T14:35:39Z",
                "end": "2025-05-26T14:36:17Z"
            }
        },
        {
            "6UxeDJcBpHPS2GeIPi_o": {
                "id": "6UxeDJcBpHPS2GeIPi_o",
                "name": "CS_surrogate_model_API",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> TrainModel -> ReadMetrics -> END;\n\n    task TrainModel;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModel.FileToRead;\n    TrainModel.jobID --> ReadMetrics.jobID;\n    TrainModel.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainModelAPI from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainModelAPI {\n        strategy gridsearch;\n        param epochs_vp = enum(20);\n        param batch_size_vp = enum(2);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6kxeDJcBpHPS2GeIPy-0"
                ],
                "start": "2025-05-26T14:33:14Z",
                "end": "2025-05-26T14:33:53Z"
            }
        },
        {
            "i0yNDJcBpHPS2GeIyjDF": {
                "id": "i0yNDJcBpHPS2GeIyjDF",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        // param max_depth = enum(3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        param min_child_weight = range(1,3,2);\n        // param min_child_weight = enum(3);\n        param gamma = range(1,3,1);\n        // param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jEyNDJcBpHPS2GeIzDDI",
                    "k0yNDJcBpHPS2GeIzTAr",
                    "mkyNDJcBpHPS2GeIzTB1",
                    "oUyNDJcBpHPS2GeIzTDB",
                    "qEyNDJcBpHPS2GeIzjAX",
                    "r0yNDJcBpHPS2GeIzjCO",
                    "tkyNDJcBpHPS2GeIzjD-",
                    "vUyNDJcBpHPS2GeIzzBy",
                    "xEyNDJcBpHPS2GeIzzDg",
                    "y0yNDJcBpHPS2GeI0DA8",
                    "0kyNDJcBpHPS2GeI0DC1",
                    "2UyNDJcBpHPS2GeI0TAW"
                ],
                "start": "2025-05-26T12:25:09Z",
                "end": "2025-05-26T13:01:36Z"
            }
        },
        {
            "4EyODJcBpHPS2GeIgTDc": {
                "id": "4EyODJcBpHPS2GeIgTDc",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "4UyODJcBpHPS2GeIgjCh"
                ],
                "start": "2025-05-26T15:25:57Z",
                "end": "2025-05-26T15:34:58Z"
            }
        },
        {
            "AkzNDJcBpHPS2GeISTEr": {
                "id": "AkzNDJcBpHPS2GeISTEr",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "A0zNDJcBpHPS2GeISTH5"
                ],
                "start": "2025-05-26T16:34:31Z",
                "end": "2025-05-26T16:48:34Z"
            }
        },
        {
            "dUx_DJcBpHPS2GeIgzBa": {
                "id": "dUx_DJcBpHPS2GeIgzBa",
                "name": "CS_surrogate_model_API",
                "model": "package CS_surrogate_model;\n\nworkflow CS_Surrogate_main {\n\n    // Task CONNECTIONS\n    START -> TrainModel -> ReadMetrics -> END;\n\n    task TrainModel;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> TrainModel.FileToRead;\n    TrainModel.jobID --> ReadMetrics.jobID;\n    TrainModel.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainModelAPI from CS_Surrogate_main {\n  task TrainModel {\n    implementation \"CS_surrogate_model.TrainModelAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainModelAPI {\n        strategy gridsearch;\n        param epochs_vp = enum(20);\n        param batch_size_vp = enum(2);\n        param lr_vp = enum(0.001);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        task TrainModel {\n            param epochs = epochs_vp;\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "dkx_DJcBpHPS2GeIhDAr"
                ],
                "start": "2025-05-26T15:09:34Z",
                "end": "2025-05-26T15:10:09Z"
            }
        },
        {
            "FEx5DJcBpHPS2GeIkzAE": {
                "id": "FEx5DJcBpHPS2GeIkzAE",
                "name": "I2CAT_workflow1_Explainability",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n    \n    define output data MLAnalysisResources;\n    \n    Explainability.MLAnalysisResources --> MLAnalysisResources;\n\n\n    configure data MLAnalysisResources {\n    path \"output/MLAnalysisResources/**\";\n    }\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth = range(3, 7, 3);\n        // param max_depth = enum(3);\n        param n_estimators = range(5, 16, 5);\n        // param n_estimators = enum(5);\n        param min_child_weight = range(1,3,2);\n        // param min_child_weight = enum(3);\n        param gamma = range(1,3,1);\n        // param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "FUx5DJcBpHPS2GeIlDD3",
                    "HEx5DJcBpHPS2GeIlTCT",
                    "I0x5DJcBpHPS2GeIlTD5",
                    "Kkx5DJcBpHPS2GeIljBx",
                    "MUx5DJcBpHPS2GeIljDf",
                    "OEx5DJcBpHPS2GeIlzBS",
                    "P0x5DJcBpHPS2GeIlzDH",
                    "Rkx5DJcBpHPS2GeImDA-",
                    "TUx5DJcBpHPS2GeImDCg",
                    "VEx5DJcBpHPS2GeImTAF",
                    "W0x5DJcBpHPS2GeImTBq",
                    "Ykx5DJcBpHPS2GeImTDJ"
                ],
                "start": "2025-05-26T12:03:04Z",
                "end": "2025-05-26T12:14:34Z"
            }
        },
        {
            "GEz-DJcBpHPS2GeIEzEi": {
                "id": "GEz-DJcBpHPS2GeIEzEi",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "GUz-DJcBpHPS2GeIFTEs"
                ],
                "start": "2025-05-26T14:27:47Z",
                "end": "2025-05-26T14:30:48Z"
            }
        },
        {
            "gEyMDJcBpHPS2GeINjDY": {
                "id": "gEyMDJcBpHPS2GeINjDY",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "gUyMDJcBpHPS2GeINzCi"
                ],
                "start": "2025-05-26T15:23:26Z",
                "end": "2025-05-26T15:24:07Z"
            }
        },
        {
            "90zMDJcBpHPS2GeIXDB3": {
                "id": "90zMDJcBpHPS2GeIXDB3",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "-EzMDJcBpHPS2GeIXTBN"
                ],
                "start": "2025-05-26T16:33:30Z",
                "end": "2025-05-26T16:34:18Z"
            }
        },
        {
            "DUz5DJcBpHPS2GeISjEu": {
                "id": "DUz5DJcBpHPS2GeISjEu",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Dkz5DJcBpHPS2GeISzEB"
                ],
                "start": "2025-05-26T17:22:35Z",
                "end": "2025-05-26T17:31:37Z"
            }
        },
        {
            "Qkw3DZcBpHPS2GeIRTEU": {
                "id": "Qkw3DZcBpHPS2GeIRTEU",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Q0w3DZcBpHPS2GeIRTHk"
                ],
                "start": "2025-05-26T18:30:17Z",
                "end": "2025-05-26T18:39:20Z"
            }
        },
        {
            "KkwYDZcBpHPS2GeIHzGq": {
                "id": "KkwYDZcBpHPS2GeIHzGq",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "K0wYDZcBpHPS2GeIITGh"
                ],
                "start": "2025-05-26T14:56:14Z",
                "end": "2025-05-26T14:59:21Z"
            }
        },
        {
            "OkwhDZcBpHPS2GeI0THP": {
                "id": "OkwhDZcBpHPS2GeI0THP",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "O0whDZcBpHPS2GeI0zHW"
                ],
                "start": "2025-05-26T15:06:50Z",
                "end": "2025-05-26T15:09:59Z"
            }
        },
        {
            "IkwQDZcBpHPS2GeIpDG-": {
                "id": "IkwQDZcBpHPS2GeIpDG-",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "I0wQDZcBpHPS2GeIpjHo"
                ],
                "start": "2025-05-26T14:48:04Z",
                "end": "2025-05-26T14:48:12Z"
            }
        },
        {
            "MkwbDZcBpHPS2GeIODEt": {
                "id": "MkwbDZcBpHPS2GeIODEt",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "M0wbDZcBpHPS2GeIOjFf"
                ],
                "start": "2025-05-26T14:59:37Z",
                "end": "2025-05-26T15:02:44Z"
            }
        },
        {
            "IEwJDZcBpHPS2GeIwzH6": {
                "id": "IEwJDZcBpHPS2GeIwzH6",
                "name": "test_zenoh",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        zenoh_name \"titanic.json\";\n        zenoh_project \"demo_project_ilias\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        zenoh_name \"titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "IUwJDZcBpHPS2GeIyjFx"
                ],
                "start": "2025-05-26T16:40:35Z",
                "end": "2025-05-26T16:41:36Z"
            }
        },
        {
            "TUxJDZcBpHPS2GeIDDGs": {
                "id": "TUxJDZcBpHPS2GeIDDGs",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "TkxJDZcBpHPS2GeIDTF8"
                ],
                "start": "2025-05-26T18:49:42Z",
                "end": "2025-05-26T18:58:45Z"
            }
        },
        {
            "Z0x9DpcBpHPS2GeIdTED": {
                "id": "Z0x9DpcBpHPS2GeIdTED",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "aEx9DpcBpHPS2GeIdzHs"
                ],
                "start": "2025-05-26T21:26:35Z",
                "end": "2025-05-26T21:27:00Z"
            }
        },
        {
            "aUx-DpcBpHPS2GeIoDHP": {
                "id": "aUx-DpcBpHPS2GeIoDHP",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Ykx5DpcBpHPS2GeI_DGM": {
                "id": "Ykx5DpcBpHPS2GeI_DGM",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Y0x6DpcBpHPS2GeIADEa"
                ],
                "start": "2025-05-26T21:22:47Z",
                "end": "2025-05-26T21:23:12Z"
            }
        },
        {
            "ZEx8DpcBpHPS2GeIBjEU": {
                "id": "ZEx8DpcBpHPS2GeIBjEU",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        // path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "YExYDpcBpHPS2GeItjFB": {
                "id": "YExYDpcBpHPS2GeItjFB",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YUxYDpcBpHPS2GeIujG8"
                ],
                "start": "2025-05-26T22:46:27Z",
                "end": "2025-05-26T22:47:15Z"
            }
        },
        {
            "WExMDpcBpHPS2GeIwzHm": {
                "id": "WExMDpcBpHPS2GeIwzHm",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "WUxMDpcBpHPS2GeIyDEg"
                ],
                "start": "2025-05-26T22:33:24Z",
                "end": "2025-05-26T22:34:09Z"
            }
        },
        {
            "WkxRDpcBpHPS2GeIdDHF": {
                "id": "WkxRDpcBpHPS2GeIdDHF",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "W0xRDpcBpHPS2GeIdzHZ"
                ],
                "start": "2025-05-26T22:38:31Z",
                "end": "2025-05-26T22:39:00Z"
            }
        },
        {
            "XExTDpcBpHPS2GeIhzGn": {
                "id": "XExTDpcBpHPS2GeIhzGn",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "XUxTDpcBpHPS2GeIijGa"
                ],
                "start": "2025-05-26T22:40:47Z",
                "end": "2025-05-26T22:41:17Z"
            }
        },
        {
            "XkxWDpcBpHPS2GeIYzGG": {
                "id": "XkxWDpcBpHPS2GeIYzGG",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "X0xWDpcBpHPS2GeIZjHt"
                ],
                "start": "2025-05-26T22:43:55Z",
                "end": "2025-05-26T22:44:26Z"
            }
        },
        {
            "ZUx8DpcBpHPS2GeIiDFl": {
                "id": "ZUx8DpcBpHPS2GeIiDFl",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "Zkx8DpcBpHPS2GeIizEh"
                ],
                "start": "2025-05-26T21:25:34Z",
                "end": "2025-05-26T21:26:00Z"
            }
        },
        {
            "cUyJDpcBpHPS2GeIKjG1": {
                "id": "cUyJDpcBpHPS2GeIKjG1",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ckyNDpcBpHPS2GeIjzHZ": {
                "id": "ckyNDpcBpHPS2GeIjzHZ",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "akx_DpcBpHPS2GeIsjF9": {
                "id": "akx_DpcBpHPS2GeIsjF9",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "a0yDDpcBpHPS2GeILzH7": {
                "id": "a0yDDpcBpHPS2GeILzH7",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bEyEDpcBpHPS2GeI3DFo": {
                "id": "bEyEDpcBpHPS2GeI3DFo",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bUyGDpcBpHPS2GeI_DFG": {
                "id": "bUyGDpcBpHPS2GeI_DFG",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "bkyHDpcBpHPS2GeI6zFn": {
                "id": "bkyHDpcBpHPS2GeI6zFn",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "c0yUDpcBpHPS2GeICTHk": {
                "id": "c0yUDpcBpHPS2GeICTHk",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "dUyVDpcBpHPS2GeInTFO": {
                "id": "dUyVDpcBpHPS2GeInTFO",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "dkyVDpcBpHPS2GeI2zEY": {
                "id": "dkyVDpcBpHPS2GeI2zEY",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "d0yYDpcBpHPS2GeIWTGq": {
                "id": "d0yYDpcBpHPS2GeIWTGq",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "gUyeDpcBpHPS2GeIpDF6": {
                "id": "gUyeDpcBpHPS2GeIpDF6",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "gkyeDpcBpHPS2GeIwjHe": {
                "id": "gkyeDpcBpHPS2GeIwjHe",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "g0yiDpcBpHPS2GeI3DF_": {
                "id": "g0yiDpcBpHPS2GeI3DF_",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "hEyiDpcBpHPS2GeI_TFe": {
                "id": "hEyiDpcBpHPS2GeI_TFe",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "hUyjDpcBpHPS2GeIGDEI": {
                "id": "hUyjDpcBpHPS2GeIGDEI",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "hkykDpcBpHPS2GeIwTGc": {
                "id": "hkykDpcBpHPS2GeIwTGc",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "h0ykDpcBpHPS2GeI5DEU": {
                "id": "h0ykDpcBpHPS2GeI5DEU",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "iEylDpcBpHPS2GeIPzEg": {
                "id": "iEylDpcBpHPS2GeIPzEg",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask21\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "fEyZDpcBpHPS2GeIhjFu": {
                "id": "fEyZDpcBpHPS2GeIhjFu",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "fUyZDpcBpHPS2GeIiDHv"
                ],
                "start": "2025-05-26T21:57:14Z",
                "end": "2025-05-26T21:57:39Z"
            }
        },
        {
            "fkyaDpcBpHPS2GeIADGg": {
                "id": "fkyaDpcBpHPS2GeIADGg",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "f0ycDpcBpHPS2GeIajE9": {
                "id": "f0ycDpcBpHPS2GeIajE9",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "dEyVDpcBpHPS2GeIgTFs": {
                "id": "dEyVDpcBpHPS2GeIgTFs",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "gEyeDpcBpHPS2GeIVzEW": {
                "id": "gEyeDpcBpHPS2GeIVzEW",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "eEyYDpcBpHPS2GeIaTFl": {
                "id": "eEyYDpcBpHPS2GeIaTFl",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "eUyYDpcBpHPS2GeIhzFH": {
                "id": "eUyYDpcBpHPS2GeIhzFH",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "b0yIDpcBpHPS2GeIezGz": {
                "id": "b0yIDpcBpHPS2GeIezGz",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "cEyIDpcBpHPS2GeIfzFc"
                ],
                "start": "2025-05-26T21:38:37Z",
                "end": "2025-05-26T21:39:03Z"
            }
        },
        {
            "ekyYDpcBpHPS2GeIwTEf": {
                "id": "ekyYDpcBpHPS2GeIwTEf",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "e0yYDpcBpHPS2GeIxDEi"
                ],
                "start": "2025-05-26T21:56:23Z",
                "end": "2025-05-26T21:56:48Z"
            }
        },
        {
            "iUyqDpcBpHPS2GeIOzHr": {
                "id": "iUyqDpcBpHPS2GeIOzHr",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask21\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ikyqDpcBpHPS2GeIXTHz": {
                "id": "ikyqDpcBpHPS2GeIXTHz",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask21\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "i0yqDpcBpHPS2GeIwTF5": {
                "id": "i0yqDpcBpHPS2GeIwTF5",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "t0zxEJcBpHPS2GeI8jEq": {
                "id": "t0zxEJcBpHPS2GeI8jEq",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "uEzxEJcBpHPS2GeI8zFc"
                ],
                "start": "2025-05-27T08:53:01Z",
                "end": "2025-05-27T08:53:10Z"
            }
        },
        {
            "r0zwEJcBpHPS2GeIiDG8": {
                "id": "r0zwEJcBpHPS2GeIiDG8",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "sEzwEJcBpHPS2GeIijHD"
                ],
                "start": "2025-05-27T08:51:29Z",
                "end": "2025-05-27T08:54:35Z"
            }
        },
        {
            "j0zUEJcBpHPS2GeIpzGA": {
                "id": "j0zUEJcBpHPS2GeIpzGA",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "kEzUEJcBpHPS2GeIqTGv"
                ],
                "start": "2025-05-27T08:21:01Z",
                "end": "2025-05-27T08:23:12Z"
            }
        },
        {
            "l0zmEJcBpHPS2GeIvDF_": {
                "id": "l0zmEJcBpHPS2GeIvDF_",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "mEzmEJcBpHPS2GeIvjGE"
                ],
                "start": "2025-05-27T08:40:46Z",
                "end": "2025-05-27T08:42:17Z"
            }
        },
        {
            "n0zpEJcBpHPS2GeIUjHv": {
                "id": "n0zpEJcBpHPS2GeIUjHv",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "oEzpEJcBpHPS2GeIVTEZ"
                ],
                "start": "2025-05-27T08:43:36Z",
                "end": "2025-05-27T08:45:54Z"
            }
        },
        {
            "jEyqDpcBpHPS2GeI1DGr": {
                "id": "jEyqDpcBpHPS2GeI1DGr",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask11\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "p0zrEJcBpHPS2GeIgjGx": {
                "id": "p0zrEJcBpHPS2GeIgjGx",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "qEzrEJcBpHPS2GeIhDGu"
                ],
                "start": "2025-05-27T08:45:59Z",
                "end": "2025-05-27T08:49:08Z"
            }
        },
        {
            "jUyrDpcBpHPS2GeIyzGY": {
                "id": "jUyrDpcBpHPS2GeIyzGY",
                "name": "test_local",
                "model": "workflow SingleTaskW {\n\n    START -> Task1 -> Task2 -> END;\n\n    task Task1 {\n        implementation \"TestDatasetManagementTask1\";\n    }\n\n    task Task2 {\n        implementation \"TestDatasetManagementTask2\";\n    }\n\n    define input data InputFile;\n    configure data InputFile {\n        path \"demo_datasets/titanic.json\";\n    }\n\n    define output data OutputFile;\n    configure data OutputFile {\n        path \"output/test_local/titanic_once_more.json\";\n    }\n\n    InputFile --> Task1.TestDatasetManagementTask1InputFile;\n    Task1.TestDatasetManagementTask1OutputFile --> Task2.TestDatasetManagementTask2InputFile;\n    Task2.TestDatasetManagementTask2OutputFile --> OutputFile;\n\n}\n\nworkflow SingleTaskAW from SingleTaskW {\n}\n\nexperiment TestZenohExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW {\n        strategy gridsearch;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "jkyrDpcBpHPS2GeIzzF0"
                ],
                "start": "2025-05-26T22:17:11Z",
                "end": "2025-05-26T22:17:37Z"
            }
        },
        {
            "0Ew4EZcBpHPS2GeIDjHC": {
                "id": "0Ew4EZcBpHPS2GeIDjHC",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (Implementation paths as per your updates)\n  // Each of these tasks will have its own corresponding .xxp and .py file.\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize { implementation \"UC4/pythia_pipeline_tasks/DataIngestion\"; }\n  task UC4_Task2_PreprocessAndFormTrajectory { implementation \"UC4/pythia_pipeline_tasks/PreProcessing\"; }\n  task UC4_Task3_CoreSegmentationAndDictCreation { implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\"; }\n  task UC4_Task4_SemanticEnrichmentAndModeInference { implementation \"UC4/pythia_pipeline_tasks/ModeDetection\"; }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement { implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\"; }\n  task UC4_Task6_TimelineFinalizationAndOutput { implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\"; }\n  task UC4_Task7_EvaluateAndVisualize { implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\"; }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Intermediate data passed between tasks)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF { is_optional true; }\n\n  // === Intermediate Data Items (Outputs of one task, inputs to another) ===\n\n  // Outputs from Task 1 (Extracted and processed from Input_MasterDataJSON_WF)\n  // Suffixes like _Pickle, _JSON, _Text indicate suggested file types for ProActive helper.\n  define data T1_RawLocationsDF_Pickle;\n  define data T1_UserPlacesList_JSON;\n  define data T1_MobilitiesList_JSON;\n  define data T1_GeofenceEventsStruct_JSON;\n  define data T1_MostRecentPrediction_JSON;\n  define data T1_UserID_Text;\n  define data T1_AccThresh_Value_Text;       // Value extracted by Task 1\n  define data T1_TimelineMode_Value_Text;    // Value extracted by Task 1\n\n  // Outputs from Task 2\n  define data T2_PreprocessedDF_Pickle;\n\n  // Outputs from Task 3\n  define data T3_InitialSegmentDict_Pickle;\n  define data T3_SyndetiresSummaryDF_Pickle;\n  define data T3_SyndetiresProcessedDF_Pickle;\n\n  // Outputs from Task 4\n  define data T4_EnrichedSegmentDict_Pickle;\n\n  // Outputs from Task 5\n  define data T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // Outputs from Task 6\n  define data T6_FinalFormattedTrips_Pickle;\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (TaskName.Input/OutputPortName)\n  // Ensure port names here match exactly those in individual task .xxp files.\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Intermediate Workflow Data ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         -> T1_RawLocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         -> T1_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         -> T1_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   -> T1_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   -> T1_MostRecentPrediction_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 -> T1_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  -> T1_AccThresh_Value_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  -> T1_TimelineMode_Value_Text;\n\n  // --- Intermediate Workflow Data to Task 2 Inputs ---\n  T1_RawLocationsDF_Pickle    --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  T1_AccThresh_Value_Text     --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text; // Task 2 reads the value\n  T1_UserID_Text              --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n\n  // --- Task 2 Outputs to Intermediate Workflow Data ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle -> T2_PreprocessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 3 Inputs ---\n  T2_PreprocessedDF_Pickle     --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  T1_GeofenceEventsStruct_JSON --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON; // From Task 1\n  T1_UserID_Text               --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;              // From Task 1\n\n  // --- Task 3 Outputs to Intermediate Workflow Data ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      -> T3_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   -> T3_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> T3_SyndetiresProcessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 4 Inputs ---\n  T3_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  T3_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  T3_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  T1_UserPlacesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;      // From Task 1\n  T1_MobilitiesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;      // From Task 1\n  T1_GeofenceEventsStruct_JSON    --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;// From Task 1\n  T1_UserID_Text                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;              // From Task 1\n\n  // --- Task 4 Outputs to Intermediate Workflow Data ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle -> T4_EnrichedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 5 Inputs ---\n  T4_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  T1_GeofenceEventsStruct_JSON  --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // From Task 1\n  T1_UserID_Text                --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;               // From Task 1\n\n  // --- Task 5 Outputs to Intermediate Workflow Data ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle -> T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 6 Inputs ---\n  T5_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  T1_MobilitiesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;   // From Task 1\n  T1_UserPlacesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;     // From Task 1\n  T1_UserID_Text                         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;           // From Task 1\n  T1_TimelineMode_Value_Text             --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text; // From Task 1\n\n  // --- Task 6 Outputs to Intermediate Workflow Data ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle -> T6_FinalFormattedTrips_Pickle;\n\n  // --- Intermediate Workflow Data to Task 7 Inputs ---\n  T6_FinalFormattedTrips_Pickle         --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  Input_GroundTruthDataFile_WF        --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;          // Workflow input for GT\n  T2_PreprocessedDF_Pickle            --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // From Task 2 for viz context\n  T1_UserID_Text                      --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;                    // From Task 1\n\n  // --- Workflow Outputs from Tasks ---\n  T6_FinalFormattedTrips_Pickle                -> Output_FinalSegmentTimeline_Pickle;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      -> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory -> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Dataset1/userX_master_data.json\"; // Example path to the single master JSON\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path\n    is_optional true;\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\"; // Example: use input filename\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n    // Parameter names (left of '=') must match those in individual task .xxp files.\n    // Virtual parameter names (right of '=') are experiment-space variables.\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\"); // Booleans as strings\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      // To vary acc_thresh used by preproccess_data, you'd vary t1_default_acc_thresh_vp\n      // AND ensure your master JSON doesn't contain \"AccThreshold\", or modify Task1 logic.\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp; // Assuming metro is always true or similar\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Input_TimelineModeConfig_Text for Task 6 comes from T1_TimelineMode_Value_Text.\n    // To vary this in the experiment, you'd vary t1_default_timeline_mode_vp (Task1 param)\n    // AND ensure your master JSON doesn't contain \"TimelineMode\", or modify Task1 logic.\n    // Example: If you want to directly control a param on Task 6 for its timeline function:\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation (parameters are usually for output paths, fixed here)\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example, but could define which metrics to run, etc.\n    // }\n\n    // --- Example: Parameterizing the input master data file for the workflow ---\n    // This allows running the entire pipeline on different datasets.\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // This configures the WORKFLOW INPUT, not a task parameter.\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file if it changes with the input dataset\n    // This requires a way to map input_master_json_path_vp to the correct ground truth path.\n    // A simple way if filenames correspond:\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //  path input_gt_file_path_vp;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "0Uw5EZcBpHPS2GeI7zEV": {
                "id": "0Uw5EZcBpHPS2GeI7zEV",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (Implementation paths as per your updates)\n  // Each of these tasks will have its own corresponding .xxp and .py file.\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize { implementation \"UC4/pythia_pipeline_tasks/DataIngestion\"; }\n  task UC4_Task2_PreprocessAndFormTrajectory { implementation \"UC4/pythia_pipeline_tasks/PreProcessing\"; }\n  task UC4_Task3_CoreSegmentationAndDictCreation { implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\"; }\n  task UC4_Task4_SemanticEnrichmentAndModeInference { implementation \"UC4/pythia_pipeline_tasks/ModeDetection\"; }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement { implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\"; }\n  task UC4_Task6_TimelineFinalizationAndOutput { implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\"; }\n  task UC4_Task7_EvaluateAndVisualize { implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\"; }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Intermediate data passed between tasks)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF { is_optional true; }\n\n  // === Intermediate Data Items (Outputs of one task, inputs to another) ===\n\n  // Outputs from Task 1 (Extracted and processed from Input_MasterDataJSON_WF)\n  // Suffixes like _Pickle, _JSON, _Text indicate suggested file types for ProActive helper.\n  define data T1_RawLocationsDF_Pickle;\n  define data T1_UserPlacesList_JSON;\n  define data T1_MobilitiesList_JSON;\n  define data T1_GeofenceEventsStruct_JSON;\n  define data T1_MostRecentPrediction_JSON;\n  define data T1_UserID_Text;\n  define data T1_AccThresh_Value_Text;       // Value extracted by Task 1\n  define data T1_TimelineMode_Value_Text;    // Value extracted by Task 1\n\n  // Outputs from Task 2\n  define data T2_PreprocessedDF_Pickle;\n\n  // Outputs from Task 3\n  define data T3_InitialSegmentDict_Pickle;\n  define data T3_SyndetiresSummaryDF_Pickle;\n  define data T3_SyndetiresProcessedDF_Pickle;\n\n  // Outputs from Task 4\n  define data T4_EnrichedSegmentDict_Pickle;\n\n  // Outputs from Task 5\n  define data T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // Outputs from Task 6\n  define data T6_FinalFormattedTrips_Pickle;\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (TaskName.Input/OutputPortName)\n  // Ensure port names here match exactly those in individual task .xxp files.\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Intermediate Workflow Data ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         -> T1_RawLocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         -> T1_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         -> T1_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   -> T1_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   -> T1_MostRecentPrediction_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 -> T1_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  -> T1_AccThresh_Value_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  -> T1_TimelineMode_Value_Text;\n\n  // --- Intermediate Workflow Data to Task 2 Inputs ---\n  T1_RawLocationsDF_Pickle    --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  T1_AccThresh_Value_Text     --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text; // Task 2 reads the value\n  T1_UserID_Text              --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n\n  // --- Task 2 Outputs to Intermediate Workflow Data ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle -> T2_PreprocessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 3 Inputs ---\n  T2_PreprocessedDF_Pickle     --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  T1_GeofenceEventsStruct_JSON --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON; // From Task 1\n  T1_UserID_Text               --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;              // From Task 1\n\n  // --- Task 3 Outputs to Intermediate Workflow Data ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      -> T3_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   -> T3_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> T3_SyndetiresProcessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 4 Inputs ---\n  T3_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  T3_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  T3_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  T1_UserPlacesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;      // From Task 1\n  T1_MobilitiesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;      // From Task 1\n  T1_GeofenceEventsStruct_JSON    --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;// From Task 1\n  T1_UserID_Text                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;              // From Task 1\n\n  // --- Task 4 Outputs to Intermediate Workflow Data ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle -> T4_EnrichedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 5 Inputs ---\n  T4_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  T1_GeofenceEventsStruct_JSON  --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // From Task 1\n  T1_UserID_Text                --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;               // From Task 1\n\n  // --- Task 5 Outputs to Intermediate Workflow Data ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle -> T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 6 Inputs ---\n  T5_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  T1_MobilitiesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;   // From Task 1\n  T1_UserPlacesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;     // From Task 1\n  T1_UserID_Text                         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;           // From Task 1\n  T1_TimelineMode_Value_Text             --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text; // From Task 1\n\n  // --- Task 6 Outputs to Intermediate Workflow Data ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle -> T6_FinalFormattedTrips_Pickle;\n\n  // --- Intermediate Workflow Data to Task 7 Inputs ---\n  T6_FinalFormattedTrips_Pickle         --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  Input_GroundTruthDataFile_WF        --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;          // Workflow input for GT\n  T2_PreprocessedDF_Pickle            --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // From Task 2 for viz context\n  T1_UserID_Text                      --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;                    // From Task 1\n\n  // --- Workflow Outputs from Tasks ---\n  T6_FinalFormattedTrips_Pickle                -> Output_FinalSegmentTimeline_Pickle;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      -> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory -> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path\n    is_optional true;\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\"; // Example: use input filename\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n    // Parameter names (left of '=') must match those in individual task .xxp files.\n    // Virtual parameter names (right of '=') are experiment-space variables.\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\"); // Booleans as strings\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      // To vary acc_thresh used by preproccess_data, you'd vary t1_default_acc_thresh_vp\n      // AND ensure your master JSON doesn't contain \"AccThreshold\", or modify Task1 logic.\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp; // Assuming metro is always true or similar\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Input_TimelineModeConfig_Text for Task 6 comes from T1_TimelineMode_Value_Text.\n    // To vary this in the experiment, you'd vary t1_default_timeline_mode_vp (Task1 param)\n    // AND ensure your master JSON doesn't contain \"TimelineMode\", or modify Task1 logic.\n    // Example: If you want to directly control a param on Task 6 for its timeline function:\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation (parameters are usually for output paths, fixed here)\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example, but could define which metrics to run, etc.\n    // }\n\n    // --- Example: Parameterizing the input master data file for the workflow ---\n    // This allows running the entire pipeline on different datasets.\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // This configures the WORKFLOW INPUT, not a task parameter.\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file if it changes with the input dataset\n    // This requires a way to map input_master_json_path_vp to the correct ground truth path.\n    // A simple way if filenames correspond:\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //  path input_gt_file_path_vp;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "2kx0EZcBpHPS2GeICDG2": {
                "id": "2kx0EZcBpHPS2GeICDG2",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (Implementation paths as per your updates)\n  // Each of these tasks will have its own corresponding .xxp and .py file.\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize { implementation \"UC4/pythia_pipeline_tasks/DataIngestion\"; }\n  task UC4_Task2_PreprocessAndFormTrajectory { implementation \"UC4/pythia_pipeline_tasks/PreProcessing\"; }\n  task UC4_Task3_CoreSegmentationAndDictCreation { implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\"; }\n  task UC4_Task4_SemanticEnrichmentAndModeInference { implementation \"UC4/pythia_pipeline_tasks/ModeDetection\"; }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement { implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\"; }\n  task UC4_Task6_TimelineFinalizationAndOutput { implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\"; }\n  task UC4_Task7_EvaluateAndVisualize { implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\"; }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Intermediate data passed between tasks)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  //define input data Input_GroundTruthDataFile_WF { is_optional true; }\n\n  // === Intermediate Data Items (Outputs of one task, inputs to another) ===\n\n  // Outputs from Task 1 (Extracted and processed from Input_MasterDataJSON_WF)\n  // Suffixes like _Pickle, _JSON, _Text indicate suggested file types for ProActive helper.\n  define data T1_RawLocationsDF_Pickle;\n  define data T1_UserPlacesList_JSON;\n  define data T1_MobilitiesList_JSON;\n  define data T1_GeofenceEventsStruct_JSON;\n  define data T1_MostRecentPrediction_JSON;\n  define data T1_UserID_Text;\n  define data T1_AccThresh_Value_Text;       // Value extracted by Task 1\n  define data T1_TimelineMode_Value_Text;    // Value extracted by Task 1\n\n  // Outputs from Task 2\n  define data T2_PreprocessedDF_Pickle;\n\n  // Outputs from Task 3\n  define data T3_InitialSegmentDict_Pickle;\n  define data T3_SyndetiresSummaryDF_Pickle;\n  define data T3_SyndetiresProcessedDF_Pickle;\n\n  // Outputs from Task 4\n  define data T4_EnrichedSegmentDict_Pickle;\n\n  // Outputs from Task 5\n  define data T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // Outputs from Task 6\n  define data T6_FinalFormattedTrips_Pickle;\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (TaskName.Input/OutputPortName)\n  // Ensure port names here match exactly those in individual task .xxp files.\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Intermediate Workflow Data ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         -> T1_RawLocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         -> T1_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         -> T1_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   -> T1_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   -> T1_MostRecentPrediction_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 -> T1_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  -> T1_AccThresh_Value_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  -> T1_TimelineMode_Value_Text;\n\n  // --- Intermediate Workflow Data to Task 2 Inputs ---\n  T1_RawLocationsDF_Pickle    --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  T1_AccThresh_Value_Text     --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text; // Task 2 reads the value\n  T1_UserID_Text              --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n\n  // --- Task 2 Outputs to Intermediate Workflow Data ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle -> T2_PreprocessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 3 Inputs ---\n  T2_PreprocessedDF_Pickle     --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  T1_GeofenceEventsStruct_JSON --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON; // From Task 1\n  T1_UserID_Text               --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;              // From Task 1\n\n  // --- Task 3 Outputs to Intermediate Workflow Data ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      -> T3_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   -> T3_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> T3_SyndetiresProcessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 4 Inputs ---\n  T3_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  T3_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  T3_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  T1_UserPlacesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;      // From Task 1\n  T1_MobilitiesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;      // From Task 1\n  T1_GeofenceEventsStruct_JSON    --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;// From Task 1\n  T1_UserID_Text                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;              // From Task 1\n\n  // --- Task 4 Outputs to Intermediate Workflow Data ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle -> T4_EnrichedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 5 Inputs ---\n  T4_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  T1_GeofenceEventsStruct_JSON  --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // From Task 1\n  T1_UserID_Text                --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;               // From Task 1\n\n  // --- Task 5 Outputs to Intermediate Workflow Data ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle -> T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 6 Inputs ---\n  T5_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  T1_MobilitiesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;   // From Task 1\n  T1_UserPlacesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;     // From Task 1\n  T1_UserID_Text                         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;           // From Task 1\n  T1_TimelineMode_Value_Text             --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text; // From Task 1\n\n  // --- Task 6 Outputs to Intermediate Workflow Data ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle -> T6_FinalFormattedTrips_Pickle;\n\n  // --- Intermediate Workflow Data to Task 7 Inputs ---\n  T6_FinalFormattedTrips_Pickle         --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  Input_GroundTruthDataFile_WF        --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;          // Workflow input for GT\n  T2_PreprocessedDF_Pickle            --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // From Task 2 for viz context\n  T1_UserID_Text                      --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;                    // From Task 1\n\n  // --- Workflow Outputs from Tasks ---\n  T6_FinalFormattedTrips_Pickle                -> Output_FinalSegmentTimeline_Pickle;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      -> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory -> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    // Path to your test JSON file, relative to where the engine expects data\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path\n    is_optional true;\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\"; // Example: use input filename\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n    // Parameter names (left of '=') must match those in individual task .xxp files.\n    // Virtual parameter names (right of '=') are experiment-space variables.\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\"); // Booleans as strings\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      // To vary acc_thresh used by preproccess_data, you'd vary t1_default_acc_thresh_vp\n      // AND ensure your master JSON doesn't contain \"AccThreshold\", or modify Task1 logic.\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp; // Assuming metro is always true or similar\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Input_TimelineModeConfig_Text for Task 6 comes from T1_TimelineMode_Value_Text.\n    // To vary this in the experiment, you'd vary t1_default_timeline_mode_vp (Task1 param)\n    // AND ensure your master JSON doesn't contain \"TimelineMode\", or modify Task1 logic.\n    // Example: If you want to directly control a param on Task 6 for its timeline function:\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation (parameters are usually for output paths, fixed here)\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example, but could define which metrics to run, etc.\n    // }\n\n    // --- Example: Parameterizing the input master data file for the workflow ---\n    // This allows running the entire pipeline on different datasets.\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // This configures the WORKFLOW INPUT, not a task parameter.\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file if it changes with the input dataset\n    // This requires a way to map input_master_json_path_vp to the correct ground truth path.\n    // A simple way if filenames correspond:\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //  path input_gt_file_path_vp;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "zkwwEZcBpHPS2GeIgzFa": {
                "id": "zkwwEZcBpHPS2GeIgzFa",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style)\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow - Linear for now, can be adapted for parallel tasks if applicable)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize // Evaluation might be external or a final step\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n\n  task UC4_Task1_IngestAndContextualize {\n    // Corresponds to Task 1: Data Ingestion & Contextualization\n    // Implementation would wrap functions like restructure_mobility_options,\n    // restructure_geofence_events, process_recent_predictions\n    implementation \"UC4/pythia_pipeline_tasks/Task1_IngestAndContextualize\";\n  }\n\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    // Corresponds to Task 2: Preprocessing & Initial Trajectory Formation\n    // Implementation would wrap preproccess_data and optionally apply_kalman_filter\n    implementation \"UC4/pythia_pipeline_tasks/Task2_PreprocessAndFormTrajectory\";\n    // Parameters for this task could be defined here or in the experiment\n    // param acc_thresh = 50; // default\n    // param enable_kalman_filter = false; // default\n  }\n\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    // Corresponds to Task 3: Core Segmentation & Initial Dictionary Creation\n    // Implementation wraps hybrid_segmentation, convert_segmentation_to_pythia_format,\n    // fix_false_positives, syndetiras_v2_1, apply_wandering_to_dictionary\n    implementation \"UC4/pythia_pipeline_tasks/Task3_CoreSegmentationAndDictCreation\";\n    // param hybrid_speed_threshold = 1.0;\n    // param hybrid_min_stationary_time = 60;\n  }\n\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    // Corresponds to Task 4: Semantic Enrichment & Mode Inference\n    // Wraps mode detection, GTFS, Overpass, geofence application, place correction\n    implementation \"UC4/pythia_pipeline_tasks/Task4_SemanticEnrichmentAndModeInference\";\n    // param use_improved_mode_detection = true;\n    // param enable_gtfs = true;\n    // param enable_overpass_api = false; // Often disabled for performance/rate limits\n  }\n\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    // Corresponds to Task 5: Timeline Coherence & Structural Refinement\n    // Wraps cold start fix, merging rules, missing trip insertion, etc.\n    implementation \"UC4/pythia_pipeline_tasks/Task5_TimelineCoherenceAndStructuralRefinement\";\n    // This task has many internal parameters that could be exposed.\n    // param cold_start_distance_thresh_m = 100.0;\n    // param missing_trip_max_gap_s = 1800;\n  }\n\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    // Corresponds to Task 6: Timeline Finalization & Output Generation\n    // Wraps mobility constraints, distance calculation, overlap resolution, timeline completion, formatting\n    implementation \"UC4/pythia_pipeline_tasks/Task6_TimelineFinalizationAndOutput\";\n    // param skip_day_boundaries_config = \"open\"; // false, true, \"open\"\n  }\n\n  task UC4_Task7_EvaluateAndVisualize {\n    // Corresponds to Task 7: Evaluation & Visualization\n    // Wraps comparison against ground truth, metric calculation, visualize_segments\n    implementation \"UC4/pythia_pipeline_tasks/Task7_EvaluateAndVisualize\";\n    // param ground_truth_data_path = null; // Path to ground truth if available\n    // param visualization_output_path = \"UC4/PythiaPipeline/visualizations/\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW\n  // ------------------------------------------------------------------------------------------------\n\n  // Inputs to Task 1\n  define input data RawLocations;\n  define input data UserPlaces;\n  define input data UserGeofences;\n  define input data UserMobilityPreferences;\n  define input data PreviousPredictions; // Optional, for context\n\n  // Outputs from Task 1 / Inputs to Task 2\n  define data Task1_ContextualizedData; // Could be a bundle: locations_df, mobilities_list, geofence_events, most_recent_prediction, places_list, uuId\n  define data Task1_RawLocationsDF; // Specifically the DataFrame part for Task 2\n\n  // Outputs from Task 2 / Inputs to Task 3\n  define data Task2_PreprocessedDF;\n\n  // Outputs from Task 3 / Inputs to Task 4\n  define data Task3_InitialSegmentDict;       // dic_segments\n  define data Task3_SyndetiresSummaryDF;    // trips_df_syndetires_output\n  define data Task3_SyndetiresProcessedDF;  // df_syndetires_output\n  define data Task3_ContextualizedData_Passthrough; // Pass through from Task 1\n\n  // Outputs from Task 4 / Inputs to Task 5\n  define data Task4_EnrichedSegmentDict;\n  define data Task4_ContextualizedData_Passthrough; // Pass through\n\n  // Outputs from Task 5 / Inputs to Task 6\n  define data Task5_RefinedSegmentDict;\n  define data Task5_ContextualizedData_Passthrough; // Pass through\n\n  // Outputs from Task 6 / Inputs to Task 7\n  define data Task6_FinalFormattedTrips; // final_output_trips\n\n  // Final Workflow Outputs\n  define output data FinalSegmentTimeline; // Same as Task6_FinalFormattedTrips for now\n  define output data EvaluationReport;     // From Task 7\n  define output data VisualizationFiles;   // From Task 7 (e.g., path to saved images)\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Inputs to Task 1 ---\n  RawLocations            --> UC4_Task1_IngestAndContextualize.Input_RawLocations;\n  UserPlaces              --> UC4_Task1_IngestAndContextualize.Input_UserPlaces;\n  UserGeofences           --> UC4_Task1_IngestAndContextualize.Input_UserGeofences;\n  UserMobilityPreferences --> UC4_Task1_IngestAndContextualize.Input_UserMobilityPreferences;\n  PreviousPredictions     --> UC4_Task1_IngestAndContextualize.Input_PreviousPredictions;\n\n  // --- Task 1 to Task 2 ---\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle      --> Task2_PreprocessedDF.Input_ContextBundle; // For acc_thresh, uuId\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF     --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF;\n\n  // --- Task 2 to Task 3 ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle       --> UC4_Task3_CoreSegmentationAndDictCreation.Input_ContextBundle; // For geofence_events in wandering detection\n\n  // --- Task 3 to Task 4 ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SegmentDict;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_ContextBundle; // places, mobilities_list, geofence_events, uuId\n\n  // --- Task 4 to Task 5 ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_SegmentDict;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_ContextBundle; // geofence_events, uuId\n\n  // --- Task 5 to Task 6 ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_RefinedSegmentDict --> UC4_Task6_TimelineFinalizationAndOutput.Input_SegmentDict;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle                      --> UC4_Task6_TimelineFinalizationAndOutput.Input_ContextBundle; // mobilities_list, places, uuId\n\n  // --- Task 6 to Task 7 ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips --> UC4_Task7_EvaluateAndVisualize.Input_SegmentTimeline;\n  // UC4_Task7_EvaluateAndVisualize might also need GroundTruthData as a direct input\n\n  // --- Last Task Outputs to Workflow Outputs ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips --> FinalSegmentTimeline;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport           --> EvaluationReport;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles         --> VisualizationFiles;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Example paths)\n  // ------------------------------------------------------------------------------------------------\n  configure data RawLocations {\n    path \"UC4/RawTraces/Dataset1/userX_locations.json\"; // Example\n  }\n  configure data UserPlaces {\n    path \"UC4/ContextData/Dataset1/userX_places.json\"; // Example\n  }\n  configure data UserGeofences {\n    path \"UC4/ContextData/Dataset1/userX_geofences.json\"; // Example\n  }\n  configure data UserMobilityPreferences {\n    path \"UC4/ContextData/Dataset1/userX_mobilities.json\"; // Example\n  }\n  configure data PreviousPredictions {\n    // path \"UC4/ContextData/Dataset1/userX_previous_predictions.json\"; // Optional\n    is_optional true; // Mark as optional if not always available\n  }\n\n  configure data FinalSegmentTimeline {\n    path \"UC4/PythiaPipeline/results/userX_final_timeline.json\";\n  }\n  configure data EvaluationReport {\n    path \"UC4/PythiaPipeline/results/userX_evaluation_report.txt\";\n  }\n  configure data VisualizationFiles {\n    path \"UC4/PythiaPipeline/results/userX_visualizations/\"; // Directory for images\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, bayesian_optimization etc.\n    // runs = N; // Define number of runs for stochastic strategies\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 2: Preprocessing\n    param acc_thresh_vp = enum(30, 50, 75); // enum for discrete values\n    param enable_kalman_vp = enum(true, false);\n    // param kalman_process_noise_vp = range(0.001, 0.05, step=0.005); // Example range\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param acc_thresh = acc_thresh_vp;\n      param enable_kalman_filter = enable_kalman_vp;\n      // param kalman_params.process_noise = kalman_process_noise_vp; // Assuming kalman_params is a dict\n    }\n\n    // For Task 3: Core Segmentation\n    param hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param hybrid_min_stationary_vp = enum(45, 60, 90);\n    // param wander_score_thresh_vp = range(40.0, 60.0, step=5.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_segmentation_params.speed_threshold = hybrid_speed_thresh_vp;\n      param hybrid_segmentation_params.min_stationary_time = hybrid_min_stationary_vp;\n      // param gps_error_detection_params.wandering_score_threshold = wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param use_improved_mode_vp = enum(true, false);\n    param geofence_time_window_vp = enum(10, 20, 30); // minutes\n    param geofence_max_dist_vp = enum(100, 150, 200); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = use_improved_mode_vp;\n      param apply_geofences_params.time_window_minutes = geofence_time_window_vp;\n      param apply_geofences_params.max_distance_meters = geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    // Example: Experiment with the order of two specific refinement steps\n    // This is more complex and might require different workflow definitions or a DSL that supports conditional task execution order.\n    // For a simpler approach, you might vary parameters of individual functions:\n    param missing_trip_gap_s_vp = enum(1200, 1800, 2400); // 20, 30, 40 minutes\n    param stationary_merge_gap_s_vp = enum(60, 120, 180);\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param detect_and_insert_missing_trips_params.max_gap_seconds = missing_trip_gap_s_vp;\n      param merge_continuous_stationary_segments_params.max_gap_seconds = stationary_merge_gap_s_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    param skip_day_boundaries_vp = enum(\"open\", \"true_skip\", \"false_full\"); // Representing \"open\", True, False\n\n    task UC4_Task6_TimelineFinalizationAndOutput {\n      param skip_day_boundaries_config = skip_day_boundaries_vp;\n    }\n\n    // --- Evaluation Configuration (Task 7) ---\n    // param ground_truth_vp = \"path/to/ground_truth_for_userX.json\";\n    task UC4_Task7_EvaluateAndVisualize {\n      // param ground_truth_data_path = ground_truth_vp; // If ground truth varies per run\n    }\n\n    // Potentially define different input datasets for the experiment\n    // This can be done by overriding the 'path' in 'configure data RawLocations'\n    // space_param input_dataset_path_vp = enum(\n    //   \"UC4/RawTraces/Dataset1/userX_locations.json\",\n    //   \"UC4/RawTraces/Dataset2/userY_locations.json\"\n    // );\n    // configure data RawLocations {\n    //   path input_dataset_path_vp;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "z0wzEZcBpHPS2GeIKjEU": {
                "id": "z0wzEZcBpHPS2GeIKjEU",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style)\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow - Linear for now, can be adapted for parallel tasks if applicable)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize // Evaluation might be external or a final step\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n\n  task UC4_Task1_IngestAndContextualize {\n    // Corresponds to Task 1: Data Ingestion & Contextualization\n    // Implementation would wrap functions like restructure_mobility_options,\n    // restructure_geofence_events, process_recent_predictions\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    // Corresponds to Task 2: Preprocessing & Initial Trajectory Formation\n    // Implementation would wrap preproccess_data and optionally apply_kalman_filter\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n    // Parameters for this task could be defined here or in the experiment\n    // param acc_thresh = 50; // default\n    // param enable_kalman_filter = false; // default\n  }\n\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    // Corresponds to Task 3: Core Segmentation & Initial Dictionary Creation\n    // Implementation wraps hybrid_segmentation, convert_segmentation_to_pythia_format,\n    // fix_false_positives, syndetiras_v2_1, apply_wandering_to_dictionary\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n    // param hybrid_speed_threshold = 1.0;\n    // param hybrid_min_stationary_time = 60;\n  }\n\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    // Corresponds to Task 4: Semantic Enrichment & Mode Inference\n    // Wraps mode detection, GTFS, Overpass, geofence application, place correction\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n    // param use_improved_mode_detection = true;\n    // param enable_gtfs = true;\n    // param enable_overpass_api = false; // Often disabled for performance/rate limits\n  }\n\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    // Corresponds to Task 5: Timeline Coherence & Structural Refinement\n    // Wraps cold start fix, merging rules, missing trip insertion, etc.\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n    // This task has many internal parameters that could be exposed.\n    // param cold_start_distance_thresh_m = 100.0;\n    // param missing_trip_max_gap_s = 1800;\n  }\n\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    // Corresponds to Task 6: Timeline Finalization & Output Generation\n    // Wraps mobility constraints, distance calculation, overlap resolution, timeline completion, formatting\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n    // param skip_day_boundaries_config = \"open\"; // false, true, \"open\"\n  }\n\n  task UC4_Task7_EvaluateAndVisualize {\n    // Corresponds to Task 7: Evaluation & Visualization\n    // Wraps comparison against ground truth, metric calculation, visualize_segments\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n    // param ground_truth_data_path = null; // Path to ground truth if available\n    // param visualization_output_path = \"UC4/PythiaPipeline/visualizations/\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW\n  // ------------------------------------------------------------------------------------------------\n\n  // Inputs to Task 1\n  define input data RawLocations;\n  define input data UserPlaces;\n  define input data UserGeofences;\n  define input data UserMobilityPreferences;\n  define input data PreviousPredictions; // Optional, for context\n\n  // Outputs from Task 1 / Inputs to Task 2\n  define data Task1_ContextualizedData; // Could be a bundle: locations_df, mobilities_list, geofence_events, most_recent_prediction, places_list, uuId\n  define data Task1_RawLocationsDF; // Specifically the DataFrame part for Task 2\n\n  // Outputs from Task 2 / Inputs to Task 3\n  define data Task2_PreprocessedDF;\n\n  // Outputs from Task 3 / Inputs to Task 4\n  define data Task3_InitialSegmentDict;       // dic_segments\n  define data Task3_SyndetiresSummaryDF;    // trips_df_syndetires_output\n  define data Task3_SyndetiresProcessedDF;  // df_syndetires_output\n  define data Task3_ContextualizedData_Passthrough; // Pass through from Task 1\n\n  // Outputs from Task 4 / Inputs to Task 5\n  define data Task4_EnrichedSegmentDict;\n  define data Task4_ContextualizedData_Passthrough; // Pass through\n\n  // Outputs from Task 5 / Inputs to Task 6\n  define data Task5_RefinedSegmentDict;\n  define data Task5_ContextualizedData_Passthrough; // Pass through\n\n  // Outputs from Task 6 / Inputs to Task 7\n  define data Task6_FinalFormattedTrips; // final_output_trips\n\n  // Final Workflow Outputs\n  define output data FinalSegmentTimeline; // Same as Task6_FinalFormattedTrips for now\n  define output data EvaluationReport;     // From Task 7\n  define output data VisualizationFiles;   // From Task 7 (e.g., path to saved images)\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Inputs to Task 1 ---\n  RawLocations            --> UC4_Task1_IngestAndContextualize.Input_RawLocations;\n  UserPlaces              --> UC4_Task1_IngestAndContextualize.Input_UserPlaces;\n  UserGeofences           --> UC4_Task1_IngestAndContextualize.Input_UserGeofences;\n  UserMobilityPreferences --> UC4_Task1_IngestAndContextualize.Input_UserMobilityPreferences;\n  PreviousPredictions     --> UC4_Task1_IngestAndContextualize.Input_PreviousPredictions;\n\n  // --- Task 1 to Task 2 ---\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle      --> Task2_PreprocessedDF.Input_ContextBundle; // For acc_thresh, uuId\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF     --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF;\n\n  // --- Task 2 to Task 3 ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle       --> UC4_Task3_CoreSegmentationAndDictCreation.Input_ContextBundle; // For geofence_events in wandering detection\n\n  // --- Task 3 to Task 4 ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SegmentDict;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_ContextBundle; // places, mobilities_list, geofence_events, uuId\n\n  // --- Task 4 to Task 5 ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_SegmentDict;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_ContextBundle; // geofence_events, uuId\n\n  // --- Task 5 to Task 6 ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_RefinedSegmentDict --> UC4_Task6_TimelineFinalizationAndOutput.Input_SegmentDict;\n  UC4_Task1_IngestAndContextualize.Output_ContextBundle                      --> UC4_Task6_TimelineFinalizationAndOutput.Input_ContextBundle; // mobilities_list, places, uuId\n\n  // --- Task 6 to Task 7 ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips --> UC4_Task7_EvaluateAndVisualize.Input_SegmentTimeline;\n  // UC4_Task7_EvaluateAndVisualize might also need GroundTruthData as a direct input\n\n  // --- Last Task Outputs to Workflow Outputs ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips --> FinalSegmentTimeline;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport           --> EvaluationReport;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles         --> VisualizationFiles;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Example paths)\n  // ------------------------------------------------------------------------------------------------\n  configure data RawLocations {\n    path \"UC4/RawTraces/Dataset1/userX_locations.json\"; // Example\n  }\n  configure data UserPlaces {\n    path \"UC4/ContextData/Dataset1/userX_places.json\"; // Example\n  }\n  configure data UserGeofences {\n    path \"UC4/ContextData/Dataset1/userX_geofences.json\"; // Example\n  }\n  configure data UserMobilityPreferences {\n    path \"UC4/ContextData/Dataset1/userX_mobilities.json\"; // Example\n  }\n  configure data PreviousPredictions {\n    // path \"UC4/ContextData/Dataset1/userX_previous_predictions.json\"; // Optional\n    is_optional true; // Mark as optional if not always available\n  }\n\n  configure data FinalSegmentTimeline {\n    path \"UC4/PythiaPipeline/results/userX_final_timeline.json\";\n  }\n  configure data EvaluationReport {\n    path \"UC4/PythiaPipeline/results/userX_evaluation_report.txt\";\n  }\n  configure data VisualizationFiles {\n    path \"UC4/PythiaPipeline/results/userX_visualizations/\"; // Directory for images\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch; // Or randomsearch, bayesian_optimization etc.\n    // runs = N; // Define number of runs for stochastic strategies\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 2: Preprocessing\n    param acc_thresh_vp = enum(30, 50, 75); // enum for discrete values\n    param enable_kalman_vp = enum(true, false);\n    // param kalman_process_noise_vp = range(0.001, 0.05, step=0.005); // Example range\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param acc_thresh = acc_thresh_vp;\n      param enable_kalman_filter = enable_kalman_vp;\n      // param kalman_params.process_noise = kalman_process_noise_vp; // Assuming kalman_params is a dict\n    }\n\n    // For Task 3: Core Segmentation\n    param hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param hybrid_min_stationary_vp = enum(45, 60, 90);\n    // param wander_score_thresh_vp = range(40.0, 60.0, step=5.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_segmentation_params.speed_threshold = hybrid_speed_thresh_vp;\n      param hybrid_segmentation_params.min_stationary_time = hybrid_min_stationary_vp;\n      // param gps_error_detection_params.wandering_score_threshold = wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param use_improved_mode_vp = enum(true, false);\n    param geofence_time_window_vp = enum(10, 20, 30); // minutes\n    param geofence_max_dist_vp = enum(100, 150, 200); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = use_improved_mode_vp;\n      param apply_geofences_params.time_window_minutes = geofence_time_window_vp;\n      param apply_geofences_params.max_distance_meters = geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    // Example: Experiment with the order of two specific refinement steps\n    // This is more complex and might require different workflow definitions or a DSL that supports conditional task execution order.\n    // For a simpler approach, you might vary parameters of individual functions:\n    param missing_trip_gap_s_vp = enum(1200, 1800, 2400); // 20, 30, 40 minutes\n    param stationary_merge_gap_s_vp = enum(60, 120, 180);\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param detect_and_insert_missing_trips_params.max_gap_seconds = missing_trip_gap_s_vp;\n      param merge_continuous_stationary_segments_params.max_gap_seconds = stationary_merge_gap_s_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    param skip_day_boundaries_vp = enum(\"open\", \"true_skip\", \"false_full\"); // Representing \"open\", True, False\n\n    task UC4_Task6_TimelineFinalizationAndOutput {\n      param skip_day_boundaries_config = skip_day_boundaries_vp;\n    }\n\n    // --- Evaluation Configuration (Task 7) ---\n    // param ground_truth_vp = \"path/to/ground_truth_for_userX.json\";\n    task UC4_Task7_EvaluateAndVisualize {\n      // param ground_truth_data_path = ground_truth_vp; // If ground truth varies per run\n    }\n\n    // Potentially define different input datasets for the experiment\n    // This can be done by overriding the 'path' in 'configure data RawLocations'\n    // space_param input_dataset_path_vp = enum(\n    //   \"UC4/RawTraces/Dataset1/userX_locations.json\",\n    //   \"UC4/RawTraces/Dataset2/userY_locations.json\"\n    // );\n    // configure data RawLocations {\n    //   path input_dataset_path_vp;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "3Ex3EZcBpHPS2GeIETH4": {
                "id": "3Ex3EZcBpHPS2GeIETH4",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Intermediate Data Items ===\n  define data T1_RawLocationsDF_Pickle;\n  define data T1_UserPlacesList_JSON;\n  define data T1_MobilitiesList_JSON;\n  define data T1_GeofenceEventsStruct_JSON;\n  define data T1_MostRecentPrediction_JSON;\n  define data T1_UserID_Text;\n  define data T1_AccThresh_Value_Text;\n  define data T1_TimelineMode_Value_Text;\n\n  define data T2_PreprocessedDF_Pickle;\n\n  define data T3_InitialSegmentDict_Pickle;\n  define data T3_SyndetiresSummaryDF_Pickle;\n  define data T3_SyndetiresProcessedDF_Pickle;\n\n  define data T4_EnrichedSegmentDict_Pickle;\n\n  define data T5_StructurallyRefinedSegmentDict_Pickle;\n\n  define data T6_FinalFormattedTrips_Pickle;\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Intermediate Workflow Data ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> T1_RawLocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> T1_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> T1_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> T1_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> T1_MostRecentPrediction_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> T1_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> T1_AccThresh_Value_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> T1_TimelineMode_Value_Text;\n\n  // --- Intermediate Workflow Data to Task 2 Inputs ---\n  T1_RawLocationsDF_Pickle    --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  T1_AccThresh_Value_Text     --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  T1_UserID_Text              --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n\n  // --- Task 2 Outputs to Intermediate Workflow Data ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> T2_PreprocessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 3 Inputs ---\n  T2_PreprocessedDF_Pickle     --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  T1_GeofenceEventsStruct_JSON --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text               --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;\n\n  // --- Task 3 Outputs to Intermediate Workflow Data ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> T3_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> T3_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> T3_SyndetiresProcessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 4 Inputs ---\n  T3_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  T3_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  T3_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  T1_UserPlacesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  T1_MobilitiesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  T1_GeofenceEventsStruct_JSON    --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;\n\n  // --- Task 4 Outputs to Intermediate Workflow Data ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> T4_EnrichedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 5 Inputs ---\n  T4_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  T1_GeofenceEventsStruct_JSON  --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;\n\n  // --- Task 5 Outputs to Intermediate Workflow Data ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 6 Inputs ---\n  T5_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  T1_MobilitiesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;\n  T1_UserPlacesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;\n  T1_UserID_Text                         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;\n  T1_TimelineMode_Value_Text             --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 6 Outputs to Intermediate Workflow Data (and subsequently to Workflow Output) ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> T6_FinalFormattedTrips_Pickle;\n\n  // --- Intermediate Workflow Data to Task 7 Inputs ---\n  T6_FinalFormattedTrips_Pickle         --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  Input_GroundTruthDataFile_WF        --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n  T2_PreprocessedDF_Pickle            --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle;\n  T1_UserID_Text                      --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;\n\n  // --- Workflow Outputs from Tasks/Intermediate Data ---\n  T6_FinalFormattedTrips_Pickle                              --> Output_FinalSegmentTimeline_Pickle;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path for non-parameterized run\n    is_optional true; // This data input might not always be provided\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Parameterizing workflow inputs ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp;\n    //   is_optional true; // Still optional even when parameterized\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "ukz0EJcBpHPS2GeI4zGF": {
                "id": "ukz0EJcBpHPS2GeI4zGF",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "u0z0EJcBpHPS2GeI5TGB"
                ],
                "start": "2025-05-27T08:56:14Z",
                "end": "2025-05-27T08:59:24Z"
            }
        },
        {
            "xkz5EJcBpHPS2GeIjDHA": {
                "id": "xkz5EJcBpHPS2GeIjDHA",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.trained_model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "x0z5EJcBpHPS2GeIjjHi"
                ],
                "start": "2025-05-27T09:01:19Z",
                "end": "2025-05-27T09:04:22Z"
            }
        },
        {
            "0kxqEZcBpHPS2GeI8zHF": {
                "id": "0kxqEZcBpHPS2GeI8zHF",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.trained_model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "00xqEZcBpHPS2GeI9THX"
                ],
                "start": "2025-05-27T11:05:11Z",
                "end": "2025-05-27T11:08:22Z"
            }
        },
        {
            "wkz1EJcBpHPS2GeIZTGF": {
                "id": "wkz1EJcBpHPS2GeIZTGF",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOutput\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "w0z1EJcBpHPS2GeIZjGi"
                ],
                "start": "2025-05-27T08:56:47Z",
                "end": "2025-05-27T08:56:55Z"
            }
        },
        {
            "xUz2EJcBpHPS2GeI_zF-": {
                "id": "xUz2EJcBpHPS2GeI_zF-",
                "name": "demo_oslo",
                "model": "workflow SingleTaskW {\n\n    task Task1 {\n        implementation \"demo_tasks/TestOut\";\n    }\n\n    START -> Task1 -> END;\n\n    define input data InputFile;\n    InputFile --> Task1.InputFile;\n    configure data InputFile {\n        path \"demo_datasets/demo_data.txt\";\n    }\n\n    define output data OutputFile;\n    Task1.OutputFile --> OutputFile;\n    configure data OutputFile {\n     path \"output/trained_model/**\";\n    }\n\n}\n\nworkflow SingleTaskAW2 from SingleTaskW {\n}\n\nexperiment TestKostasExp {\n\n    control {\n          START -> S1 -> END;\n    }\n\n    space S1 of SingleTaskAW2 {\n        strategy randomsearch;\n        runs = 1;\n    }\n\n}\n",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "-UyNEZcBpHPS2GeIAjEg": {
                "id": "-UyNEZcBpHPS2GeIAjEg",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path for non-parameterized run\n  //  is_optional true; // This data input might not always be provided\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Parameterizing workflow inputs ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp;\n    //   is_optional true; // Still optional even when parameterized\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "-kyNEZcBpHPS2GeIfzHa": {
                "id": "-kyNEZcBpHPS2GeIfzHa",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path for non-parameterized run\n  //  is_optional true; // This data input might not always be provided\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  //intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Parameterizing workflow inputs ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp;\n    //   is_optional true; // Still optional even when parameterized\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "50x9EZcBpHPS2GeI-zHI": {
                "id": "50x9EZcBpHPS2GeI-zHI",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Intermediate Data Items ===\n   // === Intermediate Data Items ===\n  define data T1_RawLocationsDF_Pickle[];\n  define data T1_UserPlacesList_JSON[];\n  define data T1_MobilitiesList_JSON[];\n  define data T1_GeofenceEventsStruct_JSON[];\n  define data T1_MostRecentPrediction_JSON[];\n  define data T1_UserID_Text[];\n  define data T1_AccThresh_Value_Text[];\n  define data T1_TimelineMode_Value_Text[];\n\n  define data T2_PreprocessedDF_Pickle[];\n\n  define data T3_InitialSegmentDict_Pickle[];\n  define data T3_SyndetiresSummaryDF_Pickle[];\n  define data T3_SyndetiresProcessedDF_Pickle[];\n\n  define data T4_EnrichedSegmentDict_Pickle[];\n\n  define data T5_StructurallyRefinedSegmentDict_Pickle[];\n\n  define data T6_FinalFormattedTrips_Pickle[];\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Intermediate Workflow Data ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> T1_RawLocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> T1_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> T1_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> T1_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> T1_MostRecentPrediction_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> T1_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> T1_AccThresh_Value_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> T1_TimelineMode_Value_Text;\n\n  // --- Intermediate Workflow Data to Task 2 Inputs ---\n  T1_RawLocationsDF_Pickle    --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  T1_AccThresh_Value_Text     --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  T1_UserID_Text              --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n\n  // --- Task 2 Outputs to Intermediate Workflow Data ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> T2_PreprocessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 3 Inputs ---\n  T2_PreprocessedDF_Pickle     --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  T1_GeofenceEventsStruct_JSON --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text               --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;\n\n  // --- Task 3 Outputs to Intermediate Workflow Data ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> T3_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> T3_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> T3_SyndetiresProcessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 4 Inputs ---\n  T3_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  T3_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  T3_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  T1_UserPlacesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  T1_MobilitiesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  T1_GeofenceEventsStruct_JSON    --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;\n\n  // --- Task 4 Outputs to Intermediate Workflow Data ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> T4_EnrichedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 5 Inputs ---\n  T4_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  T1_GeofenceEventsStruct_JSON  --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;\n\n  // --- Task 5 Outputs to Intermediate Workflow Data ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 6 Inputs ---\n  T5_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  T1_MobilitiesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;\n  T1_UserPlacesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;\n  T1_UserID_Text                         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;\n  T1_TimelineMode_Value_Text             --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 6 Outputs to Intermediate Workflow Data (and subsequently to Workflow Output) ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> T6_FinalFormattedTrips_Pickle;\n\n  // --- Intermediate Workflow Data to Task 7 Inputs ---\n  T6_FinalFormattedTrips_Pickle         --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  Input_GroundTruthDataFile_WF        --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n  T2_PreprocessedDF_Pickle            --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle;\n  T1_UserID_Text                      --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;\n\n  // --- Workflow Outputs from Tasks/Intermediate Data ---\n  T6_FinalFormattedTrips_Pickle                              --> Output_FinalSegmentTimeline_Pickle;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path for non-parameterized run\n    is_optional true; // This data input might not always be provided\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Parameterizing workflow inputs ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp;\n    //   is_optional true; // Still optional even when parameterized\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "5kx9EZcBpHPS2GeISjGj": {
                "id": "5kx9EZcBpHPS2GeISjGj",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Intermediate Workflow Data ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> T1_RawLocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> T1_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> T1_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> T1_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> T1_MostRecentPrediction_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> T1_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> T1_AccThresh_Value_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> T1_TimelineMode_Value_Text;\n\n  // --- Intermediate Workflow Data to Task 2 Inputs ---\n  T1_RawLocationsDF_Pickle    --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  T1_AccThresh_Value_Text     --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  T1_UserID_Text              --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n\n  // --- Task 2 Outputs to Intermediate Workflow Data ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> T2_PreprocessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 3 Inputs ---\n  T2_PreprocessedDF_Pickle     --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  T1_GeofenceEventsStruct_JSON --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text               --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;\n\n  // --- Task 3 Outputs to Intermediate Workflow Data ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> T3_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> T3_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> T3_SyndetiresProcessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 4 Inputs ---\n  T3_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  T3_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  T3_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  T1_UserPlacesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  T1_MobilitiesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  T1_GeofenceEventsStruct_JSON    --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;\n\n  // --- Task 4 Outputs to Intermediate Workflow Data ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> T4_EnrichedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 5 Inputs ---\n  T4_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  T1_GeofenceEventsStruct_JSON  --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;\n\n  // --- Task 5 Outputs to Intermediate Workflow Data ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 6 Inputs ---\n  T5_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  T1_MobilitiesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;\n  T1_UserPlacesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;\n  T1_UserID_Text                         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;\n  T1_TimelineMode_Value_Text             --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 6 Outputs to Intermediate Workflow Data (and subsequently to Workflow Output) ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> T6_FinalFormattedTrips_Pickle;\n\n  // --- Intermediate Workflow Data to Task 7 Inputs ---\n  T6_FinalFormattedTrips_Pickle         --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  Input_GroundTruthDataFile_WF        --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n  T2_PreprocessedDF_Pickle            --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle;\n  T1_UserID_Text                      --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;\n\n  // --- Workflow Outputs from Tasks/Intermediate Data ---\n  T6_FinalFormattedTrips_Pickle                              --> Output_FinalSegmentTimeline_Pickle;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path for non-parameterized run\n    is_optional true; // This data input might not always be provided\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Parameterizing workflow inputs ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp;\n    //   is_optional true; // Still optional even when parameterized\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "-0yUEZcBpHPS2GeIpjHF": {
                "id": "-0yUEZcBpHPS2GeIpjHF",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF { // CORRECTED\n    path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Provide a placeholder path\n    is_optional true; // This data input might not always be provided\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\"; // UNCOMMENTED INTENT\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Group ALL space-level virtual parameters here ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Parameterizing workflow inputs (space_param) ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n\n    // --- Now, define task-specific parameter mappings ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "5Ux8EZcBpHPS2GeITTGY": {
                "id": "5Ux8EZcBpHPS2GeITTGY",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Intermediate Data Items ===\n  define data T1_RawLocationsDF_Pickle;\n  define data T1_UserPlacesList_JSON;\n  define data T1_MobilitiesList_JSON;\n  define data T1_GeofenceEventsStruct_JSON;\n  define data T1_MostRecentPrediction_JSON;\n  define data T1_UserID_Text;\n  define data T1_AccThresh_Value_Text;\n  define data T1_TimelineMode_Value_Text;\n\n  define data T2_PreprocessedDF_Pickle;\n\n  define data T3_InitialSegmentDict_Pickle;\n  define data T3_SyndetiresSummaryDF_Pickle;\n  define data T3_SyndetiresProcessedDF_Pickle;\n\n  define data T4_EnrichedSegmentDict_Pickle;\n\n  define data T5_StructurallyRefinedSegmentDict_Pickle;\n\n  define data T6_FinalFormattedTrips_Pickle;\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Intermediate Workflow Data ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> T1_RawLocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> T1_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> T1_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> T1_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> T1_MostRecentPrediction_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> T1_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> T1_AccThresh_Value_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> T1_TimelineMode_Value_Text;\n\n  // --- Intermediate Workflow Data to Task 2 Inputs ---\n  T1_RawLocationsDF_Pickle    --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  T1_AccThresh_Value_Text     --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  T1_UserID_Text              --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n\n  // --- Task 2 Outputs to Intermediate Workflow Data ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> T2_PreprocessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 3 Inputs ---\n  T2_PreprocessedDF_Pickle     --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  T1_GeofenceEventsStruct_JSON --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text               --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;\n\n  // --- Task 3 Outputs to Intermediate Workflow Data ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> T3_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> T3_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> T3_SyndetiresProcessedDF_Pickle;\n\n  // --- Intermediate Workflow Data to Task 4 Inputs ---\n  T3_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  T3_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  T3_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  T1_UserPlacesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  T1_MobilitiesList_JSON          --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  T1_GeofenceEventsStruct_JSON    --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                  --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;\n\n  // --- Task 4 Outputs to Intermediate Workflow Data ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> T4_EnrichedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 5 Inputs ---\n  T4_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  T1_GeofenceEventsStruct_JSON  --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON;\n  T1_UserID_Text                --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;\n\n  // --- Task 5 Outputs to Intermediate Workflow Data ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> T5_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Intermediate Workflow Data to Task 6 Inputs ---\n  T5_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  T1_MobilitiesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;\n  T1_UserPlacesList_JSON                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;\n  T1_UserID_Text                         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;\n  T1_TimelineMode_Value_Text             --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 6 Outputs to Intermediate Workflow Data (and subsequently to Workflow Output) ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> T6_FinalFormattedTrips_Pickle;\n\n  // --- Intermediate Workflow Data to Task 7 Inputs ---\n  T6_FinalFormattedTrips_Pickle         --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  Input_GroundTruthDataFile_WF        --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n  T2_PreprocessedDF_Pickle            --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle;\n  T1_UserID_Text                      --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;\n\n  // --- Workflow Outputs from Tasks/Intermediate Data ---\n  T6_FinalFormattedTrips_Pickle                              --> Output_FinalSegmentTimeline_Pickle;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path for non-parameterized run\n    is_optional true; // This data input might not always be provided\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Parameterizing workflow inputs ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp;\n    //   is_optional true; // Still optional even when parameterized\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "8EyAEZcBpHPS2GeIyTG9": {
                "id": "8EyAEZcBpHPS2GeIyTG9",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    // path \"UC4/GroundTruth/Dataset1/userX_ground_truth.json\"; // Example path for non-parameterized run\n    is_optional true; // This data input might not always be provided\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Parameterization Examples (Variability Points) ---\n\n    // For Task 1: Ingestion (Defaults for extraction if not in JSON)\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    // For Task 2: Preprocessing\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    // For Task 3: Core Segmentation\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    // For Task 4: Semantic Enrichment\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    // For Task 5: Timeline Coherence\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // For Task 6: Timeline Finalization\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // For Task 7: Evaluation\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Parameterizing workflow inputs ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp;\n    //   is_optional true; // Still optional even when parameterized\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "3Ux7EZcBpHPS2GeIOjEW": {
                "id": "3Ux7EZcBpHPS2GeIOjEW",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.trained_model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "3kx7EZcBpHPS2GeIPDFE"
                ],
                "start": "2025-05-27T11:22:58Z",
                "end": "2025-05-27T11:26:04Z"
            }
        },
        {
            "6Ex-EZcBpHPS2GeIwDGo": {
                "id": "6Ex-EZcBpHPS2GeIwDGo",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.trained_model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "6Ux-EZcBpHPS2GeIwjGo"
                ],
                "start": "2025-05-27T11:26:49Z",
                "end": "2025-05-27T11:29:52Z"
            }
        },
        {
            "8UyEEZcBpHPS2GeIuTGJ": {
                "id": "8UyEEZcBpHPS2GeIuTGJ",
                "name": "I2CAT_workflow1_explain",
                "model": "workflow uc2_workflow1 {\n\n    START -> read_data -> split_dataset -> train_model -> benchmark_model -> Explainability -> WaitTask -> END;\n\n    task read_data {\n        implementation \"UC2/read_data\";\n    }\n\n    task split_dataset {\n        implementation \"UC2/split_dataset\";\n    }\n\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n\n    task benchmark_model {\n        implementation \"UC2/benchmark_model\";\n    }\n\n    task Explainability {\n        implementation \"UC2/Explainability\";\n    }\n\n    task WaitTask {\n        implementation \"UC2/WaitTask\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> read_data.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"UC2/Crypto_desktop.parquet\";\n    }\n\n    define output data xtrain;\n    define output data xtest;\n    define output data ytrain;\n    define output data ytest;\n    define output data ypred;\n    define output data rocdata;\n    define output data model;\n\n    configure data xtrain {\n        zenoh_name \"X_train.csv\";\n    }\n    configure data xtest {\n        zenoh_name \"X_test.csv\";\n    }\n    configure data ytrain {\n        zenoh_name \"Y_train.csv\";\n    }\n    configure data ytest {\n        zenoh_name \"Y_test.csv\";\n    }\n    configure data ypred {\n        zenoh_name \"Y_pred.csv\";\n    }\n    configure data model {\n        zenoh_name \"model.pkl\";\n    }\n    configure data rocdata {\n        zenoh_name \"roc_data.json\";\n    }\n    \n    Explainability.xtrain --> xtrain;\n    Explainability.xtest --> xtest;\n    Explainability.ytrain --> ytrain;\n    Explainability.ytest --> ytest;\n    Explainability.ypred --> ypred;\n    Explainability.rocdata --> rocdata;\n    Explainability.model --> model;\n\n\n\n\n    read_data.dataset --> split_dataset.dataset;\n    split_dataset.train_data --> train_model.train_data;\n    split_dataset.test_data --> train_model.test_data;\n    train_model.model --> benchmark_model.model;\n    train_model.train_data --> benchmark_model.train_data;\n    train_model.test_data --> benchmark_model.test_data;\n    benchmark_model.model --> Explainability.trained_model;\n    benchmark_model.train_data --> Explainability.train_data;\n    benchmark_model.test_data --> Explainability.test_data;\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task train_model {\n        implementation \"UC2/train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        // param max_depth = range(3, 7, 3);\n        param max_depth = enum(3);\n        // param n_estimators = range(5, 16, 5);\n        param n_estimators = enum(5);\n        // param min_child_weight = range(1,3,2);\n        param min_child_weight = enum(3);\n        // param gamma = range(1,3,1);\n        param gamma = enum(3);\n        task train_model {\n        param max_depth = max_depth;\n        param n_estimators = n_estimators;\n        param min_child_weight = min_child_weight;\n        param gamma = gamma;\n        }\n    }\n\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "8kyEEZcBpHPS2GeIuzG2"
                ],
                "start": "2025-05-27T11:33:20Z",
                "end": "2025-05-27T11:36:36Z"
            }
        },
        {
            "_EyUEZcBpHPS2GeI9jEc": {
                "id": "_EyUEZcBpHPS2GeI9jEc",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF { // CORRECTED\n  //  path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Provide a placeholder path\n  //  is_optional true; // This data input might not always be provided\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\"; // UNCOMMENTED INTENT\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Group ALL space-level virtual parameters here ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Parameterizing workflow inputs (space_param) ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n\n    // --- Now, define task-specific parameter mappings ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "_UyVEZcBpHPS2GeIOzEJ": {
                "id": "_UyVEZcBpHPS2GeIOzEJ",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF { // CORRECTED\n  //  path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Provide a placeholder path\n  //  is_optional true; // This data input might not always be provided\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\"; // UNCOMMENTED INTENT\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Group ALL space-level virtual parameters here ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Parameterizing workflow inputs (space_param) ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n\n    // --- Now, define task-specific parameter mappings ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "_kyWEZcBpHPS2GeIkTH6": {
                "id": "_kyWEZcBpHPS2GeIkTH6",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF { // CORRECTED\n  //  path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Provide a placeholder path\n  //  is_optional true; // This data input might not always be provided\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\"; // UNCOMMENTED INTENT\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Group ALL space-level virtual parameters here ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Parameterizing workflow inputs (space_param) ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Giannis/Giannis235.json\",\n      \"UC4/RawTraces/Giannis/Giannis245.json\",\n    );\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n\n    // --- Now, define task-specific parameter mappings ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "AUyYEZcBpHPS2GeI4zJc": {
                "id": "AUyYEZcBpHPS2GeI4zJc",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF {\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Placeholder path\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS for TASKS (using 'param') ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Define VIRTUAL PARAMETERS for WORKFLOW INPUTS (using 'space_param') ---\n    // This placement assumes 'space_param' is recognized after 'task' blocks,\n    // or that it's generally a valid top-level item within 'space'.\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // Example: Parameterizing the Ground Truth file\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "AkyZEZcBpHPS2GeIAjLI": {
                "id": "AkyZEZcBpHPS2GeIAjLI",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF {\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Placeholder path\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS for TASKS (using 'param') ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Define VIRTUAL PARAMETERS for WORKFLOW INPUTS (using 'space_param') ---\n    // This placement assumes 'space_param' is recognized after 'task' blocks,\n    // or that it's generally a valid top-level item within 'space'.\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // Example: Parameterizing the Ground Truth file\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "A0yZEZcBpHPS2GeIkTLk": {
                "id": "A0yZEZcBpHPS2GeIkTLk",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF {\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Placeholder path\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS for TASKS (using 'param') ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Define VIRTUAL PARAMETERS for WORKFLOW INPUTS (using 'space_param') ---\n    // This placement assumes 'space_param' is recognized after 'task' blocks,\n    // or that it's generally a valid top-level item within 'space'.\n\n    // Example: Parameterizing the Ground Truth file\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "_0yWEZcBpHPS2GeIqjHl": {
                "id": "_0yWEZcBpHPS2GeIqjHl",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF { // CORRECTED\n  //  path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Provide a placeholder path\n  //  is_optional true; // This data input might not always be provided\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\"; // UNCOMMENTED INTENT\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Group ALL space-level virtual parameters here ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Parameterizing workflow inputs (space_param) ---\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Giannis/Giannis235.json\",\n      \"UC4/RawTraces/Giannis/Giannis245.json\"\n    );\n    // Example: Parameterizing the Ground Truth file (conceptual, depends on engine support for functions)\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n\n    // --- Now, define task-specific parameter mappings ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      // Input_AccThreshold_Text is data, not a direct param of Task 2 here.\n      // Its value is determined by Task 1's output (T1_AccThresh_Value_Text).\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "AEyYEZcBpHPS2GeIgzJl": {
                "id": "AEyYEZcBpHPS2GeIgzJl",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  configure data Input_GroundTruthDataFile_WF {\n    path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Placeholder path\n    is_optional true;\n  }\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n  intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS for TASKS (using 'param') ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Define VIRTUAL PARAMETERS for WORKFLOW INPUTS (using 'space_param') ---\n    // This placement assumes 'space_param' is recognized after 'task' blocks,\n    // or that it's generally a valid top-level item within 'space'.\n    space_param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // Example: Parameterizing the Ground Truth file\n    // space_param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n    // --- Configure data using space_params ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF {\n    //   path input_gt_file_path_vp; // This would use the conceptual 'input_gt_file_path_vp'\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "BUybEZcBpHPS2GeIxzLp": {
                "id": "BUybEZcBpHPS2GeIxzLp",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline { // BASE WORKFLOW DEFINITION\n\n  // ... (Task Connections and Task Definitions as before) ...\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // Task Definitions\n  task UC4_Task1_IngestAndContextualize { implementation \"UC4/pythia_pipeline_tasks/DataIngestion\"; }\n  task UC4_Task2_PreprocessAndFormTrajectory { implementation \"UC4/pythia_pipeline_tasks/PreProcessing\"; }\n  task UC4_Task3_CoreSegmentationAndDictCreation { implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\"; }\n  task UC4_Task4_SemanticEnrichmentAndModeInference { implementation \"UC4/pythia_pipeline_tasks/ModeDetection\"; }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement { implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\"; }\n  task UC4_Task6_TimelineFinalizationAndOutput { implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\"; }\n  task UC4_Task7_EvaluateAndVisualize { implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\"; }\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // DATA CONNECTIONS (Direct Connections - as before)\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle;\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // DATA CONFIGURATIONS at WORKFLOW LEVEL\n  // Path for Input_MasterDataJSON_WF will be overridden by the experiment space parameter.\n  // Provide a default or placeholder if needed for validation, or rely on engine behavior.\n  configure data Input_MasterDataJSON_WF {\n    // Default path if not overridden by experiment, or simply 'path input_master_json_path_vp;'\n    // The exact syntax here depends on how the engine links workflow-level 'configure data'\n    // to space parameters. Assuming direct reference by name is possible:\n    path input_master_json_path_vp; // THIS IS THE KEY CHANGE\n                                     // It references a parameter expected from the space.\n                                     // If this causes a \"unknown variable\" error here,\n                                     // then it might need a default: path \"default.json\";\n                                     // and the engine handles the override.\n                                     // For now, let's assume direct reference.\n  }\n  //configure data Input_GroundTruthDataFile_WF { // As per your version, this is commented out\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\n// The 'assembled' workflow inherits the 'configure data' from uc4_pythia_full_pipeline.\n// The space parameter 'input_master_json_path_vp' will effectively override the path for Input_MasterDataJSON_WF\n// when a run from the space is executed.\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // Parameter for workflow input data path\n    param input_master_json_path_vp = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // param input_gt_file_path_vp = F(...); // If used for the ground truth path\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The connection is made by the 'input_master_json_path_vp' parameter\n    // defined in this space being used in the workflow-level 'configure data'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "B0yfEZcBpHPS2GeINTKm": {
                "id": "B0yfEZcBpHPS2GeINTKm",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "CEyhEZcBpHPS2GeIlTLU": {
                "id": "CEyhEZcBpHPS2GeIlTLU",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "CUykEZcBpHPS2GeIojLE": {
                "id": "CUykEZcBpHPS2GeIojLE",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "BEyaEZcBpHPS2GeI2TLL": {
                "id": "BEyaEZcBpHPS2GeI2TLL",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n\n  // === Inputs to the Workflow ===\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF; // is_optional handled in configure\n\n  // === Outputs of the Workflow ===\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/Giannis/Giannis015.json\";\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\"; // Placeholder path\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20); // minutes\n    param t4_geofence_max_dist_vp = enum(100, 150); // meters\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7); // minutes\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180); // seconds\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100); // seconds\n\n    // Example: If you want to directly control a param on Task 6\n    // param t6_ect_max_gap_hours_vp = enum(4.0, 6.0, 8.0);\n\n    // Parameter for workflow input data path\n    param input_master_json_path_vp = enum( // MOVED UP and uses 'param'\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\", // These paths being non-existent is NOT a syntax error\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // Example: Parameterizing the Ground Truth file\n    // param input_gt_file_path_vp = F(input_master_json_path_vp, master_path -> master_path.replace(\"_master_data.json\", \"_ground_truth.json\").replace(\"RawTraces\", \"GroundTruth\"));\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput {\n    //   param ect_max_gap_hours = t6_ect_max_gap_hours_vp;\n    // }\n\n    // task UC4_Task7_EvaluateAndVisualize {\n    //   // No tunable params in this example for Task 7\n    // }\n\n    // --- Configure data using the defined parameters ---\n    configure data Input_MasterDataJSON_WF {\n      path input_master_json_path_vp;\n    }\n\n    // configure data Input_GroundTruthDataFile_WF { // This whole block is commented out\n    //   path input_gt_file_path_vp;\n    //   is_optional true;\n    // }\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "BkydEZcBpHPS2GeIWDL9": {
                "id": "BkydEZcBpHPS2GeIWDL9",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // Task CONNECTIONS (Control Flow) - (Same as before)\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // Task Definitions - (Same as before)\n  task UC4_Task1_IngestAndContextualize { implementation \"UC4/pythia_pipeline_tasks/DataIngestion\"; }\n  task UC4_Task2_PreprocessAndFormTrajectory { implementation \"UC4/pythia_pipeline_tasks/PreProcessing\"; }\n  // ... (all task definitions)\n  task UC4_Task7_EvaluateAndVisualize { implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\"; }\n\n\n  // DATA DEFINITIONS for the WORKFLOW\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // DATA CONNECTIONS (Direct Connections) - (Same as before)\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n  // ... (all data connections)\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File; // Assuming GT is still used\n\n\n  // DATA CONFIGURATIONS at WORKFLOW LEVEL\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // Keep this commented as per your last version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    // ... (other tX_..._vp params for tasks 3, 4, 5)\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name.\n    param Input_MasterDataJSON_WF = enum( // CRITICAL: Name matches workflow input data item\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n    // ... (other task mappings for tasks 3, 4, 5)\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "DEysEZcBpHPS2GeI2jKA": {
                "id": "DEysEZcBpHPS2GeI2jKA",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "DUyuEZcBpHPS2GeIATKJ": {
                "id": "DUyuEZcBpHPS2GeIATKJ",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "DkyuEZcBpHPS2GeI3zJY": {
                "id": "DkyuEZcBpHPS2GeI3zJY",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "D0yvEZcBpHPS2GeI5jKQ": {
                "id": "D0yvEZcBpHPS2GeI5jKQ",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "CkyoEZcBpHPS2GeIbjKY": {
                "id": "CkyoEZcBpHPS2GeIbjKY",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "C0ysEZcBpHPS2GeICDIv": {
                "id": "C0ysEZcBpHPS2GeICDIv",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "EUywEZcBpHPS2GeIiDLU": {
                "id": "EUywEZcBpHPS2GeIiDLU",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "EkywEZcBpHPS2GeIizJ_"
                ],
                "start": "2025-05-27T12:21:13Z"
            }
        },
        {
            "E0ywEZcBpHPS2GeInDLQ": {
                "id": "E0ywEZcBpHPS2GeInDLQ",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "EEywEZcBpHPS2GeITTI3": {
                "id": "EEywEZcBpHPS2GeITTI3",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "GEy2EZcBpHPS2GeI6zJs": {
                "id": "GEy2EZcBpHPS2GeI6zJs",
                "name": "I2CAT_workflow1",
                "model": "workflow uc2_workflow1 {\n\n    START -> ReadData -> Partitioning -> ModelTrain -> ModelPredict -> END;\n\n    task ReadData {\n        implementation \"I2CAT.read_data\";\n    }\n\n    task Partitioning {\n        implementation \"I2CAT.split_dataset\";\n    }\n\n    task ModelTrain;\n\n    task ModelPredict {\n        implementation \"I2CAT.benchmark_model\";\n    }\n\n    define input data ExternalDataFile;\n    ExternalDataFile --> ReadData.FileToRead;\n\n    configure data ExternalDataFile {\n        path \"uc2-datasets/dataset_RiskBehaviour/RiskActivities/Crypto_desktop.parquet\";\n    }\n}\n\nworkflow Assembled_uc2_workflow1 from uc2_workflow1 {\n    task ModelTrain {\n        implementation \"I2CAT.train_model\";\n    }\n}\n\nexperiment uc2_workflow1_Exp {\n    intent FindBestClassifier;\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow1 {\n        strategy gridsearch;\n        param max_depth_vp = range(3, 4);\n        param n_estimators_vp = range(5, 6);\n\n        task ModelTrain {\n          param max_depth = max_depth_vp;\n          param n_estimators = n_estimators_vp;\n        }\n    }\n}\n",
                "status": "running",
                "workflow_ids": [
                    "GUy2EZcBpHPS2GeI7jJh"
                ],
                "start": "2025-05-27T12:28:11Z"
            }
        },
        {
            "FkyzEZcBpHPS2GeI_TKX": {
                "id": "FkyzEZcBpHPS2GeI_TKX",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "F0y0EZcBpHPS2GeIADIw"
                ],
                "start": "2025-05-27T12:24:59Z"
            }
        },
        {
            "FEyyEZcBpHPS2GeI-jK_": {
                "id": "FEyyEZcBpHPS2GeI-jK_",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "FUyyEZcBpHPS2GeI_TJ6"
                ],
                "start": "2025-05-27T12:23:53Z"
            }
        },
        {
            "KEzDEZcBpHPS2GeI5DLG": {
                "id": "KEzDEZcBpHPS2GeI5DLG",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "KUzFEZcBpHPS2GeIZjJe": {
                "id": "KUzFEZcBpHPS2GeIZjJe",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "MkzSEZcBpHPS2GeIAjJa": {
                "id": "MkzSEZcBpHPS2GeIAjJa",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "HUy6EZcBpHPS2GeIJjKg": {
                "id": "HUy6EZcBpHPS2GeIJjKg",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "Hky6EZcBpHPS2GeIKTI-"
                ],
                "start": "2025-05-27T12:31:43Z"
            }
        },
        {
            "H0y9EZcBpHPS2GeIeDKV": {
                "id": "H0y9EZcBpHPS2GeIeDKV",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "K0zKEZcBpHPS2GeIuzKd": {
                "id": "K0zKEZcBpHPS2GeIuzKd",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "LEzKEZcBpHPS2GeIvjJl"
                ],
                "start": "2025-05-27T12:49:50Z"
            }
        },
        {
            "LUzMEZcBpHPS2GeIJjI1": {
                "id": "LUzMEZcBpHPS2GeIJjI1",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "L0zPEZcBpHPS2GeIETKl": {
                "id": "L0zPEZcBpHPS2GeIETKl",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "MEzPEZcBpHPS2GeIFDKR"
                ],
                "start": "2025-05-27T12:54:34Z"
            }
        },
        {
            "MUzQEZcBpHPS2GeIUzJ3": {
                "id": "MUzQEZcBpHPS2GeIUzJ3",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "I0zAEZcBpHPS2GeI-DL_": {
                "id": "I0zAEZcBpHPS2GeI-DL_",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "JEzAEZcBpHPS2GeI-zK4"
                ],
                "start": "2025-05-27T12:39:10Z"
            }
        },
        {
            "JUzBEZcBpHPS2GeIGTI0": {
                "id": "JUzBEZcBpHPS2GeIGTI0",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "JkzBEZcBpHPS2GeIHDIt"
                ],
                "start": "2025-05-27T12:39:18Z"
            }
        },
        {
            "J0zDEZcBpHPS2GeIDDLq": {
                "id": "J0zDEZcBpHPS2GeIDDLq",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "IEy-EZcBpHPS2GeI-DKx": {
                "id": "IEy-EZcBpHPS2GeI-DKx",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "IUy-EZcBpHPS2GeI-zJ3"
                ],
                "start": "2025-05-27T12:36:59Z"
            }
        },
        {
            "Iky_EZcBpHPS2GeImjJO": {
                "id": "Iky_EZcBpHPS2GeImjJO",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "KkzGEZcBpHPS2GeISDIm": {
                "id": "KkzGEZcBpHPS2GeISDIm",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "LkzMEZcBpHPS2GeIiTLl": {
                "id": "LkzMEZcBpHPS2GeIiTLl",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "O0zbEZcBpHPS2GeIBjJx": {
                "id": "O0zbEZcBpHPS2GeIBjJx",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "PEzbEZcBpHPS2GeIKjIj": {
                "id": "PEzbEZcBpHPS2GeIKjIj",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "PUzbEZcBpHPS2GeIWzLl": {
                "id": "PUzbEZcBpHPS2GeIWzLl",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "PkzbEZcBpHPS2GeIujJb": {
                "id": "PkzbEZcBpHPS2GeIujJb",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "NEzUEZcBpHPS2GeIxTJN": {
                "id": "NEzUEZcBpHPS2GeIxTJN",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "NUzVEZcBpHPS2GeIOTKM": {
                "id": "NUzVEZcBpHPS2GeIOTKM",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "OEzWEZcBpHPS2GeI6TJK": {
                "id": "OEzWEZcBpHPS2GeI6TJK",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "OUzZEZcBpHPS2GeI0TLP": {
                "id": "OUzZEZcBpHPS2GeI0TLP",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "P0zbEZcBpHPS2GeI-DLZ": {
                "id": "P0zbEZcBpHPS2GeI-DLZ",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "QEzcEZcBpHPS2GeIvzIj": {
                "id": "QEzcEZcBpHPS2GeIvzIj",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "M0zUEZcBpHPS2GeIfDI1": {
                "id": "M0zUEZcBpHPS2GeIfDI1",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "NkzVEZcBpHPS2GeIjTJy": {
                "id": "NkzVEZcBpHPS2GeIjTJy",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "N0zWEZcBpHPS2GeIwzLA": {
                "id": "N0zWEZcBpHPS2GeIwzLA",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "OkzaEZcBpHPS2GeIzzJN": {
                "id": "OkzaEZcBpHPS2GeIzzJN",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "QUzdEZcBpHPS2GeIszLX": {
                "id": "QUzdEZcBpHPS2GeIszLX",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "RkziEZcBpHPS2GeIMjLa": {
                "id": "RkziEZcBpHPS2GeIMjLa",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "R0ziEZcBpHPS2GeIQTLA": {
                "id": "R0ziEZcBpHPS2GeIQTLA",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SEzjEZcBpHPS2GeIzzKM": {
                "id": "SEzjEZcBpHPS2GeIzzKM",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SkzmEZcBpHPS2GeIfzID": {
                "id": "SkzmEZcBpHPS2GeIfzID",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n    param t1_default_user_id_vp = enum(\"unknown_user_t1_param\");\n    param t1_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n    param t2_kalman_initial_velocity_std_vp = enum(0.5, 1.0, 1.5);\n    param t2_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n      param default_user_id = t1_default_user_id_vp;\n      param dependent_modules_folders = t1_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n      param kalman_initial_velocity_std = t2_kalman_initial_velocity_std_vp;\n      param dependent_modules_folders = t2_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "S0znEZcBpHPS2GeIdTLA": {
                "id": "S0znEZcBpHPS2GeIdTLA",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n    param t1_default_user_id_vp = enum(\"unknown_user_t1_param\");\n    param t1_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n    param t2_kalman_initial_velocity_std_vp = enum(0.5, 1.0, 1.5);\n    param t2_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n      param default_user_id = t1_default_user_id_vp;\n      param dependent_modules_folders = t1_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n      param kalman_initial_velocity_std = t2_kalman_initial_velocity_std_vp;\n      param dependent_modules_folders = t2_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "TEznEZcBpHPS2GeI6DIa": {
                "id": "TEznEZcBpHPS2GeI6DIa",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n    param t1_default_user_id_vp = enum(\"unknown_user_t1_param\");\n    param t1_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n    param t2_kalman_initial_velocity_std_vp = enum(0.5, 1.0, 1.5);\n    param t2_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n      param default_user_id = t1_default_user_id_vp;\n      param dependent_modules_folders = t1_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n      param kalman_initial_velocity_std = t2_kalman_initial_velocity_std_vp;\n      param dependent_modules_folders = t2_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "TUzpEZcBpHPS2GeIfzJx": {
                "id": "TUzpEZcBpHPS2GeIfzJx",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n    param t1_default_user_id_vp = enum(\"unknown_user_t1_param\");\n    param t1_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n    param t2_kalman_initial_velocity_std_vp = enum(0.5, 1.0, 1.5);\n    param t2_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n      param default_user_id = t1_default_user_id_vp;\n      param dependent_modules_folders = t1_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n      param kalman_initial_velocity_std = t2_kalman_initial_velocity_std_vp;\n      param dependent_modules_folders = t2_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "REzfEZcBpHPS2GeIGTLL": {
                "id": "REzfEZcBpHPS2GeIGTLL",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "RUzhEZcBpHPS2GeIYzIr": {
                "id": "RUzhEZcBpHPS2GeIYzIr",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "SUzmEZcBpHPS2GeIZjLC": {
                "id": "SUzmEZcBpHPS2GeIZjLC",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n    param t1_default_user_id_vp = enum(\"unknown_user_t1_param\");\n    param t1_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n    param t2_kalman_initial_velocity_std_vp = enum(0.5, 1.0, 1.5);\n    param t2_dependent_modules_folders_vp = enum(\"moby_src\");\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n      param default_user_id = t1_default_user_id_vp;\n      param dependent_modules_folders = t1_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n      param kalman_initial_velocity_std = t2_kalman_initial_velocity_std_vp;\n      param dependent_modules_folders = t2_dependent_modules_folders_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "QkzeEZcBpHPS2GeIADKK": {
                "id": "QkzeEZcBpHPS2GeIADKK",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "Q0zeEZcBpHPS2GeIozKu": {
                "id": "Q0zeEZcBpHPS2GeIozKu",
                "name": "uc4_refined_pythia",
                "model": "// ************************************************************************************************\n// * PYTHIA FULL PIPELINE WORKFLOW (UC4 Style - Minimal Example Format) - Single Input JSON\n// ************************************************************************************************\n\nworkflow uc4_pythia_full_pipeline {\n\n  // ------------------------------------------------------------------------------------------------\n  // Task Definitions (MOVED UP - DECLARE TASKS BEFORE USE)\n  // ------------------------------------------------------------------------------------------------\n  task UC4_Task1_IngestAndContextualize {\n    implementation \"UC4/pythia_pipeline_tasks/DataIngestion\";\n  }\n  task UC4_Task2_PreprocessAndFormTrajectory {\n    implementation \"UC4/pythia_pipeline_tasks/PreProcessing\";\n  }\n  task UC4_Task3_CoreSegmentationAndDictCreation {\n    implementation \"UC4/pythia_pipeline_tasks/CoreSegmentation\";\n  }\n  task UC4_Task4_SemanticEnrichmentAndModeInference {\n    implementation \"UC4/pythia_pipeline_tasks/ModeDetection\";\n  }\n  task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n    implementation \"UC4/pythia_pipeline_tasks/TimelineCoherence\";\n  }\n  task UC4_Task6_TimelineFinalizationAndOutput {\n    implementation \"UC4/pythia_pipeline_tasks/FinalizationDaily\";\n  }\n  task UC4_Task7_EvaluateAndVisualize {\n    implementation \"UC4/pythia_pipeline_tasks/EvaluationViz\";\n  }\n\n  // ------------------------------------------------------------------------------------------------\n  // Task CONNECTIONS (Control Flow) (NOW AFTER TASK DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n  START\n    -> UC4_Task1_IngestAndContextualize\n    -> UC4_Task2_PreprocessAndFormTrajectory\n    -> UC4_Task3_CoreSegmentationAndDictCreation\n    -> UC4_Task4_SemanticEnrichmentAndModeInference\n    -> UC4_Task5_TimelineCoherenceAndStructuralRefinement\n    -> UC4_Task6_TimelineFinalizationAndOutput\n    -> UC4_Task7_EvaluateAndVisualize\n    -> END;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA DEFINITIONS for the WORKFLOW (Only Workflow Inputs and Outputs)\n  // ------------------------------------------------------------------------------------------------\n  define input data Input_MasterDataJSON_WF;\n  define input data Input_GroundTruthDataFile_WF;\n\n  define output data Output_FinalSegmentTimeline_Pickle;\n  define output data Output_EvaluationReport_File;\n  define output data Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONNECTIONS (Direct Connections) (MUST BE AFTER TASK & DATA DEFINITIONS)\n  // ------------------------------------------------------------------------------------------------\n\n  // --- Workflow Input to Task 1 ---\n  Input_MasterDataJSON_WF --> UC4_Task1_IngestAndContextualize.Input_MasterJSONFile;\n\n  // --- Task 1 Outputs to Task 2, 3, 4, 5, 6, 7 Inputs ---\n  UC4_Task1_IngestAndContextualize.Output_RawLocationsDF         --> UC4_Task2_PreprocessAndFormTrajectory.Input_LocationsDF_Pickle;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserPlacesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_UserPlacesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserPlacesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_MobilitiesList_JSON;\n  UC4_Task1_IngestAndContextualize.Output_MobilitiesList         --> UC4_Task6_TimelineFinalizationAndOutput.Input_MobilitiesList_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task3_CoreSegmentationAndDictCreation.Input_GeofenceEventsStruct_JSON;\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_GeofenceEventsStruct_JSON; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_GeofenceEventsStruct   --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_GeofenceEventsStruct_JSON; // Fan-out\n  // UC4_Task1_IngestAndContextualize.Output_MostRecentPrediction   --> [NOT CONNECTED TO ANY TASK INPUT IN ORIGINAL]\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task2_PreprocessAndFormTrajectory.Input_UserID_Text;\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task3_CoreSegmentationAndDictCreation.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task6_TimelineFinalizationAndOutput.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_UserID                 --> UC4_Task7_EvaluateAndVisualize.Input_UserID_Text; // Fan-out\n  UC4_Task1_IngestAndContextualize.Output_ExtractedAccThreshold  --> UC4_Task2_PreprocessAndFormTrajectory.Input_AccThreshold_Text;\n  UC4_Task1_IngestAndContextualize.Output_ExtractedTimelineMode  --> UC4_Task6_TimelineFinalizationAndOutput.Input_TimelineModeConfig_Text;\n\n  // --- Task 2 Outputs to Task 3, 7 Inputs ---\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task3_CoreSegmentationAndDictCreation.Input_PreprocessedDF_Pickle;\n  UC4_Task2_PreprocessAndFormTrajectory.Output_PreprocessedDF_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_DataFrameForVisualization_Pickle; // Fan-out\n\n  // --- Task 3 Outputs to Task 4 Inputs ---\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_InitialSegmentDict_Pickle      --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_InitialSegmentDict_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresSummaryDF_Pickle   --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresSummaryDF_Pickle;\n  UC4_Task3_CoreSegmentationAndDictCreation.Output_SyndetiresProcessedDF_Pickle --> UC4_Task4_SemanticEnrichmentAndModeInference.Input_SyndetiresProcessedDF_Pickle;\n\n  // --- Task 4 Outputs to Task 5 Inputs ---\n  UC4_Task4_SemanticEnrichmentAndModeInference.Output_EnrichedSegmentDict_Pickle --> UC4_Task5_TimelineCoherenceAndStructuralRefinement.Input_EnrichedSegmentDict_Pickle;\n\n  // --- Task 5 Outputs to Task 6 Inputs ---\n  UC4_Task5_TimelineCoherenceAndStructuralRefinement.Output_StructurallyRefinedSegmentDict_Pickle --> UC4_Task6_TimelineFinalizationAndOutput.Input_StructurallyRefinedSegmentDict_Pickle;\n\n  // --- Task 6 Outputs to Task 7 Input and Workflow Output ---\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> UC4_Task7_EvaluateAndVisualize.Input_FinalTimeline_Pickle;\n  UC4_Task6_TimelineFinalizationAndOutput.Output_FinalFormattedTrips_Pickle --> Output_FinalSegmentTimeline_Pickle; // To Workflow Output\n\n  // --- Task 7 Inputs from Workflow Input (already defined) ---\n  Input_GroundTruthDataFile_WF --> UC4_Task7_EvaluateAndVisualize.Input_GroundTruthData_File;\n\n  // --- Task 7 Outputs to Workflow Outputs ---\n  UC4_Task7_EvaluateAndVisualize.Output_EvaluationReport_File      --> Output_EvaluationReport_File;\n  UC4_Task7_EvaluateAndVisualize.Output_VisualizationFiles_Directory --> Output_VisualizationFiles_Directory;\n\n  // ------------------------------------------------------------------------------------------------\n  // DATA CONFIGURATIONS (Paths for workflow inputs & outputs)\n  // ------------------------------------------------------------------------------------------------\n  configure data Input_MasterDataJSON_WF {\n    path \"UC4/RawTraces/default_master.json\"; // DEFAULT STRING PATH\n  }\n  //configure data Input_GroundTruthDataFile_WF { // This whole block is commented out in your version\n  // path \"UC4/GroundTruth/Dataset1/placeholder_ground_truth.json\";\n  //  is_optional true;\n  //}\n\n  configure data Output_FinalSegmentTimeline_Pickle {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_timeline.pickle\";\n  }\n  configure data Output_EvaluationReport_File {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_eval_report.json\";\n    type \"json\";\n  }\n  configure data Output_VisualizationFiles_Directory {\n    path \"UC4/PythiaPipeline/results/${Input_MasterDataJSON_WF.basename}_visualizations/\";\n    type \"directory\";\n  }\n}\n\n// Assembled workflow for use in experiments\nworkflow uc4_pythia_full_pipeline_assembled from uc4_pythia_full_pipeline {\n}\n\n// ************************************************************************************************\n// * PYTHIA FULL PIPELINE EXPERIMENT\n// ************************************************************************************************\n\nexperiment UC4PythiaFullExperiment {\n // intent \"Evaluate the full Pythia segmentation pipeline with various parameter settings, using a single master JSON input.\";\n\n  control {\n    START -> PythiaParameterSpace -> END;\n  }\n\n  space PythiaParameterSpace of uc4_pythia_full_pipeline_assembled {\n    strategy gridsearch;\n    // runs = 10; // For randomsearch or if parameters have stochastic effects\n\n    // --- Define ALL VIRTUAL PARAMETERS (for TASKS and for WORKFLOW INPUTS) using 'param' ---\n    param t1_default_acc_thresh_vp = enum(50, 75, 100);\n    param t1_default_timeline_mode_vp = enum(\"open\", \"full\", \"none\");\n\n    param t2_enable_kalman_vp = enum(\"true\", \"false\");\n    param t2_kalman_process_noise_vp = enum(0.001, 0.005, 0.01);\n    param t2_kalman_measurement_noise_vp = enum(1.0, 3.0, 5.0);\n\n    param t3_hybrid_speed_thresh_vp = enum(0.8, 1.0, 1.2);\n    param t3_hybrid_min_stat_time_vp = enum(45, 60, 90);\n    param t3_enable_wandering_vp = enum(\"true\", \"false\");\n    param t3_wander_score_thresh_vp = enum(40.0, 50.0, 60.0);\n\n    param t4_use_improved_mode_vp = enum(\"true\", \"false\");\n    param t4_enable_gtfs_bus_vp = enum(\"true\", \"false\");\n    param t4_geofence_time_window_vp = enum(10, 20);\n    param t4_geofence_max_dist_vp = enum(100, 150);\n\n    param t5_cs_dist_thresh_vp = enum(80.0, 100.0, 120.0);\n    param t5_walk_round_trip_max_dur_vp = enum(3, 5, 7);\n    param t5_stat_merge_max_gap_vp = enum(90, 120, 180);\n    param t5_missing_trip_max_gap_vp = enum(1500, 1800, 2100);\n\n    // Parameter for workflow input data path.\n    // Name matches the 'define input data' name for implicit linking by the engine.\n    param Input_MasterDataJSON_WF = enum(\n      \"UC4/RawTraces/Dataset1/userX_master_data.json\",\n      \"UC4/RawTraces/Dataset1/userY_master_data.json\",\n      \"UC4/RawTraces/Dataset2/userZ_master_data.json\"\n    );\n    // If parameterizing GroundTruthDataFile_WF (which is currently commented out in workflow config):\n    // param Input_GroundTruthDataFile_WF = enum(\"gt_path1.json\", \"gt_path2.json\");\n\n\n    // --- Define TASK PARAMETER MAPPINGS ---\n    task UC4_Task1_IngestAndContextualize {\n      param default_acc_threshold = t1_default_acc_thresh_vp;\n      param default_timeline_mode = t1_default_timeline_mode_vp;\n    }\n\n    task UC4_Task2_PreprocessAndFormTrajectory {\n      param enable_kalman_filter = t2_enable_kalman_vp;\n      param kalman_process_noise_scale = t2_kalman_process_noise_vp;\n      param kalman_measurement_noise_scale = t2_kalman_measurement_noise_vp;\n    }\n\n    task UC4_Task3_CoreSegmentationAndDictCreation {\n      param hybrid_speed_threshold = t3_hybrid_speed_thresh_vp;\n      param hybrid_min_stationary_time_s = t3_hybrid_min_stat_time_vp;\n      param enable_wandering_detection = t3_enable_wandering_vp;\n      param wander_score_threshold = t3_wander_score_thresh_vp;\n    }\n\n    task UC4_Task4_SemanticEnrichmentAndModeInference {\n      param use_improved_mode_detection = t4_use_improved_mode_vp;\n      param enable_gtfs_bus_search = t4_enable_gtfs_bus_vp;\n      param geofence_time_window_minutes = t4_geofence_time_window_vp;\n      param geofence_max_distance_meters = t4_geofence_max_dist_vp;\n    }\n\n    task UC4_Task5_TimelineCoherenceAndStructuralRefinement {\n      param cs_distance_threshold_m = t5_cs_dist_thresh_vp;\n      param walk_round_trip_max_duration_min = t5_walk_round_trip_max_dur_vp;\n      param stat_merge_max_gap_s = t5_stat_merge_max_gap_vp;\n      param missing_trip_max_gap_s = t5_missing_trip_max_gap_vp;\n    }\n\n    // task UC4_Task6_TimelineFinalizationAndOutput { ... }\n    // task UC4_Task7_EvaluateAndVisualize { ... }\n\n    // NO 'configure data' blocks INSIDE the 'space' block.\n    // The engine implicitly uses the value of space param 'Input_MasterDataJSON_WF'\n    // to set the path for the workflow input 'Input_MasterDataJSON_WF'.\n  }\n}",
                "status": "new",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": []
            }
        },
        {
            "VkzNFZcBpHPS2GeI9zKV": {
                "id": "VkzNFZcBpHPS2GeI9zKV",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    define output data TrainedModelFolder;\n\n    configure data TrainedModelFolder {\n        path \"output/trained_model/**\";\n        type \"generated-ML-model\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n    ReadMetrics.OutputFolder --> TrainedModelFolder;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "V0zNFZcBpHPS2GeI-zJR"
                ],
                "start": "2025-05-28T07:31:49Z",
                "end": "2025-05-28T07:41:09Z"
            }
        },
        {
            "YUzvFZcBpHPS2GeItjLl": {
                "id": "YUzvFZcBpHPS2GeItjLl",
                "name": "CS_surrogate_model_DEMO",
                "model": "package CS_surrogate_model_DEMO;\n\nworkflow CS_surrogate_model_DEMO {\n\n    // Task CONNECTIONS\n    START -> DemoAPI -> ReadMetrics -> END;\n\n    task DemoAPI;\n\n    task ReadMetrics {\n        implementation \"CS_surrogate_model.ReadMetricsAPI\";\n    }\n\n    // DATA\n    define input data ExternalDataFile;\n\n    configure data ExternalDataFile {\n        path \"CS_surrogate_model/nimes_2005_STAC.json\";\n    }\n\n    // DATA CONNECTIONS\n    ExternalDataFile --> DemoAPI.FileToRead;\n    DemoAPI.jobID --> ReadMetrics.jobID;\n    DemoAPI.DataVolumeRatio --> ReadMetrics.DataVolumeRatio;\n}\n\nworkflow TrainDemoAPI from CS_surrogate_model_DEMO {\n  task DemoAPI {\n    implementation \"CS_surrogate_model.DemoAPI\";\n  }\n}\n\nexperiment EXP {\n    intent FindBestClassifier;\n    control {\n        START -> S1 -> END;\n    }\n\n    space S1 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(256);\n        param model_config_vp = enum(0);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n\n    space S2 of TrainDemoAPI {\n        strategy gridsearch;\n        param batch_size_vp = enum(1);\n        param lr_vp = enum(0.01);\n        param optimizer_vp = enum(\"Adam\");\n        param scheduler_vp = enum(\"reduce_plateau\");\n        param patch_size_vp = enum(512);\n        param model_config_vp = enum(1);\n        task DemoAPI {\n            param batch_size = batch_size_vp;\n            param lr = lr_vp;\n            param optimizer = optimizer_vp;\n            param scheduler = scheduler_vp;\n            param patch_size = patch_size_vp;\n            param model_config = model_config_vp;\n        }\n    }\n}\n",
                "status": "completed",
                "creator": {
                    "name": "dummy_user"
                },
                "workflow_ids": [
                    "YkzvFZcBpHPS2GeIujKG"
                ],
                "start": "2025-05-28T08:08:42Z",
                "end": "2025-05-28T08:17:59Z"
            }
        },
        {
            "UkxJEpcBpHPS2GeI0jI9": {
                "id": "UkxJEpcBpHPS2GeI0jI9",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "U0xJEpcBpHPS2GeI1TIH"
                ],
                "start": "2025-05-27T15:08:39Z"
            }
        },
        {
            "VEzAFZcBpHPS2GeIczJ7": {
                "id": "VEzAFZcBpHPS2GeIczJ7",
                "name": "I2CAT_workflow0",
                "model": "workflow uc2_workflow0 {\n\n    START -> create_scenario -> create_operation -> create_dataset -> benchmarking -> END;\n\n    task create_scenario {\n        implementation \"I2CAT.create_scenario\";\n    }\n    task create_operation {\n        implementation \"I2CAT.create_operation\";\n    }\n    task create_dataset {\n        implementation \"I2CAT.create_dataset\";\n    }\n    \n    task benchmarking {\n        implementation \"I2CAT.benchmarking\";\n    }\n}\n\nworkflow Assembled_uc2_workflow0 from uc2_workflow0 {\n}\n\nexperiment uc2_workflow0_Exp {\n\n    control {\n        START -> s1 -> END;\n    }\n\n    space s1 of Assembled_uc2_workflow0 {\n        strategy gridsearch;\n    }\n\n}\n\n",
                "status": "running",
                "workflow_ids": [
                    "VUzAFZcBpHPS2GeIdTLq"
                ],
                "start": "2025-05-28T07:17:05Z"
            }
        }
    ]
}